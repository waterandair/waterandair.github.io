<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Go语言-结构体的零值问题</title>
      <link href="/2018-11-05-go-the-zero-value-of-struct-is-not-nil.html"/>
      <url>/2018-11-05-go-the-zero-value-of-struct-is-not-nil.html</url>
      
        <content type="html"><![CDATA[<p>Pay  attention： 结构体类型的 零值 不是 nil</p><a id="more"></a><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">结构体类型的 零值 不是 nil， 如果一个结构体作为字段类型的话，后续对其的零值判断会有小问题</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">所以一般尽量把结构体类型改为指针的形式，这样既方便做零值判断，又可以减少元素复制带来的开销</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">type</span> Sub <span class="keyword">struct</span> &#123;</span><br><span class="line">SubName <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">注意这里的 Sub 字段类型是一个结构体</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">type</span> Foo <span class="keyword">struct</span> &#123;</span><br><span class="line">Sub Sub</span><br><span class="line">Name <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">注意这里的 Sub 字段类型是一个 结构体类型的指针</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">type</span> Foo2 <span class="keyword">struct</span> &#123;</span><br><span class="line">Sub *Sub</span><br><span class="line">Name <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> foo Foo</span><br><span class="line"><span class="comment">// 打印结构体的零值</span></span><br><span class="line">fmt.Println(foo.Sub)  <span class="comment">// &#123;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> foo2 Foo2</span><br><span class="line"><span class="comment">// 打印结构体指针的零值</span></span><br><span class="line">fmt.Println(foo2.Sub)  <span class="comment">// &lt;nil&gt;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
          <category> Go 语言注意点 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zookeeper python 客户端 kazoo 使用指南(译自官网)</title>
      <link href="/2018-10-03-zookeeper-kazoo-usage.html"/>
      <url>/2018-10-03-zookeeper-kazoo-usage.html</url>
      
        <content type="html"><![CDATA[<p>kazoo 基本使用方法，翻译自<a href="https://kazoo.readthedocs.io/en/latest/" target="_blank" rel="noopener">官网</a></p><a id="more"></a><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><h4 id="处理连接"><a href="#处理连接" class="headerlink" title="处理连接"></a>处理连接</h4><p>要开始使用Kazoo，必须创建一个KazooClient对象并建立连接：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"></span><br><span class="line">zk = KazooClient(hosts=<span class="string">'127.0.0.1:2181'</span>)</span><br><span class="line">zk.start()</span><br></pre></td></tr></table></figure><p>默认情况下，客户端将连接到<code>127.0.0.1:2181</code>。<br>应该确保 Zookeeper 服务器正在运行中，否则 start 命令将一直等到它的默认超时(10s)。<br>客户端一旦连接到服务器，不论是连接丢失或是会话到期，都会尝试保持连接。<br>可以调用 <code>stop</code> 方法删除客户端连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zk.stop()</span><br></pre></td></tr></table></figure><h5 id="日志设置"><a href="#日志设置" class="headerlink" title="日志设置"></a>日志设置</h5><p>如果不设置日志，会收到以下消息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No handlers could be found for logger &quot;kazoo.client&quot;</span><br></pre></td></tr></table></figure><p>为了避免这个问题，仅仅需要加入以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig()</span><br></pre></td></tr></table></figure><p>详细了解<code>logging</code>请阅读<a href="https://docs.python.org/howto/logging.html" target="_blank" rel="noopener">python 日志指南</a></p><h5 id="监听连接事件"><a href="#监听连接事件" class="headerlink" title="监听连接事件"></a>监听连接事件</h5><p>客户端需要知道连接什么时候被删除、重置或过期。<code>kazoo</code> 简化了这个过程，当连接状态发生变化的时候，会调用被注册的监听函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooState</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_listener</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state == KazooState.LOST:</span><br><span class="line">        <span class="comment"># Register somewhere that the session was lost</span></span><br><span class="line">    <span class="keyword">elif</span> state == KazooState.SUSPENDED:</span><br><span class="line">        <span class="comment"># Handle being disconnected from Zookeeper</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Handle being connected/reconnected to Zookeeper</span></span><br><span class="line"></span><br><span class="line">zk.add_listener(my_listener)</span><br></pre></td></tr></table></figure><p>在使用<code>kazoo.recipe.lock.Lock</code>或新建临时节点的时候，强烈建议注册状态监听函数去处理连接中断或会话丢失</p><h5 id="理解-Kazoo-状态"><a href="#理解-Kazoo-状态" class="headerlink" title="理解 Kazoo 状态"></a>理解 Kazoo 状态</h5><p><code>KazooState</code> 对象表示客户端连接的几个状态，始终可以通过 <code>state</code> 属性查看连接当前的状态，可能的状态是:</p><ul><li>LOST</li><li>CONNECTED</li><li>SUSPENDED  </li></ul><p>首次创建<code>KazooClient</code>状态是 <code>LOST</code>， 成功建立连接后，状态会转换为 <code>CONNECTED</code>。在创建连接的过程中，如果出现连接问题或者需要连接到不同的zookeeper集群节点，状态将转换为 <code>SUSPENDED</code>，表示当前命令不能正确执行。如果 zookeeper 节点已经不在是集群节点的一部分时，状态也会为<code>SUSPENDED</code>。<br>重新建立连接后，如果会话已过期，连接转换为<code>LOST</code>，如果会话仍然有效，则可以转换为<code>CONNECTED</code>。<br><code>建议注册状态监听函数监听连接状态，以保证客户端连接正常运行</code><br>当连接处于<code>SUSPENDED</code>的时候，如果客户端正在执行需要其他系统协商的操作(比如锁操作)，应当暂停操作。重新连接后在继续操作，当连接朱状态为<code>LOST</code>时，zookeeper 会删除已创建的临时节点，这回影响到临时节点的操作(比如锁操作)，当状态再次转化为<code>CONNECTED</code>后，需要重新获取锁。</p><h6 id="有效的状态转换"><a href="#有效的状态转换" class="headerlink" title="有效的状态转换"></a>有效的状态转换</h6><ul><li><code>LOST -&gt; CONNECTED</code>： 新建连接或已经创建但丢失的连接重新连接正常</li><li><code>CONNECTED -&gt; SUSPENDED</code>： 处于连接状态的连接发生连接丢失</li><li><code>CONNECTED -&gt; LOST</code>： 仅当建立连接后不能有效的验证身份的时候发生</li><li><code>SUSPENDED -&gt; LOST</code>： 因为连接重连后会话过期而丢失</li><li><code>SUSPENDED -&gt; CONNECTED</code>： 丢失的连接重连成功</li></ul><h5 id="只读连接-0-6版后加入"><a href="#只读连接-0-6版后加入" class="headerlink" title="只读连接(0.6版后加入)"></a>只读连接(0.6版后加入)</h5><p>zookeeper 3.4版加入<code>只读模式</code>的功能，要先在zookeeper服务器中启用此模式。<br>在 kazoo 中，调用 KazooClient 时设置 <code>read_only</code> 为 <code>True</code>，这样客户端将连接到已经变为<code>只读</code>的节点，并且会继续扫描其他<code>读写</code>节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"></span><br><span class="line">zk = KazooClient(hosts=<span class="string">'127.0.0.1:2181'</span>, read_only=<span class="literal">True</span>)</span><br><span class="line">zk.start()</span><br></pre></td></tr></table></figure><p>KeeperState 添加了一个新属性 <code>CONNECTED_RO</code>. 之前提到的状态仍然有效，但是在 <code>CONNECTED</code> 时，需要检查状态是否为 <code>CONNECTED_RO</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooState</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KeeperState</span><br><span class="line"></span><br><span class="line">zk = KazooClient()</span><br><span class="line"></span><br><span class="line"><span class="meta">@zk.add_listener</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_for_ro</span><span class="params">(state)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state == KazooState.CONNECTED:</span><br><span class="line">        <span class="keyword">if</span> zk.client_state == KeeperState.CONNECTED_RO:</span><br><span class="line">            print(<span class="string">"read only mode!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"read/write mode"</span>)</span><br></pre></td></tr></table></figure><p>注意将 <code>KazooState</code> 传给监听函数，只有通过与 <code>KeeperState</code> 对象比较才能判断</p><h4 id="zookeeper-增删改查"><a href="#zookeeper-增删改查" class="headerlink" title="zookeeper 增删改查"></a>zookeeper 增删改查</h4><p>kazoo 针对 zookeeper 对 znode 的增删改查提供了优雅的 API</p><h5 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h5><p>方法:</p><ul><li><code>ensure_path()</code>：可以递归的创建节点，但不能为节点设置数据，只能设置 ACL。 </li><li><code>create()</code>：创建节点并且可以设置数据以及监视功能，它需要一个已存在的路径，也可以传入<code>makepath=True</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确认路径，如果不存在则创建</span></span><br><span class="line">zk.ensure_path(<span class="string">"/my/favorite"</span>)</span><br><span class="line"><span class="comment"># 创建节点并设置数据</span></span><br><span class="line">zk.create(<span class="string">"/my/favorite/node"</span>, <span class="string">b"a value"</span>)</span><br></pre></td></tr></table></figure><h5 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h5><p>方法:</p><ul><li><code>exists()</code>： 检查节点是否存在</li><li><code>get()</code>： 获取节点数据以及 ZnodeStat 的详细节点信息</li><li><code>get_children()</code>： 获取节点的子节点列表<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　判断节点是否存在</span></span><br><span class="line"><span class="keyword">if</span> zk.exists(<span class="string">"/my/favorite"</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印节点的数据和version</span></span><br><span class="line">data, stat = zk.get(<span class="string">"/my/favorite"</span>)</span><br><span class="line">print(<span class="string">"Version: %s, data: %s"</span> % (stat.version, data.decode(<span class="string">"utf-8"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子节点列表</span></span><br><span class="line">children = zk.get_children(<span class="string">"/my/favorite"</span>)</span><br><span class="line">print(<span class="string">"There are %s children with names %s"</span> % (len(children), children))</span><br></pre></td></tr></table></figure></li></ul><h5 id="更新数据"><a href="#更新数据" class="headerlink" title="更新数据"></a>更新数据</h5><p>方法：</p><ul><li><code>set()</code>： 为指定节点更新数据。支持<code>version</code>选项， 在更新数据前会检查<code>version</code>是否匹配，如果不匹配，则不能更新数据并抛出<code>BadVersionError</code>错误。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zk.set(<span class="string">"/my/favorite"</span>, <span class="string">b"some data"</span>)</span><br></pre></td></tr></table></figure></li></ul><h5 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h5><p>方法:</p><ul><li><code>delete()</code>: 删除节点，支持递归删除选项(recursive=True)。支持<code>version</code>选项，在删除节点前会与节点的版本匹配，不匹配不会删除且抛出<code>BadVersionError</code>错误</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zk.delete(<span class="string">"/my/favorite/node"</span>, recursive=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="重试指令"><a href="#重试指令" class="headerlink" title="重试指令"></a>重试指令</h4><p>客户端与 zookeeper 服务器的连接可能会中断，默认情况下，kazoo 不会重连，将抛出一个异常。为了帮助解决这个问题，kazoo 提供了一个 <code>retry()</code> 辅助函数，<br>如果连接跑出异常，它将重试一个指令，例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = zk.retry(zk.get, <span class="string">"/path/to/node"</span>)</span><br></pre></td></tr></table></figure><p>某些指令具有唯一性，不保证每个指令都会自动重试。例如，创建节点成功后连接丢失，这时再次重试的话会引发<code>NodeExistsError</code>错误。<br><code>retry()</code> 方法接受指令函数及其参数，因此可以将多个 zookeeper 指令传给它，以便在连接丢失的时候重试整个函数。  </p><p>下面是一个<code>kazoo</code>实现的 <code>锁</code>的例子，说明了如果重试去获得锁:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kazoo.recipe.lock snippet</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquire</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Acquire the mutex, blocking until it is obtained"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.client.retry(self._inner_acquire)</span><br><span class="line">        self.is_acquired = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">except</span> KazooException:</span><br><span class="line">        <span class="comment"># if we did ultimately fail, attempt to clean up</span></span><br><span class="line">        self._best_effort_cleanup()</span><br><span class="line">        self.cancelled = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_inner_acquire</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.wake_event.clear()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make sure our election parent node exists</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.assured_path:</span><br><span class="line">        self.client.ensure_path(self.path)</span><br><span class="line"></span><br><span class="line">    node = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> self.create_tried:</span><br><span class="line">        node = self._find_node()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.create_tried = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        node = self.client.create(self.create_path, self.data,</span><br><span class="line">            ephemeral=<span class="literal">True</span>, sequence=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># strip off path to node</span></span><br><span class="line">        node = node[len(self.path) + <span class="number">1</span>:]</span><br></pre></td></tr></table></figure><p><code>create_tried</code> 记录判断 znode 在 连接中断以前是否已经创建。</p><h5 id="自定义重试"><a href="#自定义重试" class="headerlink" title="自定义重试"></a>自定义重试</h5><p>手动创建<code>KazooRetry</code>， 可以自定义特定的重试策略</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.retry <span class="keyword">import</span> KazooRetry</span><br><span class="line"></span><br><span class="line">kr = KazooRetry(max_tries=<span class="number">3</span>, ignore_expire=<span class="literal">False</span>)</span><br><span class="line">result = kr(client.get, <span class="string">"/some/path"</span>)</span><br></pre></td></tr></table></figure><p>这个例子将最多重试指令 3 次，并且在会话到期时会抛出异常。</p><h4 id="Watcher-监视器"><a href="#Watcher-监视器" class="headerlink" title="Watcher(监视器)"></a>Watcher(监视器)</h4><p>kazoo 可以在节点上设置 watcher，节点或子节点发生变化时触发。  </p><p>可以通过两种方式设置 watcher， 第一种是 zookeeper 模式支持的一次性监听事件。不同于本机 zookeeper 监视器，这些监听函数会被 kazoo 调用一次并且不接收会话事件.<br>使用这种模式需要将监听函数传入下列某个函数:  </p><ul><li>get()</li><li>get_children()</li><li>exists()</li></ul><p>当节点变化或被删除，传入<code>get()</code> 和 <code>exists()</code>的监听函数会被调用，并传给监听函数一个<code>WatchedEvent</code>对象 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="comment"># check to see what the children are now</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Call my_func when the children change</span></span><br><span class="line">children = zk.get_children(<span class="string">"/my/favorite/node"</span>, watch=my_func)</span><br></pre></td></tr></table></figure><p>kazoo 提供了一个更高级的 API， 不需要每次触发事件使重新设置 watcher， 它不仅可以监听节点和子节点的变化，还可以查看 ZnodeStat。使用这个 API 注册<br>的监听函数在每次发生更改时都会立即调用，直到函数返回 False。 如果<code>allow_session_lost</code>设置为True，则会话丢失时将不再调用该函数。<br>下列函数提供这种功能:</p><ul><li>ChildrenWatch</li><li>DataWatch</li></ul><p>这些类可以直接在 KazooClient 实例上使用，通过实例化任返回的实例可以直接调用，也允许它们用作装饰器:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@zk.ChildrenWatch("/my/favorite/node")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_children</span><span class="params">(children)</span>:</span></span><br><span class="line">    print(<span class="string">"Children are now: %s"</span> % children)</span><br><span class="line"><span class="comment"># Above function called immediately, and from then on</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@zk.DataWatch("/my/favorite")</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_node</span><span class="params">(data, stat)</span>:</span></span><br><span class="line">    print(<span class="string">"Version: %s, data: %s"</span> % (stat.version, data.decode(<span class="string">"utf-8"</span>)))</span><br></pre></td></tr></table></figure><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>zookeeper 3.4 开始允许一次发送多个命令，这些命令将作为一个原子操作，都成功或都失败。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transaction = zk.transaction()</span><br><span class="line">transaction.check(<span class="string">'/node/a'</span>, version=<span class="number">3</span>)</span><br><span class="line">transaction.create(<span class="string">'/node/b'</span>, <span class="string">b"a value"</span>)</span><br><span class="line">results = transaction.commit()</span><br></pre></td></tr></table></figure><p><code>transaction()</code> 方法返回 <code>TransactionRequest</code> 实例，可以调用它提供的方法去排队在事务中要完成的命令。当事务准备好提交时调用<code>commit()</code>方法。<br>在上面的示例中，<code>check</code> 指令只在进行事务是有效，它可以检查 znode 的 version，如果 version 不匹配，事务就会失败。上面的例子表示:<code>只有在 /node/a 的 version 为 3 的时候，/node/b 才会被创建</code></p><h3 id="异步用法"><a href="#异步用法" class="headerlink" title="异步用法"></a>异步用法</h3><p>kazoo 异步 API 依赖所有异步方法返回的 <code>IAsyncResult</code> 对象。它可以使用 <code>rawlink()</code> 方法添加回调，像线程或gevent一样的工作方式。<br>kazoo 使用插入式的 IHandler 接口，该接口抽象回调系统以确保一致的工作</p><h4 id="处理连接-1"><a href="#处理连接-1" class="headerlink" title="处理连接"></a>处理连接</h4><p>创建连接:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.handlers.gevent <span class="keyword">import</span> SequentialGeventHandler</span><br><span class="line"></span><br><span class="line">zk = KazooClient(handler=SequentialGeventHandler())</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns immediately</span></span><br><span class="line">event = zk.start_async()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wait for 30 seconds and see if we're connected</span></span><br><span class="line">event.wait(timeout=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> zk.connected:</span><br><span class="line">    <span class="comment"># Not connected, stop trying to connect</span></span><br><span class="line">    zk.stop()</span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">"Unable to connect."</span>)</span><br></pre></td></tr></table></figure><p>在这个例子中，<code>wait()</code> 方法是 <code>start_async()</code> 方法返回的 <code>event</code> 对象上调用的。设置 <code>timeout</code> 选项可以优雅的处理无法连接的情况。<br>当使用 gevent 时应该使用 SequentialGeventHandler， 使用 eventlet 时使用 SequentialEventletHandler。kazoo 要求传入合适的处理程序，默认的处理程序是 SequentialThreadingHandler。</p><h4 id="异步回调"><a href="#异步回调" class="headerlink" title="异步回调"></a>异步回调</h4><p>kazoo 所有的异步函数除了 <code>start_async()</code> 都会返回一个 <code>IAsyncResult</code> 实例，通过这些实例可以查看结果何时就绪，或者连接一个或多个回调函数到将要就绪的结果上。<br>回调函数将传递给 <code>IAsyncResult</code> 实例并调用 <code>get()</code> 方法获取值。如果异步函数遇到错误，调用 <code>get()</code> 会抛出一个异常，应该获取并处理这个异常</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> kazoo.exceptions <span class="keyword">import</span> ConnectionLossException</span><br><span class="line"><span class="keyword">from</span> kazoo.exceptions <span class="keyword">import</span> NoAuthException</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_callback</span><span class="params">(async_obj)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        children = async_obj.get()</span><br><span class="line">        do_something(children)</span><br><span class="line">    <span class="keyword">except</span> (ConnectionLossException, NoAuthException):</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Both these statements return immediately, the second sets a callback</span></span><br><span class="line"><span class="comment"># that will be run when get_children_async has its return value</span></span><br><span class="line">async_obj = zk.get_children_async(<span class="string">"/some/node"</span>)</span><br><span class="line">async_obj.rawlink(my_callback)</span><br></pre></td></tr></table></figure><h4 id="zookeeper-CURD"><a href="#zookeeper-CURD" class="headerlink" title="zookeeper CURD"></a>zookeeper CURD</h4><p>异步curd与同步curd工作方式相同，只是返回一个 <code>IAsyncResult</code> 对象</p><h5 id="新建"><a href="#新建" class="headerlink" title="新建"></a>新建</h5><ul><li>create_async()</li></ul><h5 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h5><ul><li>exists_async()</li><li>get_async()</li><li>get_children_async()</li></ul><h5 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h5><ul><li>set_async()</li></ul><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><ul><li>delete_async()</li></ul><p><code>ensure_path()</code> 目前没有异步方法对应，<code>delete_async()</code> 方法也不能进行递归删除。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 分布式协作框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式锁(redis &amp; zookeeper)</title>
      <link href="/2018-09-10-distributed-lock.html"/>
      <url>/2018-09-10-distributed-lock.html</url>
      
        <content type="html"><![CDATA[<p>多个客户端同时对同一数据进行修改，由于读取和存储操作不是原子性的，所以就会遇到并发问题，这种场景需要用分布式锁解决。</p><a id="more"></a><h4 id="redis-分布式锁"><a href="#redis-分布式锁" class="headerlink" title="redis 分布式锁"></a>redis 分布式锁</h4><h5 id="锁操作"><a href="#锁操作" class="headerlink" title="锁操作"></a>锁操作</h5><p>redis 2.6.12 版本中加入了针对分布式锁的命令: <code>SET key value [expiration EX seconds|PX milliseconds] [NX|XX]</code></p><ul><li>EX seconds – 设置过期时间，单位秒</li><li>PX milliseconds – 设置过期时间，单位毫秒</li><li>NX – key 不存在才设置 key 的值</li><li>XX – key 已经存在才可以设置 key 的值</li></ul><p>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis&gt; set lock_name true ex 5 nx</span><br><span class="line">...... 处理业务逻辑 ......</span><br><span class="line">redis&gt; del lock_name</span><br></pre></td></tr></table></figure><h5 id="超时问题"><a href="#超时问题" class="headerlink" title="超时问题"></a>超时问题</h5><p>redis 的分布式锁机制存在超时问题，如果处理业务逻辑所用的时间超过了锁的过期时间，就会出现加锁和释放锁混乱。比如:  </p><ul><li>线程1 加锁 <code>set lock_name true ex 5 nx</code></li><li>线程1 处理业务逻辑时间大于 5s 后，redis 自动释放锁</li><li>线程2 获得锁 <code>set lock_name true ex 5 nx</code></li><li>线程2 处理业务逻辑中……线程1 业务处理完成，执行 <code>del lock_name</code></li><li>线程2 的锁没有达到超时时间就被释放，从此，锁就混乱了</li></ul><p>redis 并没有为锁的超时问题提供可靠的解决方案，所以 redis 更适合为业务逻辑简单的场景加锁，如果业务逻辑处理时间比较长，可以相应延长超时时间。对于要求非常严格的场景，可以考虑使用更加严谨的分布式锁机制，比如 zookeeper，etcd。</p><h5 id="锁冲突处理"><a href="#锁冲突处理" class="headerlink" title="锁冲突处理"></a>锁冲突处理</h5><p>高并发的场景下，客户端在请求加锁时很可能不成功，处理锁冲突有一下几种方式</p><h6 id="直接抛出特定类型的异常"><a href="#直接抛出特定类型的异常" class="headerlink" title="直接抛出特定类型的异常"></a>直接抛出特定类型的异常</h6><p>把异常反馈给客户端，由客户端用户决定是否重新连接</p><h6 id="sleep"><a href="#sleep" class="headerlink" title="sleep"></a>sleep</h6><p>sleep 的方式会阻塞当前连接，对于高并发的应用，这种并不合适</p><h6 id="延时队列"><a href="#延时队列" class="headerlink" title="延时队列"></a>延时队列</h6><p>把冲突的请求的消息加入另一个队列，另做处理</p><h4 id="zookeeper-分布式锁-kazoo"><a href="#zookeeper-分布式锁-kazoo" class="headerlink" title="zookeeper 分布式锁(kazoo)"></a>zookeeper 分布式锁(kazoo)</h4><h5 id="zookeeper-实现锁的原理"><a href="#zookeeper-实现锁的原理" class="headerlink" title="zookeeper 实现锁的原理"></a>zookeeper 实现锁的原理</h5><ul><li>设置一个 znode， 假设为 <code>/lock</code></li><li>客户端连接 zookeeper，并在<code>/lock</code>下创建临时有序(ephemeral_sequential))的子节点, 例如：第一个客户端对应的子节点为<code>/lock/lock-0000000000</code>，第二个为<code>/lock/lock-0000000001</code>，依次类推。</li><li>客户端调用<code>getChild()</code> 获取 <code>/lock</code>下的子节点列表，判断自己创建的子节点是否为所有子节点中序号最小的，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁。</li><li>执行业务代码。</li><li>完成业务流程后，删除对应的子节点释放锁。</li></ul><h5 id="独占锁-排它锁"><a href="#独占锁-排它锁" class="headerlink" title="独占锁(排它锁)"></a>独占锁(排它锁)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zk &#x3D; KazooClient()</span><br><span class="line">zk.start()</span><br><span class="line">lock &#x3D; zk.Lock(&quot;&#x2F;lockpath&quot;, &quot;my-identifier&quot;)</span><br><span class="line">with lock:  # blocks waiting for lock acquisition</span><br><span class="line">    # do something with the lock</span><br></pre></td></tr></table></figure><h5 id="共享锁"><a href="#共享锁" class="headerlink" title="共享锁"></a>共享锁</h5><p>共享锁的读取端:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zk &#x3D; KazooClient()</span><br><span class="line">zk.start()</span><br><span class="line">lock &#x3D; zk.ReadLock(&quot;&#x2F;lockpath&quot;, &quot;my-identifier&quot;)</span><br><span class="line">with lock:  # blocks waiting for outstanding writers</span><br><span class="line">    # do something with the lock</span><br></pre></td></tr></table></figure><p>共享锁的写入端:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zk &#x3D; KazooClient()</span><br><span class="line">zk.start()</span><br><span class="line">lock &#x3D; zk.WriteLock(&quot;&#x2F;lockpath&quot;, &quot;my-identifier&quot;)</span><br><span class="line">with lock:  # blocks waiting for lock acquisition</span><br><span class="line">    # do something with the lock</span><br></pre></td></tr></table></figure><h5 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import time</span><br><span class="line">from kazoo.client import KazooClient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def work(zk, path, sleep, is_lock, times):</span><br><span class="line">    lock &#x3D; zk.Lock(&quot;&#x2F;lock&quot;, &quot;incr lock&quot;)</span><br><span class="line"></span><br><span class="line">    if is_lock &#x3D;&#x3D; &quot;yes&quot;:</span><br><span class="line">        with lock:</span><br><span class="line">            _work(zk, path, sleep, times)</span><br><span class="line">    else:</span><br><span class="line">        _work(zk, path, sleep, times)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _work(zk, path, sleep, times):</span><br><span class="line">    value &#x3D; int(zk.get(path)[0])</span><br><span class="line">    get_time &#x3D; time.time()</span><br><span class="line">    time.sleep(sleep)</span><br><span class="line">    zk.set(path, str(value + 1).encode())</span><br><span class="line">    set_time &#x3D; time.time()</span><br><span class="line">    print(&quot;time:&#123;:&lt;20&#125; get:&#123;:&lt;5&#125; time:&#123;:&lt;20&#125; set:&#123;:&lt;5&#125; loop_times:&#123;:&lt;5&#125;&quot;</span><br><span class="line">          .format(get_time, value, set_time, value + 1, times))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # 为了方便观察而设置的阻塞时间</span><br><span class="line">    sleep &#x3D; 1 if len(sys.argv) &lt; 2 else int(sys.argv[1])</span><br><span class="line">    # 是否使用锁 &quot;yes&quot;&#x2F;&quot;no&quot;</span><br><span class="line">    is_lock &#x3D; &quot;yes&quot; if len(sys.argv) &gt; 2 and sys.argv[2] &#x3D;&#x3D; &quot;yes&quot; else &quot;no&quot;</span><br><span class="line"></span><br><span class="line">    # zookeeper 地址</span><br><span class="line">    hosts &#x3D; &quot;127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183&quot;</span><br><span class="line">    # 在这个节点上做自增操作</span><br><span class="line">    path &#x3D; &quot;&#x2F;data&quot;</span><br><span class="line"></span><br><span class="line">    # 启动 kazoo</span><br><span class="line">    zk &#x3D; KazooClient(hosts&#x3D;hosts)</span><br><span class="line">    zk.start()</span><br><span class="line"></span><br><span class="line">    # 初始化数据节点</span><br><span class="line">    exists &#x3D; zk.exists(path)</span><br><span class="line">    if exists is None:</span><br><span class="line">        zk.create(path, b&quot;0&quot;)</span><br><span class="line"></span><br><span class="line">    # 执行次数</span><br><span class="line">    times &#x3D; 0</span><br><span class="line">    while True:</span><br><span class="line">        times +&#x3D; 1</span><br><span class="line">        work(zk, path, sleep, is_lock, times)</span><br><span class="line"></span><br><span class="line">        if times &gt; 10 &#x2F;&#x2F; sleep:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    print(&quot;该节点当前的值:&quot;, zk.get(path)[0])</span><br><span class="line">    zk.stop()</span><br></pre></td></tr></table></figure><h6 id="观察不加锁的情况"><a href="#观察不加锁的情况" class="headerlink" title="观察不加锁的情况"></a>观察不加锁的情况</h6><p>步骤:</p><ul><li>使用 zookeeper 客户端手动设置 <code>/data</code> 为 <code>0</code>: <code>set /data 0</code></li><li>开启两个终端，传入不同的 sleep 值运行程序<br><img src="/images/zookeeper-lock-1.png" alt=""></li><li>运行结束后使用 zookeeper 客户端查看 <code>/data</code> 的值: <code>get /data</code><br><img src="/images/zookeeper-lock-2.png" alt=""></li></ul><p>观察两个终端打印的信息可以发现，一共应该自增了 11 + 6 = 17 次， 最后<code>/data</code>节点的值应该是 17 才对，这说明数据出现了不一致。</p><h6 id="观察加锁的情况"><a href="#观察加锁的情况" class="headerlink" title="观察加锁的情况"></a>观察加锁的情况</h6><p>步骤:</p><ul><li>使用 zookeeper 客户端手动设置 <code>/data</code> 为 <code>0</code>: <code>set /data 0</code></li><li>开启两个终端，传入不同的 sleep 值运行程序<br><img src="/images/zookeeper-lock-3.png" alt=""></li><li>运行结束后使用 zookeeper 客户端查看 <code>/data</code> 的值: <code>get /data</code><br><img src="/images/zookeeper-lock-4.png" alt=""></li></ul><p>观察两个终端打印的信息可以发现，一共应该自增了 11 + 6 = 17 次， 观察<code>/data</code>节点的值是正确的，再仔细观察两边 <code>get</code> 和 <code>set</code> 的时间可以发现，每当一个客户端获取到锁的时候，另一个客户端就无法获取只能等待。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 性能优化(全面)</title>
      <link href="/2018-08-26-mysql-optimization.html"/>
      <url>/2018-08-26-mysql-optimization.html</url>
      
        <content type="html"><![CDATA[<h2 id="造成性能问题的原因"><a href="#造成性能问题的原因" class="headerlink" title="造成性能问题的原因"></a>造成性能问题的原因</h2><ul><li>大量 QPS(每秒处理的查询量) 和 TPS(每秒处理的事务量)</li><li>大量并发(同一时间的请求的数量)和高CPU使用率</li><li>磁盘IO是最主要的原因,要使用更快的磁盘设备,不要在主库进行数据备份</li><li>网卡IO激增会使用户无法连接数据库,应该减少服务器的数量,建立分级缓存,做好网络规划,避免使用<code>select *</code></li><li>大表问题: 记录超过千万行或表数据超过10G, 引起慢查询,引起DDL操作延迟甚至锁表,可以用分库分表或归档历史数据的方式解决</li><li>大事务问题: 运行时间比较长,操作的数据比较多的事务, 锁定太多的数据,造成大量的阻塞和锁超时,回滚时所需时间比较大,容易造成主从延迟   </li></ul><h2 id="数据库结构设计"><a href="#数据库结构设计" class="headerlink" title="数据库结构设计"></a>数据库结构设计</h2><h3 id="范式与反范式"><a href="#范式与反范式" class="headerlink" title="范式与反范式"></a>范式与反范式</h3><h4 id="范式"><a href="#范式" class="headerlink" title="范式:"></a>范式:</h4><ul><li>1NF: 数据库表中的所有字段都只具有单一属性, 每个属性都不可再分</li><li>2NF: 要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言), 在 1NF 的基础上消除非主属性对于码的部分函数依赖</li><li>3NF: 确保数据表中的每一列数据都和主键直接相关，而不能间接相关</li></ul><h4 id="反范式"><a href="#反范式" class="headerlink" title="反范式"></a>反范式</h4><p>范式化的数据表设计,可以有效减少冗余,节省磁盘空间,但在实际应用中发现,如果严格按照范式设计表不利于数据库的读性能,而对大多数应用而言,都需要高效的读取数据.</p><p>范式化的设计在进行查询时要对多个表进行关联,且因为字段分布在不同的表中,也不能进行索引优化,所以数据表设计的难点是如果平衡处理空间和时间的矛盾,如果正确的进行反范式化设计以提高数据库的读性能</p><h3 id="选择合适的存储引擎"><a href="#选择合适的存储引擎" class="headerlink" title="选择合适的存储引擎"></a>选择合适的存储引擎</h3><table><thead><tr><th>存储引擎</th><th>事务</th><th>锁粒度</th><th>主要应用</th><th>忌用</th><th>其他</th></tr></thead><tbody><tr><td>MyISAM</td><td>不支持</td><td>支持并发插入的表级锁</td><td>非事务型应用,只读类应用</td><td>读写操作频繁</td><td>支持数据压缩(myisampack)</td></tr><tr><td>Innodb</td><td>支持</td><td>行级锁</td><td>事务处理</td><td>无</td><td>建议开启独立表空间(innodb_file_per_table=1)</td></tr><tr><td>Archive</td><td>不支持</td><td>行级锁</td><td>日志记录,只支持insert, select</td><td>需要随机读取,更新,删除</td><td>只支持在自增ID上增加索引</td></tr><tr><td>MRG_MYISAM</td><td>不支持</td><td>表级锁</td><td>分段归档,数据仓库</td><td>全局查找过多的场景</td><td></td></tr><tr><td>csv</td><td>不支持</td><td>表级锁</td><td>做为数据交换的中间表</td><td>大多数应用</td><td>数据以cs文本格式存储在文件中,所有列不能为 NULL, 不支持索引</td></tr><tr><td>memory</td><td>不支持</td><td>表级锁</td><td>用于查找或者是映射表,比如右边和地区的对应表,用于保存数据分析中的中间表,用于缓存周期性聚合数据的结果表</td><td>大多数应用</td><td>注意表中的数据丢失后无法恢复,所以所存的数据必须是可再次生成的</td></tr></tbody></table><h3 id="为字段选择合适的数据类型"><a href="#为字段选择合适的数据类型" class="headerlink" title="为字段选择合适的数据类型"></a>为字段选择合适的数据类型</h3><p>当一个列可以选择多种数据类型时,应该优先选择数字类型,其次是日期或二进制类型,最后考虑字符类型.对于相级别的数据类型,应该优先选择占用空间小的数据类型</p><p>列出一些注意点:</p><ul><li>整数类型根据取值范围合理的选择数据类型,可以节省很大空间,每种整数类型的存储空间都不同</li><li>实数类型要注意是否需要非常精确,float和double不精确,decimal精确</li><li>varchar只占用必要的存储空间,长度小于255,额外占用一个字节存长度,大于255,额外占用两个字节存长度,应使用符合需求的最小长度, 适用于最大长度比平均长度大很多的场景,且该字段很少被更新,使用了多字节字符集存储字符串</li><li>char 类型是定长的,会删除末尾的空格,最大宽度为255, 适合存储所有长度近似的值</li><li>绝不使用字符串存储日期时间</li><li>datetime类型与时区无关,占用8个字节, timestamp 类型依赖所指定的时区,占用4个字节, 用int存时间戳不如用timestamp</li><li>timestamp在行数据修改时可以自动修改为当前时间</li><li>date类型只占用3个字节,比字符串,datetime,int都小,且可以利用日期时间函数进行日期间的计算</li></ul><p><strong>整型数据大小和范围</strong><br>整型 | 大小 | 范围（无符号 unsigned）<br>—|— | —<br>TINYINT | 1 字节 | (0,255)<br>SMALLINT | 2 字节 | (0,65535)<br>MEDIUMINT | 3 字节 | (0,16777215)<br>INT/INTEGER | 4 字节 | (0,4294967295)<br>BIGINT | 8 字节 | (0,18446744073709551615)  </p><h2 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a><a href="http://waterandair.top/mysql-index-optimization.html" target="_blank" rel="noopener">索引优化</a></h2><h2 id="sql语句优化"><a href="#sql语句优化" class="headerlink" title="sql语句优化"></a>sql语句优化</h2><h3 id="找到需要优化的sql语句"><a href="#找到需要优化的sql语句" class="headerlink" title="找到需要优化的sql语句"></a><a href="http://waterandair.top/mysql-sql-optimization-steps.html" target="_blank" rel="noopener">找到需要优化的sql语句</a></h3><h3 id="sql语句优化技巧"><a href="#sql语句优化技巧" class="headerlink" title="sql语句优化技巧"></a><a href="http://waterandair.top/mysql-select-optimization.html" target="_blank" rel="noopener">sql语句优化技巧</a></h3><h2 id="MySQL服务器参数优化"><a href="#MySQL服务器参数优化" class="headerlink" title="MySQL服务器参数优化"></a>MySQL服务器参数优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;* 内存配置相关参数 详细说明 http:&#x2F;&#x2F;waterandair.top&#x2F;mysql-memory-optimization.html *&#x2F;</span><br><span class="line"># MySQL每个连接都要分配的内存</span><br><span class="line">sort_buffer_size  # 连接进行排序时候分配该配置参数大小的内存进行排序操作，比如设置为100M，如果有100个连接同时进行排序将分配10G的内存，很容易造成服务器内存溢出；</span><br><span class="line">join_buffer_size  # 定义mysql的每个线程所使用连接的缓冲区的大小，注意，如果一个查询中关联了多张表，那么就会为每个关联分配一个连接缓存，所以每个查询可能会有多个连接缓冲；</span><br><span class="line">read_buffer_size  # 对MyISAM表进行全表扫描时分配的读缓存池的大小，mysql只会在有查询需要时为该缓存分配内存，分配的内存为配置参数指定内存的大小，大小一般为4K的倍数；</span><br><span class="line">read_rnd_buffer_size  # 索引缓冲区的大小，有查询需要时才分配内存，分配的大小为需要内存的大小，而不是配置参数的大小</span><br><span class="line"></span><br><span class="line"># 为 Innodb 缓存池分配内存(非常重要,用于缓存索引,数据,延迟写入等等,需要足够大) </span><br><span class="line">Innodb_buffer_pool_size</span><br><span class="line"># 这个参数的值可以用这个公式计算: 总内存 - (每个线程所需要的内存*连接数) - 系统保留内存</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;* Innodb IO相关配置 *&#x2F;</span><br><span class="line"># 事务日志是循环使用的,使用完一个再写下一个,多个事务日志文件并不能起到并发写入的作用</span><br><span class="line">innodb_log_file_size  # 控制单个事务日志的大小</span><br><span class="line">innodb_log_files_in_group  # 控制事务日志文件的个数</span><br><span class="line">innodb_log_buffer_size  # 控制事务日志缓冲区的大小,不需要很大, 32m-128m</span><br><span class="line">innodb_flush_log_at_trx_commit  # 刷新事务日志的频繁程度,</span><br><span class="line">                                # 0: 每秒进行一次log写入cache, 并flush log 到磁盘(可能会丢失这一秒的log)</span><br><span class="line">                                # 1[默认]: 在每次事务提交执行log写入cache,并flush log到磁盘(最安全)</span><br><span class="line">                                # 2[建议]: 每次事务提交,执行log数据写入cache, 每秒执行一次flush log到磁盘</span><br><span class="line">innodb_flush_method&#x3D;O_DIRECT</span><br><span class="line">innodb_file_per_table&#x3D;1  # 启用独立表空间</span><br><span class="line">innodb_doublewrite&#x3D;1  # 启用双写缓冲</span><br><span class="line"></span><br><span class="line">&#x2F;* myisam IO相关配置 *&#x2F;</span><br><span class="line">delay_key_write  # OFF(最安全): 每次写操作后刷新缓冲中的脏块到磁盘</span><br><span class="line">                 # ON: 只对在建表时指定了 dela_key_write 选项的表使用延迟刷新</span><br><span class="line">                 # ALL: 对所有 myisam 表都使用延迟键写入</span><br><span class="line"></span><br><span class="line">&#x2F;* 安全相关配置 *&#x2F;</span><br><span class="line">exprire_logs_days  # 指定自动清理binlog的天数</span><br><span class="line">max_allowed_packet  # 控制MySQL可以接收包的大小,可以小一点 32M, 如果做了主从复制,那么主从这个配置必须一致</span><br><span class="line">skip_name_resolve  # 禁用 DNS 查找</span><br><span class="line">sysdate_is_now  # 确保 sysdate()返回确定性的日期</span><br><span class="line">read_only  # 禁止非super权限的用户写权限,在从库中启用</span><br><span class="line">skip_slave_start  # 禁止 slave 自动恢复</span><br><span class="line">sql_mode  # 设置MySQL使用的sql模式,mysql默认的模式比较宽松,如果发现同样一行sql在有的mysql服务器可以运行,有的却不可以,首先考虑是 sql_mode 设置的问题</span><br><span class="line"></span><br><span class="line">&#x2F;* 其它常用配置 *&#x2F;</span><br><span class="line">sync_binlog  # 控制 MySQL 如何向磁盘刷新 binlog, 主库建议设置为1</span><br><span class="line">tmp_table_size 和 max_heap_table_size  # 控制内存临时表大小,两个值应该相同</span><br><span class="line">max_connections  # 控制允许的最大连接数,默认100,要配置高一点, 2000</span><br></pre></td></tr></table></figure><h2 id="服务器系统参数优化"><a href="#服务器系统参数优化" class="headerlink" title="服务器系统参数优化"></a>服务器系统参数优化</h2><h3 id="内核相关参数-etc-sysctl-conf"><a href="#内核相关参数-etc-sysctl-conf" class="headerlink" title="内核相关参数 (/etc/sysctl.conf)"></a>内核相关参数 <code>(/etc/sysctl.conf)</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;* 增加系统可接受请求的数量 *&#x2F;</span><br><span class="line"># 系统中每一个端口最大的监听队列的长度，这是个全局的参数。</span><br><span class="line">net.core.somaxconn&#x3D;65535 </span><br><span class="line"># 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。</span><br><span class="line">net.core.netdev_max_backlog&#x3D;65535</span><br><span class="line"># 对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字。</span><br><span class="line">net.ipv4.tcp_max_syn_backlog&#x3D;65535</span><br><span class="line"></span><br><span class="line">&#x2F;* 加快回收 TCP 连接 *&#x2F;</span><br><span class="line"># 对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间（秒）。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。</span><br><span class="line">net.ipv4.tcp_fin_timeout&#x3D;10</span><br><span class="line"># 表示是否允许将处于TIME-WAIT状态的socket（TIME-WAIT的端口）用于新的TCP连接 。</span><br><span class="line">net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line"># 能够更快地回收TIME-WAIT套接字。</span><br><span class="line">net.ipv4.tcp_tw_recycle&#x3D;1</span><br><span class="line"></span><br><span class="line">&#x2F;* TCP 连接接收和发送数据缓冲区大小默认值的最小值和最大值,尽量调大*&#x2F;</span><br><span class="line"># 默认的TCP数据发送窗口大小（字节）。</span><br><span class="line">net.core.wmem_default&#x3D;87380</span><br><span class="line"># 最大的TCP数据发送窗口（字节）。</span><br><span class="line">net.core.wmem_max&#x3D;16777216</span><br><span class="line"># 默认的TCP数据接收窗口大小（字节）。</span><br><span class="line">net.core.rmem_default&#x3D;87380</span><br><span class="line"># 最大的TCP数据接收窗口（字节）</span><br><span class="line">net.core.rmem_max&#x3D;16777216</span><br><span class="line"></span><br><span class="line">&#x2F;* 减少失效连接占用系统资源,加快资源回收的效率, 调小一些 *&#x2F;</span><br><span class="line"># TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。</span><br><span class="line">net.ipv4.tcp_keepalive_time&#x3D;120</span><br><span class="line"># 探测消息未获得响应时，重发该消息的间隔时间（秒）。</span><br><span class="line">net.ipv4.tcp_keepalive_intvl&#x3D;30</span><br><span class="line"># 在认定TCP连接失效之前，最多发送多少个keepalive探测消息。</span><br><span class="line">net.ipv4.tcp_keepalive_probes&#x3D;3</span><br><span class="line"></span><br><span class="line">&#x2F;* 内存相关 *&#x2F;</span><br><span class="line"># linux 中最重要的参数之一,用于定义单个共享内存段的最大值,这个参数应该设置的足够大,以便能在一个共享内存段下容纳下整个Innodb缓冲池的大小,可以取物理内存-1byte</span><br><span class="line">ketnel.shmmax&#x3D;4294967295</span><br></pre></td></tr></table></figure><h3 id="增加资源限制-etc-security-limit-conf"><a href="#增加资源限制-etc-security-limit-conf" class="headerlink" title="增加资源限制 (/etc/security/limit.conf)"></a>增加资源限制 <code>(/etc/security/limit.conf)</code></h3><p>增加可打开文件数(文件描述符)的限制, <a href="http://waterandair.top/file-descriptor.html" target="_blank" rel="noopener">关于文件描述符的详细介绍</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* - nofile 65535</span><br></pre></td></tr></table></figure><h3 id="磁盘调度策略-sys-block-devname-queue-scheduler"><a href="#磁盘调度策略-sys-block-devname-queue-scheduler" class="headerlink" title="磁盘调度策略 (/sys/block/devname/queue/scheduler)"></a>磁盘调度策略 <code>(/sys/block/devname/queue/scheduler)</code></h3><p>查看: <code>cat /sys/block/sda/queue/scheduler</code>, 默认为 cfq</p><p>修改为deadline策略: echo deadline &gt; /sys/block/sda/queue/scheduler</p><h4 id="磁盘调度策略介绍"><a href="#磁盘调度策略介绍" class="headerlink" title="磁盘调度策略介绍"></a>磁盘调度策略介绍</h4><h5 id="noop-电梯式调度策略"><a href="#noop-电梯式调度策略" class="headerlink" title="noop (电梯式调度策略)"></a>noop (电梯式调度策略)</h5><p>noop 实现了一个 FIFO 队列,它像电梯的工作方式一样对 I/O 请求进行组织,当有一个新的请求到来时,它将请求合并到最近的请求之后,以此来保证请求同一介质.noop倾向饿死读而有利于写,因此noop对于闪存设备,RAM及嵌入式系统是最好的选择</p><h5 id="deadline-截止时间调度策略-数据库类应用最好的选择"><a href="#deadline-截止时间调度策略-数据库类应用最好的选择" class="headerlink" title="deadline (截止时间调度策略, 数据库类应用最好的选择)"></a>deadline (截止时间调度策略, 数据库类应用最好的选择)</h5><p>deadline 确保了在一个截止时间内服务请求,这个截止时间是可调整的,而默认读期限短于写期限,这样就防止了写操作因为不能被读取而饿死的现象.</p><h2 id="分布式架构优化"><a href="#分布式架构优化" class="headerlink" title="分布式架构优化"></a>分布式架构优化</h2><h3 id="MySQL-复制"><a href="#MySQL-复制" class="headerlink" title="MySQL 复制"></a>MySQL 复制</h3><p><a href="http://waterandair.top/mysql-replication.html" target="_blank" rel="noopener">MySQL复制原理及实现</a></p><h3 id="表分区"><a href="#表分区" class="headerlink" title="表分区"></a>表分区</h3><p><a href="http://waterandair.top/mysql-partition.html" target="_blank" rel="noopener">MySql 表分区介绍</a></p><h2 id="服务器硬件优化"><a href="#服务器硬件优化" class="headerlink" title="服务器硬件优化"></a>服务器硬件优化</h2><h3 id="选择-CPU"><a href="#选择-CPU" class="headerlink" title="选择 CPU"></a>选择 CPU</h3><p>在选择CPU前,要明确程序是计算密集型还是IO密集型,如果是计算密集型,就要选择频率更高的CPU,如果是IO密集型,就要选择核心数量更多的CPU.因为MySQL不支持对同一 sql 的并发处理, 所以多核的 CPU 对于 sql 执行速度并没有帮助.但是MySQL的应用一般会要求并发量要高,CPU核心数越多,并发能力就越强 ,所以在资金有限的情况下,优先选择核心数量更多的CPU</p><h3 id="选择内存"><a href="#选择内存" class="headerlink" title="选择内存"></a>选择内存</h3><p>把数据缓存到内存中可以提高 MySQL 的性能, 常用的 MySQL 引擎中, MyISAM将索引缓存到内存中,数据通过操作系统进行缓存;InnoDB 中会同时在内存中缓存索引和数据.选择内存时,应该选择主板支持额最大内存频率,尽量购买相同品牌,颗粒,频率,电压,校验技术和型号的内存,尽可能买大内存</p><h3 id="配置和选择磁盘"><a href="#配置和选择磁盘" class="headerlink" title="配置和选择磁盘"></a>配置和选择磁盘</h3><h4 id="使用传统机器硬盘"><a href="#使用传统机器硬盘" class="headerlink" title="使用传统机器硬盘"></a>使用传统机器硬盘</h4><p>最常见,使用最多,价格低,存储空间大,读写慢</p><p>选择传统硬盘主要考虑以下几个因素:</p><ol><li>存储容量</li><li>传输速度</li><li>访问时间</li><li>主轴转速</li><li>物理尺寸(同等条件下,尺寸越小越好)</li></ol><h5 id="传统硬盘读取过程-2用的时间称为访问时间-3的过程称为传输速度"><a href="#传统硬盘读取过程-2用的时间称为访问时间-3的过程称为传输速度" class="headerlink" title="传统硬盘读取过程(2用的时间称为访问时间,3的过程称为传输速度):"></a>传统硬盘读取过程(2用的时间称为访问时间,3的过程称为传输速度):</h5><ol><li>移动磁头到磁盘表面上正确的位置</li><li>等待磁盘旋转,使所需的数据在磁头之下</li><li>等待磁盘旋转过去,所有所需的数据都被磁头读出</li></ol><h4 id="使用-RAID-增强传统机硬盘的性能"><a href="#使用-RAID-增强传统机硬盘的性能" class="headerlink" title="使用 RAID 增强传统机硬盘的性能"></a>使用 RAID 增强传统机硬盘的性能</h4><blockquote><p>RAID, 是磁盘冗余队列的简称,RAID的作用就是可以把多个容量较小的磁盘组成一组容量更大的磁盘,并提供数据冗余来保证数据完成性</p></blockquote><h5 id="RAID-级别"><a href="#RAID-级别" class="headerlink" title="RAID 级别:"></a>RAID 级别:</h5><ul><li>RAID0 :又称数据条带,是最简单的一种形式,只需要2块以上的硬盘,成本最低,但没有冗余和错误修复能力</li><li>RAID1: 又称磁盘镜像,在写入一块磁盘的同时,会在另一块闲置的磁盘生成镜像文件,在不影响性能的前提下最大限度的保证系统的可靠性和可修复性</li><li>RAID5: 又称分布式奇偶校验磁盘阵列,通过分布式奇偶校验块,把数据分散到多个磁盘上,这样如果一个盘数据失效,都可以从奇偶校验块中重建.适合用以读操作为主的场景.最好用在从服务器上.</li><li>RAID10(最常用): 由称分片的镜像,对磁盘先做 RAID1 之后对两组 RAID1 的磁盘再做 RAID0,所以读写都有良好的性能,相对于 RAID5 更简单,速度也更快</li></ul><p>等级 | 特点 | 是否冗余 | 盘数 | 读 | 写<br>—|— | — | — | — | — | —<br>RAID0 | 便宜, 快速,危险| 否 | N | 快 | 快<br>RAID1 | 高速读,简单,安全| 是 | 2 | 快 | 慢<br>RAID5 | 安全,成本折中| 是 | N + 1 | 快 | 取决于最慢的盘<br>RAID10 | 贵,高速,安全| 有 | 2N | 快 | 快</p><h4 id="使用固态硬盘-SSD-和-PCIe-卡"><a href="#使用固态硬盘-SSD-和-PCIe-卡" class="headerlink" title="使用固态硬盘 SSD 和 PCIe 卡"></a>使用固态硬盘 SSD 和 PCIe 卡</h4><p>相比机械硬盘,固态硬盘有更好的随机读写性能,能更好的支持并发,但更容易损坏,适用于存在大量随机 I/O 的场景, 用于解决单线程负载的 IO 瓶颈</p><table><thead><tr><th>类别</th><th>特点</th></tr></thead><tbody><tr><td>SSD</td><td>使用 SATA 接口,可以替换传统磁盘而不需任何改变,且支持 RAID 技术</td></tr><tr><td>PCIe卡</td><td>无法使用 SATA 接口,需要独特的驱动和配置,价格更贵,性能更好</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 数据类型及应用场景</title>
      <link href="/2018-08-11-redis-basic.html"/>
      <url>/2018-08-11-redis-basic.html</url>
      
        <content type="html"><![CDATA[<p>总结redis常用数据类型</p><a id="more"></a><h3 id="redis-特点"><a href="#redis-特点" class="headerlink" title="redis 特点"></a>redis 特点</h3><ul><li>数据存储在内存中，高速效率高</li><li>提供丰富的数据类型：string、 hash、 set、 sorted set、bitmap、hyperloglog</li><li>Redis 的所有操作都是原子性的，还支持通过对几个操作合并后的原子性操作，支持事务  </li><li>提供了可靠的备份恢复和集群功能，适用于高并发场景</li></ul><p>　通常我们都把数据存到关系型数据库中，但为了提升应用的性能，我们应该把访频率高且不会经常变动的数据缓存到内存中。Redis 没有像 MySQL 这类关系型数据库那样强大的查询功能，需要考虑如何把关系型数据库中的数据，合理的对应到缓存的 key-value 数据结构中。  </p><h3 id="设计-Redis-Key"><a href="#设计-Redis-Key" class="headerlink" title="设计 Redis Key"></a>设计 Redis Key</h3><h4 id="分段设计法"><a href="#分段设计法" class="headerlink" title="分段设计法"></a>分段设计法</h4><p>　　使用冒号把 key 中要表达的多种含义分开表示，步骤如下：</p><ol><li>把表名转化为 key 前缀</li><li>主键名（或其他常用于搜索的字段）</li><li>主键值</li><li>要存储的字段。  </li></ol><p>eg. 用户表（user）</p><table><thead><tr><th>id</th><th>name</th><th>email</th></tr></thead><tbody><tr><td>1</td><td>zj</td><td><a href="mailto:156577812@qq.com">156577812@qq.com</a></td></tr><tr><td>2</td><td>ai</td><td><a href="mailto:156577813@qq.com">156577813@qq.com</a></td></tr></tbody></table><p>这个简单的表可能经常会有这个的需求：&gt;根据用户 id 查询用户邮箱地址，可以选择把邮箱地址这个数据存到 redis 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set user:id:1:email 156577812@qq.com;</span><br><span class="line">set user:id:2:email 156577812@qq.com;</span><br></pre></td></tr></table></figure><h3 id="String数据类型的应用场景"><a href="#String数据类型的应用场景" class="headerlink" title="String数据类型的应用场景"></a>String数据类型的应用场景</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>　　string 类型是 Redis 中最基本的数据类型，最常用的数据类型，甚至被很多玩家当成 redis 唯一的数据类型去使用。string 类型在 redis 中是二进制安全(binary safe)的,这意味着 string 值关心二进制的字符串，不关心具体格式，你可以用它存储 json 格式或 JPEG 图片格式的字符串。  　　</p><h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>　　string 类型是基本的 Key-Value 结构，Key 是某个数据在 Redis 中的唯一标识，Value 是具体的数据。</p><table><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>‘name’</td><td>‘redis’</td></tr><tr><td>‘type’</td><td>‘string’</td></tr></tbody></table><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><h5 id="存储-MySQL-中某个字段的值"><a href="#存储-MySQL-中某个字段的值" class="headerlink" title="存储 MySQL 中某个字段的值"></a>存储 MySQL 中某个字段的值</h5><p>把 key 设计为 表名：主键名：主键值：字段名<br>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set user:id:1:email 156577812@qq.com</span><br></pre></td></tr></table></figure><h5 id="存储对象"><a href="#存储对象" class="headerlink" title="存储对象"></a>存储对象</h5><p>string 类型支持任何格式的字符串，应用最多的就是存储 json 或其他对象格式化的字符串。(这种场景下推荐使用 hash 数据类型)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set user:id:1 &#39;[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;zj&quot;,&quot;email&quot;:&quot;156577812@qq.com&quot;&#125;,&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;zj&quot;,&quot;email&quot;:&quot;156577812@qq.com&quot;&#125;]&#39;</span><br></pre></td></tr></table></figure><h5 id="生成自增-id"><a href="#生成自增-id" class="headerlink" title="生成自增 id"></a>生成自增 id</h5><p>当 redis 的 string 类型的值为整数形式时，redis 可以把它当做是整数一样进行自增（incr）自减（decr）操作。由于 redis 所有的操作都是原子性的，所以<code>不必担心多客户端连接时可能出现的事务</code>问题。  </p><h3 id="hash-数据类型的应用场景"><a href="#hash-数据类型的应用场景" class="headerlink" title="hash 数据类型的应用场景"></a>hash 数据类型的应用场景</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>　　hash 类型很像一个关系型数据库的数据表，hash 的 Key 是一个唯一值，Value 部分是一个 hashmap 的结构。　　</p><h4 id="数据模型-1"><a href="#数据模型-1" class="headerlink" title="数据模型"></a>数据模型</h4><p>　　假设有一张数据库表如下：  </p><table><thead><tr><th>id</th><th>name</th><th>type</th></tr></thead><tbody><tr><td>1</td><td>redis</td><td>hash</td></tr></tbody></table><p>　　如果要用 redis 的 hash 结构存储，数据模型如下：  </p><p><img src="/images/redis/redis-basic-1.png" alt="hash"></p><p>　　hash 数据类型在存储结构化信息时具有比 string 类型更灵活、更快的优势。具体的说，使用 string 类型存储，必然需要转换和解析 json 格式的字符串。</p><h5 id="使用-string-还是-hash"><a href="#使用-string-还是-hash" class="headerlink" title="使用 string 还是 hash"></a>使用 string 还是 hash</h5><p>如果在大多数访问中，只使用单个字段，且明确知道哪些字段是可用的，就可以考虑使用 hash 结构。<br>如果在大多数访问中，使用了大部分字段，就更倾向与使用 string 结构。　　</p><h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><p>hash 类型十分适合存储对象类数据，相对于在 string 中介绍的把对象转化为 json 字符串存储，hash 的结构可以任意添加或删除‘字段名’，更加高效灵活。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hmset user:1 name zj email 156577812@qq.com</span><br></pre></td></tr></table></figure><h3 id="list-数据类型的应用场景"><a href="#list-数据类型的应用场景" class="headerlink" title="list 数据类型的应用场景"></a>list 数据类型的应用场景</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>　　list 是按照插入顺序排序的字符串链表，可以在头部和尾部插入新的元素（双向链表实现，两端添加元素的时间复杂度为 O(1)）。插入元素时，如果 key 不存在，redis 会为该 key 创建一个新的链表，如果链表中所有的元素都被移除，该 key 也会从 redis 中移除。</p><h4 id="数据模型-2"><a href="#数据模型-2" class="headerlink" title="数据模型"></a>数据模型</h4><p><img src="/images/redis/redis-basic-2.png" alt="clipboard.png"></p><p>　　常见操作时用 lpush 命令在 list 头部插入元素， 用 rpop 命令在 list 尾取出数据。  　　</p><h4 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h4><h5 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h5><p>　　redis 的 list 数据类型对于大部分使用者来说，是实现队列服务的最经济，最简单的方式。  </p><p>redis 实现消息队列可以使用 <code>rpush &amp; lpop</code> 或 <code>lpush &amp; rpop</code>,考虑到队列为空时客户端会陷入死循环，可以使用 <code>blpop/brpop</code> 代替 <code>lpop/rpop</code>，<code>blpop/brpop</code> 会使连接阻塞变成闲置连接，闲置久了，服务器会自动断开并跑出异常，客户端要捕获异常并重试。　　</p><h5 id="“最新内容”"><a href="#“最新内容”" class="headerlink" title="“最新内容”"></a>“最新内容”</h5><p>因为 list 结构的数据查询两端附近的数据性能非常好，所以适合一些需要获取最新数据的场景，比如新闻类应用的 “最近新闻”。　　</p><h4 id="优化建议"><a href="#优化建议" class="headerlink" title="优化建议"></a>优化建议</h4><h5 id="list-是链表结构，所有如果在头部和尾部插入数据，性能会非常高，不受链表长度的影响；但如果在链表中插入数据，性能就会越来越差。"><a href="#list-是链表结构，所有如果在头部和尾部插入数据，性能会非常高，不受链表长度的影响；但如果在链表中插入数据，性能就会越来越差。" class="headerlink" title="list 是链表结构，所有如果在头部和尾部插入数据，性能会非常高，不受链表长度的影响；但如果在链表中插入数据，性能就会越来越差。"></a>list 是链表结构，所有如果在头部和尾部插入数据，性能会非常高，不受链表长度的影响；但如果在链表中插入数据，性能就会越来越差。</h5><h3 id="set-数据类型的应用场景"><a href="#set-数据类型的应用场景" class="headerlink" title="set 数据类型的应用场景"></a>set 数据类型的应用场景</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>　　set 数据类型是一个集合（没有排序，不重复），可以对 set 类型的数据进行添加、删除、判断是否存在等操作（时间复杂度是 O(1) ）<br>　　set 集合不允许数据重复，如果添加的数据在 set 中已经存在，将只保留一份。<br>　　set 类型提供了多个 set 之间的聚合运算，如求交集、并集、补集，这些操作在 redis 内部完成，效率很高。</p><h4 id="数据模型-3"><a href="#数据模型-3" class="headerlink" title="数据模型"></a>数据模型</h4><p><img src="/images/redis/redis-basic-4.png" alt="set"></p><h4 id="应用场景-3"><a href="#应用场景-3" class="headerlink" title="应用场景"></a>应用场景</h4><p>　　set 类型的特点是——不重复且无序的一组数据，并且具有丰富的计算功能，在一些特定的场景中可以高效的解决一般关系型数据库不方便做的工作。</p><h5 id="“共同好友列表”"><a href="#“共同好友列表”" class="headerlink" title="“共同好友列表”"></a>“共同好友列表”</h5><p>　　社交类应用中，获取两个人或多个人的共同好友，两个人或多个人共同关注的微博这样类似的功能，用 MySQL 的话操作很复杂，可以把每个人的好友 id 存到集合中，获取共同好友的操作就可以简单到一个取交集的命令就搞定。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这里为了方便阅读，把 id 替换成姓名</span></span><br><span class="line">sadd user:wade james melo paul kobe</span><br><span class="line">sadd user:james wade melo paul kobe</span><br><span class="line">sadd user:paul wade james melo kobe</span><br><span class="line">sadd user:melo wade james paul kobe</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取 wade 和 james 的共同好友</span></span><br><span class="line">sinter user:wade user:james</span><br><span class="line"><span class="comment">/* 输出：</span></span><br><span class="line"><span class="comment"> *      1) "kobe"</span></span><br><span class="line"><span class="comment"> *      2) "paul"</span></span><br><span class="line"><span class="comment"> *      3) "melo"</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">// 获取香蕉四兄弟的共同好友</span></span><br><span class="line"> sinter user:wade user:james user:paul user:melo</span><br><span class="line"> <span class="comment">/* 输出：</span></span><br><span class="line"><span class="comment"> *      1) "kobe"</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">/*</span></span><br><span class="line"><span class="comment">     类似的需求还有很多 , 必须把每个标签下的文章 id 存到集合中，可以很容易的求出几个不同标签下的共同文章；</span></span><br><span class="line"><span class="comment"> 把每个人的爱好存到集合中，可以很容易的求出几个人的共同爱好。 </span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><h3 id="zset-数据类型的应用场景"><a href="#zset-数据类型的应用场景" class="headerlink" title="zset 数据类型的应用场景"></a>zset 数据类型的应用场景</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>　　在 set 的基础上给集合中每个元素关联了一个分数，往有序集合中插入数据时会自动根据这个分数排序。  </p><h4 id="应用场景-4"><a href="#应用场景-4" class="headerlink" title="应用场景"></a>应用场景</h4><p>　　在集合类型的场景上加入排序就是有序集合的应用场景了。比如根据好友的“亲密度”排序显示好友列表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 用元素的分数（score）表示与好友的亲密度</span><br><span class="line">zadd user:kobe 80 james 90 wade  85 melo  90 paul</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 根据“亲密度”给好友排序</span><br><span class="line">zrevrange user:kobe 0 -1</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 输出：</span><br><span class="line"> *      1) &quot;wade&quot;</span><br><span class="line"> *      2) &quot;paul&quot;</span><br><span class="line"> *      3) &quot;melo&quot;</span><br><span class="line"> *      4) &quot;james&quot;</span><br><span class="line"> *&#x2F;</span><br><span class="line"> </span><br><span class="line">&#x2F;&#x2F; 增加好友的亲密度</span><br><span class="line">zincrby user:kobe 15 james</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 再次根据“亲密度”给好友排序</span><br><span class="line">zrevrange user:kobe 0 -1</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 输出：</span><br><span class="line"> *      1) &quot;james&quot;</span><br><span class="line"> *      2) &quot;wade&quot;</span><br><span class="line"> *      3) &quot;paul&quot;</span><br><span class="line"> *      2) &quot;melo&quot;</span><br><span class="line"> *&#x2F;</span><br><span class="line"> </span><br><span class="line"> &#x2F;&#x2F;类似的需求还出现在根据文章的阅读量或点赞量对文章列表排序</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle 性能调优</title>
      <link href="/2018-07-19-spark-shuffle-optimization.html"/>
      <url>/2018-07-19-spark-shuffle-optimization.html</url>
      
        <content type="html"><![CDATA[<p>Spark 开发中对性能消耗最常见的地方就是 Shuffle 操作，本文介绍 Shuffle 过程原理以及三个常用的优化手段。</p><a id="more"></a><h3 id="shuffle-原理"><a href="#shuffle-原理" class="headerlink" title="shuffle 原理"></a>shuffle 原理</h3><h4 id="shuffle-过程"><a href="#shuffle-过程" class="headerlink" title="shuffle 过程"></a>shuffle 过程</h4><p>以 reduceByKey(a+b) 为例：</p><ul><li><p>当某个 action 触发 job 的时候,DAGScheduler 就会会负责划分 job 为多个 stage.划分的依据是:如果发现会触发shuffle操作的<br>算子,比如 reduceByKey.就将这个操作的前半部分,以及之前所有的RDD和transformation 操作,划分为一个 stage;shuffle 操作的后半部分,以及后面的<br>直到action 为止的 RDD 和 transformation 操作,划分为另一个 stage.</p></li><li><p>每一个 shuffle 的前半部分 stage 的 task, 都会创建下一个 stage 的 task 数量相同的文件。假设下一个 stage 会有 100 个 task.那么当前 stage 每个 task 都会创建 100 份文件, 对应的 values 写入下一个 stage 同一个 task 对应的文件中.(shuffle前半部分的task在写入数据到磁盘文件之前,都会先写入一个一个的内存缓冲,内存缓冲<br>满溢之后,再 spill 到磁盘文件中)</p></li><li><p>shuffle 的后半部分 stage 的 task, 每个 task 都会从各个节点上的 task 写的属于自己的那一份文件中,拉取 key, value对,然后 task 会有一个内<br>存缓冲区,会用 hashMap 进行key, values的汇聚。task 会用自定义的聚合函数,比如 reduceByKey(<em>+</em>),把所有 values 进行一对一的累加,聚合出来最终的值,就完成了 shuffle</p></li></ul><h4 id="会发生-shuffle-的算子"><a href="#会发生-shuffle-的算子" class="headerlink" title="会发生 shuffle 的算子"></a>会发生 shuffle 的算子</h4><p><strong>repartition操作：</strong>  repartition 和 coalesce<br><strong>ByKey操作：</strong> groupByKey 和 reduceByKey<br><strong>join操作：</strong> cogroup 和 join</p><h3 id="shuffle-优化"><a href="#shuffle-优化" class="headerlink" title="shuffle 优化"></a>shuffle 优化</h3><h4 id="合并-map-端输出文件"><a href="#合并-map-端输出文件" class="headerlink" title="合并 map 端输出文件"></a>合并 map 端输出文件</h4><h5 id="默认不合并的情况"><a href="#默认不合并的情况" class="headerlink" title="默认不合并的情况"></a>默认不合并的情况</h5><ul><li>第一个 stage 的每个 task, 都会给第二个 stage 的每个 task 创建一份 map 端的输出文件  </li><li>第二个 stage 的每个 task, 都会到各个节点上面去, 拉取第一个 stage 每个 task 输出的属于自己的那一份文件  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">假设生产环境：</span><br><span class="line">    有 100 个节点(每个节点一个 executor)，共100 个 executor</span><br><span class="line">    每个 executor 2 个 CPUcore</span><br><span class="line">    总共 1000 个task，平均每个 executor 10个 task</span><br><span class="line"></span><br><span class="line">shuffle 前半部分:</span><br><span class="line">    每个节点, 10个 task, 每个节点会输出的map端文件数量:</span><br><span class="line">        10 * 1000 &#x3D; 10000 个文件</span><br><span class="line">    总共要输出的文件数量:</span><br><span class="line">        100 * 10000 &#x3D; 100万</span><br></pre></td></tr></table></figure><h5 id="合并map端文件的情况"><a href="#合并map端文件的情况" class="headerlink" title="合并map端文件的情况"></a>合并map端文件的情况</h5></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new SparkConf().set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure><p>开启了 map 端输出文件的合并机制之后:  </p><ul><li>第一个 stage 同时只运行 CPUcore 个 task,比如 CPUcore 是 2 个,并行运行 2 个 task,每个task 都创建下一个 stage 的 task 数量个文件  </li><li>第二个 stage, task 再拉取数据的时候,拉取少量的输出文件,每个输出文件中,可能<br>包含了多个 task 给自己的 map 端输出 </li></ul><p>并行执行的 task 会创建出新的输出文件，下一批并行执行的 task 会去复用之前的已有的输出文件。<br>如果 2 个 task 并行执行,但此时又启动执行 2 个task，就无法复用刚才的 2 个 task 创建的输出文件了，只能去创建新的输出文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">假设生产环境：</span><br><span class="line">    有 100 个节点(每个节点一个 executor)，共100 个 executor</span><br><span class="line">    每个 executor 2 个 CPUcore</span><br><span class="line">    总共 1000 个task，平均每个 executor 10个 task</span><br><span class="line"></span><br><span class="line">shuffle 前半部分:</span><br><span class="line">    每个节点 2 个 cpucore,输出文件的数量是:</span><br><span class="line">        2*1000 &#x3D; 2000个</span><br><span class="line">    总共要输出的文件数量:</span><br><span class="line">        100 * 2000 &#x3D; 20 万</span><br></pre></td></tr></table></figure><p>相较之前未开启合并机制的时候,数量是开启合并机制的 5 倍</p><h5 id="优化说明"><a href="#优化说明" class="headerlink" title="优化说明"></a>优化说明</h5><p>shuffle 中的写操作是 shuffle 中性能消耗最为严重的部分,通过上面的分析,一个普通的生产环境的一个 shuffle 环节,会写入磁盘 100 万个文件, spark 作业的性能都消耗在 shuffle 中了.</p><p><strong>开启了map端文件合并后：</strong></p><ul><li>map task 写入磁盘文件的 IO 减少</li><li>第二个 stage,原本要拉取第一个 stage 的 task 数量份的文件数量(1000份);合并之后,100个节点,每个节点 2个 CPUcore,第二个 stage<br>的每个 task,主要拉取 100 * 2 = 200 个文件即可,网路传输的效率也大大提高</li></ul><h4 id="调整map端内存缓冲与reduce端内存占比"><a href="#调整map端内存缓冲与reduce端内存占比" class="headerlink" title="调整map端内存缓冲与reduce端内存占比"></a>调整map端内存缓冲与reduce端内存占比</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p>默认情况下,shuffle 的 map task 输出到磁盘文件的时,都会先写入每个 task 自己关联的一个默认 32kb 的内存缓冲区。当内存缓冲区满了之后,才会进行 spill 操作写到磁盘文件中去.<br>如果 map task 处理的数据量比较大，就会频繁进行 spill 操作，消耗磁盘IO </p><p>reduce 端 task,在拉取到数据之后,会用 hashmap 的数据格式对各个 key 对应的 values 进行汇聚。针对每个 key 对应的 values 执行自定义的聚合函数的代码,比如 <code>(_ + _)把所有values累加起来</code>。<br>reduce task 在进行汇聚聚合等操作的时候,使用的内存是对应的 executor 内存按照默认<code>0.2</code>的比例划分给 reduce task的，所以很有可能拉取过来的数据在内存中放不下,不得不将放不下的数据都 spill 到磁盘文件中去。<br>如果 reduce 端拉取过来的数据量过大，内存就会不够用，就会造成频繁的 spill 操作，消耗磁盘IO，同时会造成后续操作大量的磁盘读取，也很消耗磁盘IO</p><h5 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.file.buffer&#x3D;32kb</span><br><span class="line">spark.shuffle.memoryFraction&#x3D;0.2</span><br></pre></td></tr></table></figure><p><strong>观察 Spark UI:</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">standalone 模式:  </span><br><span class="line">    点击 job 地址可以看到每个 stage 的详情,包括有哪些 executor，task 以及每个 task 的 shuffle 操作读写磁盘和内存的数据量.</span><br><span class="line">yarn模式: </span><br><span class="line">    从 yarn 界面进去,点击对应的 application 进入 spark UI,可以查看详情</span><br></pre></td></tr></table></figure><p>如果发现 shuffle 磁盘的读写量很大,就最好调节一下 shuffle 的参数，比如将<br>spark.shuffle.file.buffer 每次扩大一倍, spark.shuffle.memoryFraction 每次提高 0.1, 观察效果。  </p><p>注意不能调节的过大,因为内存资源是有限的,这里调节的过大,其他环节的内存使用就会出问题了</p><h4 id="选择合适的shuffleManager"><a href="#选择合适的shuffleManager" class="headerlink" title="选择合适的shuffleManager"></a>选择合适的shuffleManager</h4><h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h5><ol><li>SortShuffleManager 会对每个 reduce task 要处理的数据,进行排序(默认)</li><li>SortShuffleManager 会避免像 HashShuffleManager 那样默认就去创建多份磁盘文件.一个task,只会写入一个磁盘文件,不同 reduce task的数据,<br>用 offset 来划分界定</li><li>tungsten-sort Manager 自己实现了一套内存管理机制,性能上有很大提升且可以避免 shuffle 过程中产生的大量 OOM，GC.</li></ol><p>注意: consolidateFiles机制、map端缓冲、reduce端内存占比等优化方式，对任何shuffle manager都是有用的。</p><h5 id="shuffleManager-的选择"><a href="#shuffleManager-的选择" class="headerlink" title="shuffleManager 的选择"></a>shuffleManager 的选择</h5><ol><li>如果不需要让数据排序，建议就使用最基本的 HashShuffleManager</li><li>如果需要数据按 key 排序就选择 SortShuffleManager,注意 reduce task 数量要超过 200 ,这个 sort merge(多个文件合并成一个)的机制才能生效.</li><li>如果不排序,希望每个task输出的文件最终会是合并成一份的,可以去调节 spark.shuffle.sort.bypassMergeThreshold,比如 reduce task 数量是<br>500, 默认阈值是 200,所以默认还是会进行 sort 和直接 merge 的,可以将阈值调节成 550,不会进行sort,按照 hash 的做法,每个 reduce task 创建<br>一份输出文件,最后合并成一份文件(通常很少调节这个参数)</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理）</span><br><span class="line">spark.shuffle.sort.bypassMergeThreshold：200</span><br><span class="line"># 自己可以设定一个阈值，默认是200，当reduce task数量少于等于200，map task创建的输出文件小于等于200的时候，最后会将所有的输出文件合并为一份文件。</span><br><span class="line"># 这样做的好处是避免了sort排序，节省了性能开销。而且还能将多个reduce task的文件合并成一份文件。节省了reduce task拉取数据的时候的磁盘IO的开销。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 性能调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 中的常用工具</title>
      <link href="/2018-07-08-mysql-commonly-used-tools.html"/>
      <url>/2018-07-08-mysql-commonly-used-tools.html</url>
      
        <content type="html"><![CDATA[<h3 id="mysql-（客户端连接工具）"><a href="#mysql-（客户端连接工具）" class="headerlink" title="mysql （客户端连接工具）"></a>mysql （客户端连接工具）</h3><p>使用最频繁的连接数据库的客户端工具，使用语法如下：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql [options] [database]</span><br></pre></td></tr></table></figure><p>这里的 options 表示 mysql 的可用选项，可以一次写一个或者多个，甚至可以不写；database 表示连接的数据库，一次只能写一个或者不写，如果不写，连接成功后需要用 “use database”命令来进入要操作的数据库。  </p><h4 id="连接选项"><a href="#连接选项" class="headerlink" title="连接选项"></a>连接选项</h4><p>选项的表达方式有多种，例如：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 这三种方式都是可以的</span><br><span class="line">shell&gt; mysql -u root</span><br><span class="line">shell&gt; mysql -uroot</span><br><span class="line">shell&gt; mysql -user&#x3D;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-u, --user&#x3D;name    指定用户名</span><br><span class="line">-p, --password[&#x3D;name]    指定密码  </span><br><span class="line">-h, --host&#x3D;name    指定服务器 IP 或者域名  </span><br><span class="line">-P, --port&#x3D;#    指定连接端口</span><br></pre></td></tr></table></figure><p>一般在在本地环境，为了方便，可以在 配置文件 my.cnf 中配置当前用户和密码，配置好后，直接执行 mysql 就可以连接到数据库：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">user&#x3D;root</span><br><span class="line">password&#x3D;000000</span><br><span class="line">port &#x3D; 3306</span><br><span class="line">socket &#x3D; &#x2F;tmp&#x2F;mysql.sock</span><br><span class="line">default-character-set &#x3D; utf8mb4</span><br></pre></td></tr></table></figure><p>配置好后，直接执行 mysql 即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:~$ mysql</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 19</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>登录远程服务器，需要指定地址和端口：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysql -h 192.168.10.10 -P 3306 -uroot -p</span><br></pre></td></tr></table></figure><p>** 注意： **在正式的生产环境中，为了安全起见，一般需要创建应用账号并赋予适当权限，而不会用 root 直接操作数据库；默认端口（3306）一般不要使用，可以改为任意操作系统未占用的端口。  </p><h4 id="客户端字符集选项"><a href="#客户端字符集选项" class="headerlink" title="客户端字符集选项"></a>客户端字符集选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--default-character-set&#x3D;charset-name</span><br></pre></td></tr></table></figure><p>作为服务器的字符集选项，这个选项也可以配置在 my.cnf 的 [mysqld] 组中。同样，作为客户端字符集选项，也可以配置在 my.cnf 的 [mysql]组中，这样每次用 mysql 工具连接数据库的时候就会自动使用此客户端字符集。当然，也可以在 mysql 的命令行中手工指定客户端字符集：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysql -u user -default-character-set&#x3D;charset</span><br></pre></td></tr></table></figure><p>相当于在 mysql 客户端连接成功后执行：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set names charset;</span><br></pre></td></tr></table></figure><h4 id="执行选项"><a href="#执行选项" class="headerlink" title="执行选项"></a>执行选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-e, --execute&#x3D;name        &#x2F;&#x2F; 执行 sql 语句并退出</span><br></pre></td></tr></table></figure><p>此选项可以直接在 MySQL 客户端执行 sql 语句，对于一些批处理脚本，这是方式尤其方便：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:~$ mysql mysql -e &quot;select user,host from user&quot;</span><br><span class="line">+-----------+-----------+</span><br><span class="line">| user      | host      |</span><br><span class="line">+-----------+-----------+</span><br><span class="line">| root      | 127.0.0.1 |</span><br><span class="line">| mysql.sys | localhost |</span><br><span class="line">| root      | localhost |</span><br><span class="line">+-----------+-----------+</span><br></pre></td></tr></table></figure><p>可以按这种方式连续执行多个 sql 语句，用英文分号（；）隔开。  </p><h4 id="格式化选项"><a href="#格式化选项" class="headerlink" title="格式化选项"></a>格式化选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-E, --vertical    将输出方式按照字段顺序竖着显示  </span><br><span class="line">-s, --silent    去掉 mysql 中的线条框显示</span><br></pre></td></tr></table></figure><p>“-E” 选项类似于 mysql 里面执行 sql 语句后加 “\G”, 经常和 -e 一起使用。  </p><h3 id="myisampack-（myisam-表压缩工具）"><a href="#myisampack-（myisam-表压缩工具）" class="headerlink" title="myisampack （myisam 表压缩工具）"></a>myisampack （myisam 表压缩工具）</h3><p>myisampack 是一个表压缩工具，可以使用很高的压缩率来对 myisam 存储引擎的表进行压缩，使得压缩后的表占用比压缩前小得多的空间。但是压缩后的表将成为一个只读表，不能进行 DML 操作。</p><h3 id="mysqladmin（MySQL-管理工具）"><a href="#mysqladmin（MySQL-管理工具）" class="headerlink" title="mysqladmin（MySQL 管理工具）"></a>mysqladmin（MySQL 管理工具）</h3><p>mysqladmin 是一个执行管理操作的客户端程序。可以用它来检查服务器的配置和当前状态、创建并删除数据库等。它的功能与 mysql 客户端非常类似，主要区别在于它更侧重于一些管理方面的功能。  </p><p>使用语法：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqladmin [options] command [command-options]...</span><br></pre></td></tr></table></figure><p>可以执行的命令如下：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">create databasename    Create a new database    新建数据库</span><br><span class="line">debug            Instruct server to write debug information to log     把 debug 日志记录到日志文件中</span><br><span class="line">drop databasename        Delete a database and all its tables    删除数据库</span><br><span class="line">extended-status           Gives an extended status message from the server    查看 MySQL 服务器的状态信息</span><br><span class="line">flush-hosts               Flush all cached hosts</span><br><span class="line">flush-logs                Flush all logs</span><br><span class="line">flush-status        Clear status variables</span><br><span class="line">flush-tables              Flush all tables</span><br><span class="line">flush-threads             Flush the thread cache</span><br><span class="line">flush-privileges          Reload grant tables (same as reload)</span><br><span class="line">kill id,id,...        Kill mysql threads</span><br><span class="line">password [new-password]   Change old password to new-password in current format</span><br><span class="line">ping            Check if mysqld is alive</span><br><span class="line">processlist        Show list of active threads in server</span><br><span class="line">reload            Reload grant tables</span><br><span class="line">refresh            Flush all tables and close and open logfiles</span><br><span class="line">shutdown            Take server down</span><br><span class="line">status            Gives a short status message from the server</span><br><span class="line">start-slave        Start slave</span><br><span class="line">stop-slave        Stop slave</span><br><span class="line">variables                 Prints variables available</span><br><span class="line">version            Get version info from server</span><br></pre></td></tr></table></figure><p>举例:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ mysqladmin -uroot -p shutdown</span><br><span class="line">Enter password:</span><br></pre></td></tr></table></figure><h3 id="日志管理工具"><a href="#日志管理工具" class="headerlink" title="日志管理工具"></a>日志管理工具</h3><p>由于服务器生成的二进制文件以二进制格式保存，所以如果想要检查这些文件的文本格式，就会用到 mysqlbinlog 日志管理工具。<br>用法如下：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlbinlog [option] log-file1 log-file2...</span><br></pre></td></tr></table></figure><p>option 有很多选项：  </p><ul><li>-d，–database=name： 指定数据库名称，只列出指定的数据库相关操作。  </li><li>-o, –offset=#: 忽略日志中的前 n 行命令。  </li><li>r, –result-file=name: 将输出的文本格式日志输出到指定文件  </li><li>s, –short-form: 显示简单格式，省略掉一些信息。  </li><li>-start-datetime=name –stop-datetime=name: 指定日期间隔内的所有日志。  </li><li>-start-position=# –stop-position=#： 指定位置间隔内的所有日志  </li></ul><h4 id="示例准备：创建新日志，新建库-t1-和-t2-以及分别新建表-test1-和-test2"><a href="#示例准备：创建新日志，新建库-t1-和-t2-以及分别新建表-test1-和-test2" class="headerlink" title="示例准备：创建新日志，新建库 t1 和 t2, 以及分别新建表 test1 和 test2"></a>示例准备：创建新日志，新建库 t1 和 t2, 以及分别新建表 test1 和 test2</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; reset master;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; create table t1(id int,name varchar);</span><br><span class="line">ERROR 1046 (3D000): No database selected</span><br><span class="line">MySQL [(none)]&gt; reset master;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; create database t1;</span><br><span class="line">Query OK, 1 row affected (0.04 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; create database t2;</span><br><span class="line">Query OK, 1 row affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; use t1;</span><br><span class="line">Database changed</span><br><span class="line">MySQL [t1]&gt; create table test1(id int, name varchar(30));</span><br><span class="line">Query OK, 0 rows affected (0.11 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t1]&gt; insert into  test1 value (1,&#39;zj&#39;);</span><br><span class="line">Query OK, 1 row affected (0.14 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t1]&gt; insert into  test1 value (2,&#39;zj2&#39;);</span><br><span class="line">Query OK, 1 row affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t1]&gt; use t2;</span><br><span class="line">Database changed</span><br><span class="line">MySQL [t2]&gt; create table test2(id int,name varchar(30));</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; insert into test2 select * from t1.test1;</span><br><span class="line">Query OK, 2 rows affected (0.03 sec)</span><br><span class="line">Records: 2  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; select * from t1.test1;</span><br><span class="line">+------+------+</span><br><span class="line">| id   | name |</span><br><span class="line">+------+------+</span><br><span class="line">|    1 | zj   |</span><br><span class="line">|    2 | zj2  |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; select * from test2;</span><br><span class="line">+------+------+</span><br><span class="line">| id   | name |</span><br><span class="line">+------+------+</span><br><span class="line">|    1 | zj   |</span><br><span class="line">|    2 | zj2  |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="不加任何参数，显示所有日志"><a href="#不加任何参数，显示所有日志" class="headerlink" title="不加任何参数，显示所有日志"></a>不加任何参数，显示所有日志</h4><p>注意：必须拥有访问目标文件的权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001</span><br><span class="line">[sudo] password for zj: </span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line">DELIMITER &#x2F;*!*&#x2F;;</span><br><span class="line"># at 4</span><br><span class="line">#170920 20:44:49 server id 1  end_log_pos 123 CRC32 0x42fd5a4d Start: binlog v 4, server v 5.7.18-log created 170920 20:44:49 at startup</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">create table test2(id int,name varchar(30))</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1366</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1431 CRC32 0x18a95938 Anonymous_GTIDlast_committed&#x3D;6sequence_number&#x3D;7</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;ANONYMOUS&#39;&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1431</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1509 CRC32 0x2fa8bd6c Querythread_id&#x3D;4exec_time&#x3D;0error_code&#x3D;0</span><br><span class="line">SET TIMESTAMP&#x3D;1505911829&#x2F;*!*&#x2F;;</span><br><span class="line">BEGIN</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1509</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1622 CRC32 0x77ce6f3b Querythread_id&#x3D;4exec_time&#x3D;0error_code&#x3D;0</span><br><span class="line">SET TIMESTAMP&#x3D;1505911829&#x2F;*!*&#x2F;;</span><br><span class="line">insert into test2 select * from t1.test1</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1622</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1653 CRC32 0x41b7a45b Xid &#x3D; 29</span><br><span class="line">COMMIT&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;AUTOMATIC&#39; &#x2F;* added by mysqlbinlog *&#x2F; &#x2F;*!*&#x2F;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"># End of log file</span><br><span class="line">&#x2F;*!50003 SET COMPLETION_TYPE&#x3D;@OLD_COMPLETION_TYPE*&#x2F;;</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;0*&#x2F;;</span><br></pre></td></tr></table></figure><h4 id="加-d-选项，将只显示-t2-数据库的操作日志"><a href="#加-d-选项，将只显示-t2-数据库的操作日志" class="headerlink" title="加 -d 选项，将只显示 t2 数据库的操作日志"></a>加 -d 选项，将只显示 t2 数据库的操作日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001 -d t2</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">SET TIMESTAMP&#x3D;1505911829&#x2F;*!*&#x2F;;</span><br><span class="line">insert into test2 select * from t1.test1</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1622</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1653 CRC32 0x41b7a45b Xid &#x3D; 29</span><br><span class="line">COMMIT&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;AUTOMATIC&#39; &#x2F;* added by mysqlbinlog *&#x2F; &#x2F;*!*&#x2F;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"># End of log file</span><br><span class="line">&#x2F;*!50003 SET COMPLETION_TYPE&#x3D;@OLD_COMPLETION_TYPE*&#x2F;;</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;0*&#x2F;;</span><br></pre></td></tr></table></figure><h4 id="加-o-选项-忽略掉前-20-行命令"><a href="#加-o-选项-忽略掉前-20-行命令" class="headerlink" title="加 -o 选项, 忽略掉前 20 行命令"></a>加 -o 选项, 忽略掉前 20 行命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001 -o 20</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line">DELIMITER &#x2F;*!*&#x2F;;</span><br><span class="line"># at 4</span><br><span class="line">#170920 20:44:49 server id 1  end_log_pos 123 CRC32 0x42fd5a4d Start: binlog v 4, server v 5.7.18-log created 170920 20:44:49 at startup</span><br><span class="line"># Warning: this binlog is either in use or was not closed properly.</span><br><span class="line">ROLLBACK&#x2F;*!*&#x2F;;</span><br><span class="line">BINLOG &#39;</span><br><span class="line">wWLCWQ8BAAAAdwAAAHsAAAABAAQANS43LjE4LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA</span><br><span class="line">AAAAAAAAAAAAAAAAAADBYsJZEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQA</span><br><span class="line">AU1a&#x2F;UI&#x3D;</span><br><span class="line">&#39;&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1509</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1622 CRC32 0x77ce6f3b Querythread_id&#x3D;4exec_time&#x3D;0error_code&#x3D;0</span><br><span class="line">use &#96;t2&#96;&#x2F;*!*&#x2F;;</span><br><span class="line">SET TIMESTAMP&#x3D;1505911829&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.pseudo_thread_id&#x3D;4&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.foreign_key_checks&#x3D;1, @@session.sql_auto_is_null&#x3D;0, @@session.unique_checks&#x3D;1, @@session.autocommit&#x3D;1&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.sql_mode&#x3D;1436549152&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.auto_increment_increment&#x3D;1, @@session.auto_increment_offset&#x3D;1&#x2F;*!*&#x2F;;</span><br><span class="line">&#x2F;*!\C utf8mb4 *&#x2F;&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.character_set_client&#x3D;45,@@session.collation_connection&#x3D;45,@@session.collation_server&#x3D;45&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.lc_time_names&#x3D;0&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.collation_database&#x3D;DEFAULT&#x2F;*!*&#x2F;;</span><br><span class="line">insert into test2 select * from t1.test1</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line"># at 1622</span><br><span class="line">#170920 20:50:29 server id 1  end_log_pos 1653 CRC32 0x41b7a45b Xid &#x3D; 29</span><br><span class="line">COMMIT&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;AUTOMATIC&#39; &#x2F;* added by mysqlbinlog *&#x2F; &#x2F;*!*&#x2F;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"># End of log file</span><br><span class="line">&#x2F;*!50003 SET COMPLETION_TYPE&#x3D;@OLD_COMPLETION_TYPE*&#x2F;;</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;0*&#x2F;;</span><br></pre></td></tr></table></figure><h4 id="加-r-选项，将上面的结果输出到文件-resultfile-中"><a href="#加-r-选项，将上面的结果输出到文件-resultfile-中" class="headerlink" title="加 -r 选项，将上面的结果输出到文件 resultfile 中"></a>加 -r 选项，将上面的结果输出到文件 resultfile 中</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001 -o 20 -r .&#x2F;logfile</span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo more .&#x2F;logfile</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="结果显示的内容较多，显得比较乱，加-s-选项将上面的内容进行简单显示"><a href="#结果显示的内容较多，显得比较乱，加-s-选项将上面的内容进行简单显示" class="headerlink" title="结果显示的内容较多，显得比较乱，加 -s 选项将上面的内容进行简单显示"></a>结果显示的内容较多，显得比较乱，加 -s 选项将上面的内容进行简单显示</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001 -o 20 -s</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line">DELIMITER &#x2F;*!*&#x2F;;</span><br><span class="line">ROLLBACK&#x2F;*!*&#x2F;;</span><br><span class="line">use &#96;t2&#96;&#x2F;*!*&#x2F;;</span><br><span class="line">SET TIMESTAMP&#x3D;1505911829&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.pseudo_thread_id&#x3D;999999999&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.foreign_key_checks&#x3D;1, @@session.sql_auto_is_null&#x3D;0, @@session.unique_checks&#x3D;1, @@session.autocommit&#x3D;1&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.sql_mode&#x3D;1436549152&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.auto_increment_increment&#x3D;1, @@session.auto_increment_offset&#x3D;1&#x2F;*!*&#x2F;;</span><br><span class="line">&#x2F;*!\C utf8mb4 *&#x2F;&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.character_set_client&#x3D;45,@@session.collation_connection&#x3D;45,@@session.collation_server&#x3D;45&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.lc_time_names&#x3D;0&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@session.collation_database&#x3D;DEFAULT&#x2F;*!*&#x2F;;</span><br><span class="line">insert into test2 select * from t1.test1</span><br><span class="line">&#x2F;*!*&#x2F;;</span><br><span class="line">COMMIT&#x2F;*!*&#x2F;;</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;AUTOMATIC&#39; &#x2F;* added by mysqlbinlog *&#x2F; &#x2F;*!*&#x2F;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"># End of log file</span><br><span class="line">&#x2F;*!50003 SET COMPLETION_TYPE&#x3D;@OLD_COMPLETION_TYPE*&#x2F;;</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;0*&#x2F;;</span><br></pre></td></tr></table></figure><h4 id="加-“–start-datetime-–stop-datetime”-选项显示-5-00-00-5-01-00-之间的日志"><a href="#加-“–start-datetime-–stop-datetime”-选项显示-5-00-00-5-01-00-之间的日志" class="headerlink" title="加 “–start-datetime –stop-datetime” 选项显示 5:00:00 ~ 5:01:00 之间的日志"></a>加 “–start-datetime –stop-datetime” 选项显示 5:00:00 ~ 5:01:00 之间的日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001  --start-datetime&#x3D;&quot;2017&#x2F;09&#x2F;30 05:00:00&quot; --stop-datetime&#x3D;&#39;2017&#x2F;09&#x2F;30 05:01:00&#39;</span><br></pre></td></tr></table></figure><p>开始日期和结束日期可以只写一个。如果只写开始日期，表示范围开始日期到日志结束；如果只写结束日期，表示日志开始到指定的结束日期。  </p><h4 id="–start-position-和-–stop-position-与日期范围类似，不过可以更精确的表示范围。"><a href="#–start-position-和-–stop-position-与日期范围类似，不过可以更精确的表示范围。" class="headerlink" title="–start-position=# 和 –stop-position=#, 与日期范围类似，不过可以更精确的表示范围。"></a>–start-position=# 和 –stop-position=#, 与日期范围类似，不过可以更精确的表示范围。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000001  --start-position&#x3D;4 --stop-datetime&#x3D;100</span><br></pre></td></tr></table></figure><h3 id="mysqlcheck-myisam-表维护工具"><a href="#mysqlcheck-myisam-表维护工具" class="headerlink" title="mysqlcheck (myisam 表维护工具)"></a>mysqlcheck (myisam 表维护工具)</h3><p>mysqlcheck 工具可以检查和修复 myisam 表，还可以优化和分析表。实际上，它集成了 mysql 工具中的 check、repair、analyze、optimize  </p><p>有 3 种方式可以来调用 mysqlcheck：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlcheck [options] db_name [tables]</span><br><span class="line">shell&gt; mysqlcheck [options] --database DB1 [DB2 DB3...]</span><br><span class="line">shell&gt; mysqlcheck [options] --all-databse</span><br></pre></td></tr></table></figure><p>option 中有以下常用选项：  </p><ul><li>-c, –check (检查表)</li><li>-r, –repair (修复表)</li><li>-a, –analyze (分析表)</li><li>-o, –optimize (优化表)</li></ul><p>其中，默认选项是 -c (检查表)<br>示例：  </p><h4 id="检查表"><a href="#检查表" class="headerlink" title="检查表"></a>检查表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ mysqlcheck -c t2</span><br><span class="line">t2.test1                                           OK</span><br><span class="line">t2.test2                                           OK</span><br></pre></td></tr></table></figure><h4 id="修复表"><a href="#修复表" class="headerlink" title="修复表"></a>修复表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ mysqlcheck -r t2</span><br><span class="line">t2.test1</span><br><span class="line">note     : The storage engine for the table doesn&#39;t support repair</span><br><span class="line">t2.test2                                           OK</span><br></pre></td></tr></table></figure><p>test1 表的存储引擎为 innodb，不支持 repair。  </p><h4 id="分析表"><a href="#分析表" class="headerlink" title="分析表"></a>分析表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ mysqlcheck -a t2</span><br><span class="line">t2.test1                                           OK</span><br><span class="line">t2.test2                                           OK</span><br></pre></td></tr></table></figure><h4 id="优化表"><a href="#优化表" class="headerlink" title="优化表"></a>优化表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ mysqlcheck -o t2</span><br><span class="line">t2.test1</span><br><span class="line">note     : Table does not support optimize, doing recreate + analyze instead</span><br><span class="line">status   : OK</span><br><span class="line">t2.test2</span><br></pre></td></tr></table></figure><h3 id="mysqldump-（数据导出工具）"><a href="#mysqldump-（数据导出工具）" class="headerlink" title="mysqldump （数据导出工具）"></a>mysqldump （数据导出工具）</h3><p>mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移。备份内容包含创建表或装载表的 sql 语句。  </p><p>有三中方式来调用 mysqldump：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysqldump [OPTIONS] database [tables]       &#x2F;&#x2F; 备份单个数据库或者库中部分数据表</span><br><span class="line">mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]      &#x2F;&#x2F;备份指定的一个或者多个数据库</span><br><span class="line">mysqldump [OPTIONS] --all-databases [OPTIONS]       &#x2F;&#x2F; 备份所有数据库</span><br></pre></td></tr></table></figure><h4 id="连接选项-1"><a href="#连接选项-1" class="headerlink" title="连接选项"></a>连接选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-u, --user&#x3D;name             &#x2F;&#x2F; 指定用户名</span><br><span class="line">-p, --password[&#x3D;name]       &#x2F;&#x2F; 指定密码  </span><br><span class="line">-h, --host&#x3D;name             &#x2F;&#x2F; 指定服务器 IP 或者域名</span><br><span class="line">-p, --port&#x3D;#                &#x2F;&#x2F; 指定连接端口</span><br></pre></td></tr></table></figure><p>示例：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqldump -h192.18.10.10 -p3306 -uroot -p test &gt; test.sql</span><br></pre></td></tr></table></figure><h4 id="输出内容选项"><a href="#输出内容选项" class="headerlink" title="输出内容选项"></a>输出内容选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--add-drop-database     每个数据库创建语句前加上 drop database 语句</span><br><span class="line">--add-drop-table        在每个表创建语句前加上 drop table 语句</span><br></pre></td></tr></table></figure><p>在默认情况下，这两个参数都自动加上。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-n, --no-create-db      不包含数据库的创建语句  </span><br><span class="line">-t, --no-create-info    不包含数据表的创建语句</span><br><span class="line">-d, --no-data           不包含数据</span><br></pre></td></tr></table></figure><h4 id="输出格式选项"><a href="#输出格式选项" class="headerlink" title="输出格式选项"></a>输出格式选项</h4><ul><li>–compact 选项使得输出结果简洁，不包括默认选项中的各种注释。  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysqldump --compact t2 emp &gt; emp.sql</span><br><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# more emp.sql</span><br><span class="line">&#x2F;*!40101 SET @saved_cs_client     &#x3D; @@character_set_client *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; utf8 *&#x2F;;</span><br><span class="line">CREATE TABLE &#96;emp&#96; (</span><br><span class="line">  &#96;id&#96; int(11) NOT NULL DEFAULT &#39;0&#39;,</span><br><span class="line">  &#96;name&#96; varchar(10) DEFAULT NULL,</span><br><span class="line">  &#96;context&#96; text,</span><br><span class="line">  PRIMARY KEY (&#96;id&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; @saved_cs_client *&#x2F;;</span><br><span class="line">INSERT INTO &#96;emp&#96; VALUES (1,&#39;a&#39;,&#39;a&#39;),(2,&#39;b&#39;,&#39;b&#39;);</span><br></pre></td></tr></table></figure><ul><li>-c 或者 –complete-insert 选项使得输出文件中的 insert 语句包括字段名称，默认是不包括字段名称的。  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysqldump -c --compact t2 emp &gt; emp.sql</span><br><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# more emp.sql</span><br><span class="line">&#x2F;*!40101 SET @saved_cs_client     &#x3D; @@character_set_client *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; utf8 *&#x2F;;</span><br><span class="line">CREATE TABLE &#96;emp&#96; (</span><br><span class="line">  &#96;id&#96; int(11) NOT NULL DEFAULT &#39;0&#39;,</span><br><span class="line">  &#96;name&#96; varchar(10) DEFAULT NULL,</span><br><span class="line">  &#96;context&#96; text,</span><br><span class="line">  PRIMARY KEY (&#96;id&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; @saved_cs_client *&#x2F;;</span><br><span class="line">INSERT INTO &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;context&#96;) VALUES (1,&#39;a&#39;,&#39;a&#39;),(2,&#39;b&#39;,&#39;b&#39;);</span><br></pre></td></tr></table></figure><ul><li>-T 选项将指定数据表中的数据备份为单纯的数据文本和建表 sql 两个文件，经常和下面几个选项一起配合使用，将数据导出为指定格式显示。  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-T, --tab&#x3D;name                  备份数据和建表语句</span><br><span class="line">--fileds-terminated-by&#x3D;name     域分隔符</span><br><span class="line">--fileds-enclosed-by&#x3D;name       域引用符</span><br><span class="line">--fileds-optionally-enclosed-by&#x3D;name    域可选引用符</span><br><span class="line">--fileds-escaped-by&#x3D;name        转义字符</span><br></pre></td></tr></table></figure><p>示例：将 t2 数据库中的表 emp 导出为单纯的数据文本和建表 sql 两个文件，并存放在当前路径下的 bak 目录下。  </p><h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><h5 id="创建备份目录"><a href="#创建备份目录" class="headerlink" title="创建备份目录"></a>创建备份目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin#  mkdir bak</span><br></pre></td></tr></table></figure><h5 id="将-t2-数据库下的表-emp-备份到-bak-目录下"><a href="#将-t2-数据库下的表-emp-备份到-bak-目录下" class="headerlink" title="将 t2 数据库下的表 emp 备份到 bak 目录下"></a>将 t2 数据库下的表 emp 备份到 bak 目录下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysqldump t2 emp -T .&#x2F;bak</span><br></pre></td></tr></table></figure><h5 id="查看-bak-目录，发现两个文件"><a href="#查看-bak-目录，发现两个文件" class="headerlink" title="查看 bak 目录，发现两个文件"></a>查看 bak 目录，发现两个文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# ls .&#x2F;bak</span><br><span class="line">emp.sql  emp.txt</span><br></pre></td></tr></table></figure><h5 id="查看两个文件的内容，-sql-结尾的是建表及插入数据的sql，-txt-结尾的是表数据"><a href="#查看两个文件的内容，-sql-结尾的是建表及插入数据的sql，-txt-结尾的是表数据" class="headerlink" title="查看两个文件的内容， .sql 结尾的是建表及插入数据的sql，.txt 结尾的是表数据"></a>查看两个文件的内容， .sql 结尾的是建表及插入数据的sql，.txt 结尾的是表数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# more .&#x2F;bak&#x2F;emp.sql</span><br><span class="line">-- MySQL dump 10.13  Distrib 5.7.18, for Linux (x86_64)</span><br><span class="line">--</span><br><span class="line">-- Host: localhost    Database: t2</span><br><span class="line">-- ------------------------------------------------------</span><br><span class="line">-- Server version5.7.18-log</span><br><span class="line"></span><br><span class="line">&#x2F;*!40101 SET @OLD_CHARACTER_SET_CLIENT&#x3D;@@CHARACTER_SET_CLIENT *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_CHARACTER_SET_RESULTS&#x3D;@@CHARACTER_SET_RESULTS *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_COLLATION_CONNECTION&#x3D;@@COLLATION_CONNECTION *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET NAMES utf8mb4 *&#x2F;;</span><br><span class="line">&#x2F;*!40103 SET @OLD_TIME_ZONE&#x3D;@@TIME_ZONE *&#x2F;;</span><br><span class="line">&#x2F;*!40103 SET TIME_ZONE&#x3D;&#39;+00:00&#39; *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_SQL_MODE&#x3D;@@SQL_MODE, SQL_MODE&#x3D;&#39;&#39; *&#x2F;;</span><br><span class="line">&#x2F;*!40111 SET @OLD_SQL_NOTES&#x3D;@@SQL_NOTES, SQL_NOTES&#x3D;0 *&#x2F;;</span><br><span class="line"></span><br><span class="line">--</span><br><span class="line">-- Table structure for table &#96;emp&#96;</span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">DROP TABLE IF EXISTS &#96;emp&#96;;</span><br><span class="line">&#x2F;*!40101 SET @saved_cs_client     &#x3D; @@character_set_client *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; utf8 *&#x2F;;</span><br><span class="line">CREATE TABLE &#96;emp&#96; (</span><br><span class="line">  &#96;id&#96; int(11) NOT NULL DEFAULT &#39;0&#39;,</span><br><span class="line">  &#96;name&#96; varchar(10) DEFAULT NULL,</span><br><span class="line">  &#96;context&#96; text,</span><br><span class="line">  PRIMARY KEY (&#96;id&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; @saved_cs_client *&#x2F;;</span><br><span class="line"></span><br><span class="line">&#x2F;*!40103 SET TIME_ZONE&#x3D;@OLD_TIME_ZONE *&#x2F;;</span><br><span class="line"></span><br><span class="line">&#x2F;*!40101 SET SQL_MODE&#x3D;@OLD_SQL_MODE *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET CHARACTER_SET_CLIENT&#x3D;@OLD_CHARACTER_SET_CLIENT *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET CHARACTER_SET_RESULTS&#x3D;@OLD_CHARACTER_SET_RESULTS *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET COLLATION_CONNECTION&#x3D;@OLD_COLLATION_CONNECTION *&#x2F;;</span><br><span class="line">&#x2F;*!40111 SET SQL_NOTES&#x3D;@OLD_SQL_NOTES *&#x2F;;</span><br><span class="line"></span><br><span class="line">-- Dump completed on 2017-09-21 12:07:38</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# more .&#x2F;bak&#x2F;emp.txt </span><br><span class="line">1aa</span><br><span class="line">2bb</span><br></pre></td></tr></table></figure><h4 id="字符集选项"><a href="#字符集选项" class="headerlink" title="字符集选项"></a>字符集选项</h4><p>mysqldump 导出的数据的字符集使用的是 mysqld 启动时的默认字符集，如果表的字符集用的不是默认字符集，导出的数据就有可能出现乱码。所以在导出时，应该先确定表的字符集，在导出时指定该字符集即可。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqldump -uroot --compact --default-character-set&#x3D;utf8 t2 emp &gt; emp.sql</span><br></pre></td></tr></table></figure><h4 id="其他常用选项"><a href="#其他常用选项" class="headerlink" title="其他常用选项"></a>其他常用选项</h4><ul><li>-F –flush-logs （备份前刷新日志）  </li></ul><p>加上此选项后，备份前将关闭就日志，生成新日志。使得进行恢复的时候直接从新日志开始进行重做，大大方便了恢复过程。  </p><ul><li>-l –lock-tables （给所有表加读锁）  </li></ul><p>可以在备份期间使用，使得数据无法被更新，从而使备份的数据保持一致性，可以配合 -F 选项一起使用。  </p><h3 id="mysqlimport-（数据导入工具）"><a href="#mysqlimport-（数据导入工具）" class="headerlink" title="mysqlimport （数据导入工具）"></a>mysqlimport （数据导入工具）</h3><p>mysqlimport 是客户端数据导入工具，用来导入 mysqldump 加 -T 选项后导出的文本文件。  </p><p>基本用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlimport [options] db_name textfile1</span><br></pre></td></tr></table></figure><h3 id="mysqlshow-数据库对象查看工具"><a href="#mysqlshow-数据库对象查看工具" class="headerlink" title="mysqlshow (数据库对象查看工具)"></a>mysqlshow (数据库对象查看工具)</h3><p>mysqlshow 客户端对象查找工具，用来很快的查找存在哪些数据库，数据库中的表、表中的列或索引，和 mysql 客户端工具很类似，不过有些特性是 mysql 客户端工具所不具备的。  </p><p>使用方法：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlshow [option] [db_name [tbl_name [col_name]]]</span><br></pre></td></tr></table></figure><p>如果不加任何选项，默认情况下会显示所有数据库。  </p><p>常用选项:  </p><h4 id="–count-显示数据库和表的统计信息"><a href="#–count-显示数据库和表的统计信息" class="headerlink" title="–count (显示数据库和表的统计信息)"></a>–count (显示数据库和表的统计信息)</h4><p>如果不指定数据库，则显示每个数据库的名称、表数量、记录数量；<br>如果指定数据库，则显示指定数据库的每个表名、字段数量，记录数量；<br>如果指定具体数据库中的具体表，则显示表的字段信息。  </p><h4 id="k-或者-–keys-显示指定表中的所有索引"><a href="#k-或者-–keys-显示指定表中的所有索引" class="headerlink" title="-k 或者 –keys (显示指定表中的所有索引)"></a>-k 或者 –keys (显示指定表中的所有索引)</h4><p>此选项显示了两部分内容，一部分是指定表的表结构，另一部分中是指定表的当前索引信息  </p><h4 id="i-或者-–status-显示表的一些状态信息"><a href="#i-或者-–status-显示表的一些状态信息" class="headerlink" title="-i 或者 –status (显示表的一些状态信息)"></a>-i 或者 –status (显示表的一些状态信息)</h4><h3 id="perror-（错误代码查看工具）"><a href="#perror-（错误代码查看工具）" class="headerlink" title="perror （错误代码查看工具）"></a>perror （错误代码查看工具）</h3><p>在 MySQL 的使用过程中，可能会出现各种各样的 error。这些 error 有些是由于操作系统引起的，比如文件或者目录不存在；有些则是由于存储引擎使用不当引起的。这些 error 一般都有一个代码，类似于 “error：#” 或者 “Errcode：#”，“#” 代表具体的错误号。perror 的作用就是解释这些错误代码的详细含义：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">perror [options] [errorcode [errorcode]]</span><br><span class="line"></span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ perror 30</span><br><span class="line">OS error code  30:  Read-only file system</span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ perror 60</span><br><span class="line">OS error code  60:  Device not a stream</span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ perror 30  60</span><br><span class="line">OS error code  30:  Read-only file system</span><br><span class="line">OS error code  60:  Device not a stream</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive 优化总结</title>
      <link href="/2018-06-30-hive-optimization.html"/>
      <url>/2018-06-30-hive-optimization.html</url>
      
        <content type="html"><![CDATA[<p>Hive 实际上是运行在 yarn 上的 mapreduce 任务，所以性能调优对 hql 处理数据的速度有很大的影响。</p><a id="more"></a><h4 id="Fetch-抓取选项"><a href="#Fetch-抓取选项" class="headerlink" title="Fetch 抓取选项"></a>Fetch 抓取选项</h4><p>Fetch 抓取是指 Hive 中某些查询可以不使用 MapReduce 计算。<br>例如: <code>SELECT * FROM team;</code> 可以简单地读取 <code>team</code> 对应的文件并返回结果。  </p><p>配置项 <code>hive.fetch.task.conversion</code> 指定了哪些操作不启动 mapreduce。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.fetch.task.conversion&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;more&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">      Currently the query should be single sourced not having any subquery and should not have</span><br><span class="line">      any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">      0. none : disable hive.fetch.task.conversion</span><br><span class="line">      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>因为 Hive 背后是 Hadoop 集群，所以更适合处理大数据，但有时 Hive 中的数据比较小。这种情况下在集群中查询所消耗的时间可能会比本地处理还要长。  </p><p>对于小数据集，Hive 可以使用本地模式在单台机器上处理任务，这样执行的时间可以明显被缩短。  </p><p>配置项 <code>hive.exec.mode.local.auto</code> 的值设置为 <code>true</code>，Hive 会在适当的时候自动启动这个优化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 开启自动本地模式优化</span><br><span class="line">set hive.exec.mode.local.auto&#x3D;true;</span><br><span class="line"># 设置本地模式的最大输入数据量，当小于这个设置的时候才使用本地模式, 默认为 134217728， 即 128M</span><br><span class="line">set hive.exec.mode.local.auto.inputbytes.max</span><br><span class="line"># 设置本地模式的最大输入文件个数，当输入文件个数小于这个设置的时候才使用本地模式。默认为 4</span><br><span class="line">set hive.exec.mode.local.auto.input.files.max</span><br></pre></td></tr></table></figure><h4 id="查询的优化"><a href="#查询的优化" class="headerlink" title="查询的优化"></a>查询的优化</h4><h5 id="大表，小表-join"><a href="#大表，小表-join" class="headerlink" title="大表，小表 join"></a>大表，小表 join</h5><p>建议将 key 相对分散且数据量小的小表放在 join 的左边，这样可以有效减少内存溢出错误发生的几率。<br>新版的 hive 已经对大小表之间的 join 操作做了优化。</p><h5 id="大表-join-大表"><a href="#大表-join-大表" class="headerlink" title="大表 join 大表"></a>大表 join 大表</h5><h6 id="空-key-过滤"><a href="#空-key-过滤" class="headerlink" title="空 key 过滤"></a>空 key 过滤</h6><p>有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如 key 对应的字段为空: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select p.* from (select * from player where id is not null ) p left join team  on p.team_id &#x3D; team.id;</span><br></pre></td></tr></table></figure><h6 id="空-key-转换"><a href="#空-key-转换" class="headerlink" title="空 key 转换"></a>空 key 转换</h6><p>有时虽然某个 key 为空对应的数据很多，但相应的数据不是异常数据，必须要包含在join的结果中，此时可以为表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select p.* from player p full join team on  </span><br><span class="line">case when p.id is null then concat(&#39;hive&#39;, rand()) else p.team_id end &#x3D; team.id;</span><br></pre></td></tr></table></figure><h5 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h5><h6 id="MapJoin-说明"><a href="#MapJoin-说明" class="headerlink" title="MapJoin 说明"></a>MapJoin 说明</h6><p>如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：<br>在 Reduce 阶段完成join。这样容易发生数据倾斜，可以用 MapJoin 把小表全部加载到内存先在map端进行join，避免reducer处理。</p><h6 id="开启-MapJoin-参数设置"><a href="#开启-MapJoin-参数设置" class="headerlink" title="开启 MapJoin 参数设置:"></a>开启 MapJoin 参数设置:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 开启自动选择 MapJoin, 默认为 true</span><br><span class="line">set hive.auto.convert.join &#x3D; true;</span><br><span class="line"></span><br><span class="line"># 大小表的阈值设置, 默认 25M 以下是小表</span><br></pre></td></tr></table></figure><h5 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h5><h6 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h6><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><h6 id="开启-Map-端聚合参数设置"><a href="#开启-Map-端聚合参数设置" class="headerlink" title="开启 Map 端聚合参数设置"></a>开启 Map 端聚合参数设置</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 开启在 map 端进行聚合，默认为 true</span><br><span class="line">hive.map.aggr&#x3D;true</span><br><span class="line"></span><br><span class="line"># 在 map 端进行聚合操作的条目数目</span><br><span class="line">hive.groupby.mapaggr.checkinterval &#x3D; 100000</span><br><span class="line"></span><br><span class="line"># 有数据倾斜的时候进行负载均衡（默认是false）</span><br><span class="line">hive.groupby.skewindata&#x3D;true</span><br><span class="line"># 当 hive.groupby.skewindata&#x3D;true，查询计划会有两个MR Job。第一个MR Job中，Map 的输出结果会随机分布到 Reduce 中，</span><br><span class="line"># 每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的Reduce中，从而达到负载均衡的目的；</span><br><span class="line"># 第二个MR Job再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</span><br></pre></td></tr></table></figure><h5 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct) 去重统计"></a>Count(Distinct) 去重统计</h5><p>数据量大的情况下，由于 <code>COUNT DISTINCT</code> 操作需要用一个Reduce Task来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个Job很难完成，一般 <code>COUNT DISTINCT</code> 操作可以使用先 <code>GROUP BY</code> 再 <code>COUNT</code> 的方式替换：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct team_id) from player;</span><br><span class="line"># 等价于:</span><br><span class="line">select count(team_id) from (select id from player group by team_id) p;</span><br><span class="line"># 虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</span><br></pre></td></tr></table></figure><h5 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h5><p>尽量避免笛卡尔积，Hive只能使用1个reducer来完成笛卡尔积</p><h5 id="行列过滤"><a href="#行列过滤" class="headerlink" title="行列过滤"></a>行列过滤</h5><p>列处理: 在 <code>SELECT</code> 中只拿需要的列，如果有分区尽量使用分区过滤，少用不用<code>SELECT *</code>。<br>行处理: 在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 <code>Where</code> 后面，那么就会先全表关联，之后再过滤。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 先关联两张表，再用 where 条件过滤</span><br><span class="line">select t.id from player p join team t on t.id &#x3D; p.team_id where t.id &lt;&#x3D; 3;</span><br><span class="line"># 先进行子查询，在关联表，这样更快一些</span><br></pre></td></tr></table></figure><h5 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h5><p>动态分区(Dynamic Partition) 是指 对分区表 Insert 数据时候，数据库自动会根据分区字段的值将数据插入到相应的分区中。</p><p>配置动态分区:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 开启动态分区功能(默认true 开启)</span><br><span class="line">hive.exec.dynamic.partition&#x3D;true</span><br><span class="line"></span><br><span class="line"># 设置为非严格模式(默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</span><br><span class="line">hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br><span class="line"></span><br><span class="line"># 设置在所有执行 MR 的节点上，最大一共可以创建多少个动态分区</span><br><span class="line">hive.exec.max.dynamic.partitions&#x3D;1000</span><br><span class="line"></span><br><span class="line"># 在每个执行MR的节点上，最大可以创建多少个动态分区</span><br><span class="line"># 这个参数要根据实际情况设定，设定不好会报错</span><br><span class="line"># 比如: 源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</span><br><span class="line">hive.exec.max.dynamic.partitions.pernode&#x3D;100</span><br><span class="line"></span><br><span class="line"># 整个MR Job中，最大可以创建多少个HDFS文件</span><br><span class="line">hive.exec.max.created.files&#x3D;100000</span><br><span class="line"></span><br><span class="line"># 当有空分区生成时，是否抛出异常。一般不需要设置。</span><br><span class="line">hive.error.on.empty.partition&#x3D;false</span><br></pre></td></tr></table></figure><p>动态分区表要使用 insert 从一个已存在的表的基础上建立</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table log2 partition (day) </span><br><span class="line">select id, time, uid, keyword, url_rank, click_num, click_url, day from log1;</span><br></pre></td></tr></table></figure><h5 id="合理使用分桶、分区"><a href="#合理使用分桶、分区" class="headerlink" title="合理使用分桶、分区"></a>合理使用分桶、分区</h5><h4 id="数据倾斜优化"><a href="#数据倾斜优化" class="headerlink" title="数据倾斜优化"></a>数据倾斜优化</h4><h5 id="Map-数"><a href="#Map-数" class="headerlink" title="Map 数"></a>Map 数</h5><ul><li>通常情况下，作业会通过input的目录产生一个或者多个map任务。主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</li><li>Map 数不是越多越好，如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。这种情况要减少 map 数</li><li>对于接近 128M 的文件，正常会用一个 map 去完成，但如果这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。这种情况，应该增加 map 数</li></ul><h6 id="小文件进行合并减少-map-数"><a href="#小文件进行合并减少-map-数" class="headerlink" title="小文件进行合并减少 map 数"></a>小文件进行合并减少 map 数</h6><p>在 map 执行前合并小文件，减少 map 数。<br>CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format&#x3D; org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><h6 id="复杂文件增加-map-数"><a href="#复杂文件增加-map-数" class="headerlink" title="复杂文件增加 map 数"></a>复杂文件增加 map 数</h6><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加 map 的方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M</span><br><span class="line"># 调整 maxSize 最大值。让maxSize最大值低于blocksize就可以增加map的个数</span><br></pre></td></tr></table></figure><p>案例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 普通查询</span><br><span class="line">select count(*) from team;</span><br><span class="line"># Hadoop job information for Stage-1: number of mappers: 1; number of reducers: </span><br><span class="line"></span><br><span class="line"># 设置最大切片值为100</span><br><span class="line">set mapreduce.input.fileinputformat.split.maxsize&#x3D;100;</span><br><span class="line"></span><br><span class="line"># 查询</span><br><span class="line">hive (default)&gt; select count(*) from team;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</span><br></pre></td></tr></table></figure><h5 id="Reduce数"><a href="#Reduce数" class="headerlink" title="Reduce数"></a>Reduce数</h5><p>reduce个数并不是越多越好，过多的启动和初始化reduce也会消耗时间和资源。<br>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题。<br>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</p><h6 id="计算的方式调增-reduce-数"><a href="#计算的方式调增-reduce-数" class="headerlink" title="计算的方式调增 reduce 数"></a>计算的方式调增 reduce 数</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 每个 Reduce 处理的数据量默认是 256MB</span><br><span class="line">hive.exec.reducers.bytes.per.reducer&#x3D;256000000</span><br><span class="line"># 每个任务最大的reduce数，默认为1009</span><br><span class="line">hive.exec.reducers.max&#x3D;1009</span><br><span class="line"># 计算reducer数的公式</span><br><span class="line">N&#x3D;min(hive.exec.reducers.max，总输入数据量&#x2F;hive.exec.reducers.bytes.per.reducer)</span><br></pre></td></tr></table></figure><h6 id="修改配置项调整-reduce-数"><a href="#修改配置项调整-reduce-数" class="headerlink" title="修改配置项调整 reduce 数"></a>修改配置项调整 reduce 数</h6><p>在 hadoop 的 mapred-default.xml 文件中修改，或在hive命令行中执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces&#x3D;15;</span><br></pre></td></tr></table></figure><h4 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h4><p>Hive 会将一个查询转化成一个或者多个阶段。默认情况下，Hive 一次只会执行一个阶段。<br>不过,并非完全互相依赖的阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 开启并发执行</span><br><span class="line">set hive.exec.parallel&#x3D;true </span><br><span class="line"># 在共享集群中，需要注意如果job中并行阶段增多，那么集群利用率就会增加。</span><br><span class="line"></span><br><span class="line"># 同一个sql允许最大并行度，默认为8。</span><br><span class="line">set hive.exec.parallel.thread.number&#x3D;16;</span><br></pre></td></tr></table></figure><h4 id="严格模式"><a href="#严格模式" class="headerlink" title="严格模式"></a>严格模式</h4><p>Hive 提供了一个严格模式，可以防止用户执行那些可能有不好的影响的查询。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.mapred.mode&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;strict&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>开启严格模式后会禁止三种类型的查询:</p><ul><li><code>对于分区表，如果 where 语句中没有分区字段过滤条件来限制范围，则不允许执行</code><br>这样是为了不让用户扫描所有分区，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗巨大资源。</li><li><code>对于使用了order by语句的查询，要求必须使用limit语句</code><br>order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加 LIMIT 语句可以防止 Reducer 额外执行很长一段时间。</li><li><code>限制笛卡尔积的查询</code><br>小弟出道多年，从未见过在数据库中做笛卡尔积</li></ul><h4 id="JVM-重用"><a href="#JVM-重用" class="headerlink" title="JVM 重用"></a>JVM 重用</h4><p>JVM 重用对 Hive 的性能具有很大的影响，特别是对于小文件或 task 特别多的场景。<br>Hadoop 的默认配置通常是使用派生 JVM 来执行 map 和 Reduce 任务的。<br>这时 JVM 的启动过程可能会造成相当大的开销，尤其是执行的 job 包含有成百上千 task 任务的情况。<br>JVM重用可以使得 JVM 实例在同一个job中重新使用 N 次。<br>N 的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit. </span><br><span class="line">  &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的 task 插槽，以便进行重用，直到任务完成后才能释放。<br>如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h4 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h4><p>在分布式集群环境下，同一个作业的多个任务之间运行速度通常不一致，有些任务的运行速度可能明显慢于其他任务从而拖慢整体执行进度。<br>为了避免这种情况，Hadoop 采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。  </p><p>在 Hadoop 的 mapred-site.xml 中开启推测执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>hive 也提供了配置项来控制 reduce-side 的推测执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;&#x2F;description&gt;</span><br><span class="line"> &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。<br>如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h4 id="查看执行计划-EXPLAIN"><a href="#查看执行计划-EXPLAIN" class="headerlink" title="查看执行计划(EXPLAIN)"></a>查看执行计划(EXPLAIN)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query;</span><br><span class="line"></span><br><span class="line">explain select * from team;</span><br><span class="line">explain extended select * from team;</span><br></pre></td></tr></table></figure><h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Sql 两种将 RDD 转为 DataFrame 的方式</title>
      <link href="/2018-06-19-spark-sql-rdd.html"/>
      <url>/2018-06-19-spark-sql-rdd.html</url>
      
        <content type="html"><![CDATA[<p>Spark 支持两种将 rdd 转为 dataframe 的方式: 反射推断和编程指定 Schema</p><a id="more"></a><h4 id="使用反射推断Schema"><a href="#使用反射推断Schema" class="headerlink" title="使用反射推断Schema"></a>使用反射推断Schema</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import Row</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;reflect rdd to dataFrame&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc &#x3D; spark.sparkContext</span><br><span class="line"></span><br><span class="line">lines &#x3D; sc.parallelize([</span><br><span class="line">    &quot;Michael, 29&quot;,</span><br><span class="line">    &quot;Andy, 30&quot;,</span><br><span class="line">    &quot;Justin, 19&quot;,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">parts &#x3D; lines.map(lambda l: l.split(&quot;,&quot;))</span><br><span class="line">people &#x3D; parts.map(lambda p: Row(name&#x3D;p[0], age&#x3D;int(p[1].strip())))</span><br><span class="line"></span><br><span class="line">schemaPeople &#x3D; spark.createDataFrame(people)</span><br><span class="line"></span><br><span class="line"># 必须注册为临时表才能供下面的查询使用</span><br><span class="line">schemaPeople.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line"># 返回 dataframe</span><br><span class="line">teenagers &#x3D; spark.sql(&quot;SELECT name FROM people WHERE age &gt;&#x3D; 13 AND age &lt;&#x3D;19&quot;)</span><br><span class="line"></span><br><span class="line">teenagers.show()</span><br><span class="line"># +------+</span><br><span class="line"># |  name|</span><br><span class="line"># +------+</span><br><span class="line"># |Justin|</span><br><span class="line"># +------+</span><br><span class="line"></span><br><span class="line"># 将 dataframe 再转为 rdd</span><br><span class="line">teenNames &#x3D; teenagers.rdd.map(lambda p: &quot;Name: &quot; + p.name).collect()</span><br><span class="line">for name in teenNames:</span><br><span class="line">    print(name)</span><br><span class="line"># Name: Justin</span><br></pre></td></tr></table></figure><h4 id="以编程的方式指定Schema"><a href="#以编程的方式指定Schema" class="headerlink" title="以编程的方式指定Schema"></a>以编程的方式指定Schema</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import StructField, StructType, StringType</span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder\</span><br><span class="line">    .appName(&quot;coding_rdd&quot;)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">sc &#x3D; spark.sparkContext</span><br><span class="line"></span><br><span class="line">lines &#x3D; sc.parallelize([</span><br><span class="line">    &quot;Michael, 29&quot;,</span><br><span class="line">    &quot;Andy, 30&quot;,</span><br><span class="line">    &quot;Justin, 19&quot;,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">people &#x3D; lines.map(lambda l: tuple(l.split(&quot;,&quot;)))</span><br><span class="line"></span><br><span class="line"># 定义 schema</span><br><span class="line">schemaString &#x3D; &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">fields &#x3D; [StructField(field_name, StringType(), True) for field_name in schemaString.split()]</span><br><span class="line">schema &#x3D; StructType(fields)</span><br><span class="line">schemaPeople &#x3D; spark.createDataFrame(people, schema)</span><br><span class="line"></span><br><span class="line"># 必须注册为临时表才能供下面查询使用</span><br><span class="line">schemaPeople.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">results &#x3D; spark.sql(&quot;SELECT * FROM people&quot;)</span><br><span class="line">results.show()</span><br><span class="line"># +-------+---+</span><br><span class="line"># |   name|age|</span><br><span class="line"># +-------+---+</span><br><span class="line"># |Michael| 29|</span><br><span class="line"># |   Andy| 30|</span><br><span class="line"># | Justin| 19|</span><br><span class="line"># +-------+---+</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 使用 checkpoint 做恢复</title>
      <link href="/2018-06-10-spark-streaming-checkpoint.html"/>
      <url>/2018-06-10-spark-streaming-checkpoint.html</url>
      
        <content type="html"><![CDATA[<p>Streaming 程序对可用性要求往往非常高，为此，spark 提供了 checkpoint 机制对 streaming 程序进行容灾处理</p><a id="more"></a><h4 id="在-Wordcount-程序中加入-checkpoint-机制"><a href="#在-Wordcount-程序中加入-checkpoint-机制" class="headerlink" title="在 Wordcount 程序中加入 checkpoint 机制"></a>在 Wordcount 程序中加入 checkpoint 机制</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line"></span><br><span class="line"># 在Spark Streaming中, 无法从 checkpoint 恢复 Accumulators 和 Broadcast 变量 . 如果启用 checkpoint 并使用 Accumulators 或</span><br><span class="line"># Broadcast 变量 , 则必须为 Accumulators 和 Broadcast 变量创建延迟实例化的单例实例, 以便在 driver 重新启动失败后重新实例化.</span><br><span class="line"># Get or register a Broadcast variable</span><br><span class="line">def getWordBlacklist(sparkContext):</span><br><span class="line">    if (&#39;wordBlacklist&#39; not in globals()):</span><br><span class="line">        globals()[&#39;wordBlacklist&#39;] &#x3D; sparkContext.broadcast([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])</span><br><span class="line">    return globals()[&#39;wordBlacklist&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Get or register an Accumulator</span><br><span class="line">def getDroppedWordsCounter(sparkContext):</span><br><span class="line">    if (&#39;droppedWordsCounter&#39; not in globals()):</span><br><span class="line">        globals()[&#39;droppedWordsCounter&#39;] &#x3D; sparkContext.accumulator(0)</span><br><span class="line">    return globals()[&#39;droppedWordsCounter&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def createContext(host, port, outputPath):</span><br><span class="line">    # If you do not see this printed, that means the StreamingContext has been loaded from the new checkpoint</span><br><span class="line">    print(&quot;Creating new context&quot;)</span><br><span class="line">    if os.path.exists(outputPath):</span><br><span class="line">        os.remove(outputPath)</span><br><span class="line">    sc &#x3D; SparkContext(appName&#x3D;&quot;PythonStreamingRecoverableNetworkWordCount&quot;)</span><br><span class="line">    ssc &#x3D; StreamingContext(sc, 10)</span><br><span class="line"></span><br><span class="line">    lines &#x3D; ssc.socketTextStream(host, port)</span><br><span class="line">    words &#x3D; lines.flatMap(lambda line: line.split(&quot; &quot;))</span><br><span class="line">    wordCounts &#x3D; words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)</span><br><span class="line"></span><br><span class="line">    def echo(time, rdd):</span><br><span class="line">        # Get or register the blacklist Broadcast</span><br><span class="line">        blacklist &#x3D; getWordBlacklist(rdd.context)</span><br><span class="line">        # Get or register the droppedWordsCounter Accumulator</span><br><span class="line">        droppedWordsCounter &#x3D; getDroppedWordsCounter(rdd.context)</span><br><span class="line"></span><br><span class="line">        # Use blacklist to drop words and use droppedWordsCounter to count them</span><br><span class="line">        def filterFunc(wordCount):</span><br><span class="line">            if wordCount[0] in blacklist.value:</span><br><span class="line">                droppedWordsCounter.add(wordCount[1])</span><br><span class="line">                return False</span><br><span class="line">            else:</span><br><span class="line">                return True</span><br><span class="line"></span><br><span class="line">        counts &#x3D; &quot;Counts at time %s %s&quot; % (time, rdd.filter(filterFunc).collect())</span><br><span class="line">        print(counts)</span><br><span class="line">        print(&quot;Dropped %d word(s) totally&quot; % droppedWordsCounter.value)</span><br><span class="line">        print(&quot;Appending to &quot; + os.path.abspath(outputPath))</span><br><span class="line">        with open(outputPath, &#39;a&#39;) as f:</span><br><span class="line">            f.write(counts + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    wordCounts.foreachRDD(echo)</span><br><span class="line">    return ssc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    if len(sys.argv) !&#x3D; 5:</span><br><span class="line">        print(&quot;Usage: recoverable_network_wordcount.py &lt;hostname&gt; &lt;port&gt; &quot;</span><br><span class="line">              &quot;&lt;checkpoint-directory&gt; &lt;output-file&gt;&quot;, file&#x3D;sys.stderr)</span><br><span class="line">        sys.exit(-1)</span><br><span class="line">    host, port, checkpoint, output &#x3D; sys.argv[1:]</span><br><span class="line">    ssc &#x3D; StreamingContext.getOrCreate(checkpoint, lambda: createContext(host, int(port), output))</span><br><span class="line">    ssc.sparkContext.setLogLevel(&quot;ERROR&quot;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h4 id="实验日志"><a href="#实验日志" class="headerlink" title="实验日志"></a>实验日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit project&#x2F;daily-learning&#x2F;learn-spark&#x2F;python&#x2F;base_demo&#x2F;streaming&#x2F;checkpoint_recover.py 127.0.0.1 9999 ~&#x2F;checkpoint ~&#x2F;out</span><br></pre></td></tr></table></figure><h5 id="第一次启动"><a href="#第一次启动" class="headerlink" title="第一次启动"></a>第一次启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2018-06-09 08:07:25 WARN  Checkpoint:66 - Checkpoint directory &#x2F;home&#x2F;zj&#x2F;checkpoint does not exist （第一次没有checkpoint，会创建 contex）</span><br><span class="line">Creating new context</span><br><span class="line">2018-06-09 08:07:25 INFO  SparkContext:54 - Running Spark version 2.3.1</span><br><span class="line">2018-06-09 08:07:26 INFO  SparkContext:54 - Submitted application: PythonStreamingRecoverableNetworkWordCount</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Counts at time 2018-06-09 08:07:30 []</span><br><span class="line">Dropped 0 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br><span class="line">Counts at time 2018-06-09 08:07:40 [(&#39;&#39;, 1), (&#39;e&#39;, 1), (&#39;d&#39;, 1), (&#39;f&#39;, 1)]</span><br><span class="line">Dropped 3 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br><span class="line">Counts at time 2018-06-09 08:07:50 [(&#39;g&#39;, 1), (&#39;e&#39;, 1), (&#39;d&#39;, 1), (&#39;f&#39;, 1)]</span><br><span class="line">Dropped 6 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br><span class="line">Counts at time 2018-06-09 08:08:00 []</span><br><span class="line">Dropped 6 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br></pre></td></tr></table></figure><h5 id="CTRL-C-停止后重新启动"><a href="#CTRL-C-停止后重新启动" class="headerlink" title="CTRL+C 停止后重新启动"></a>CTRL+C 停止后重新启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(发现 checkpoint)</span><br><span class="line">2018-06-09 08:15:06 INFO  CheckpointReader:54 - Checkpoint files found: hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773280000,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773280000.bk,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773270000,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773270000.bk,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773260000,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773260000.bk,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773250000,hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773250000.bk</span><br><span class="line">(尝试加载 checkpoint)</span><br><span class="line">2018-06-09 08:15:06 INFO  CheckpointReader:54 - Attempting to load checkpoint from file hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773280000</span><br><span class="line">2018-06-09 08:15:06 INFO  Checkpoint:54 - Checkpoint for time 1533773280000 ms validated</span><br><span class="line">(成功读取 checkpoint)</span><br><span class="line">2018-06-09 08:15:06 INFO  CheckpointReader:54 - Checkpoint successfully loaded from file hdfs:&#x2F;&#x2F;localhost:9000&#x2F;home&#x2F;zj&#x2F;checkpoint&#x2F;checkpoint-1533773280000</span><br><span class="line">2018-06-09 08:15:06 INFO  CheckpointReader:54 - Checkpoint was generated at time 1533773280000 ms</span><br><span class="line">2018-06-09 08:15:06 INFO  SparkContext:54 - Running Spark version 2.3.1</span><br><span class="line">2018-06-09 08:15:06 INFO  SparkContext:54 - Submitted application: PythonStreamingRecoverableNetworkWordCount</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">(继续接着上一次停止的位置开始执行)</span><br><span class="line">Counts at time 2018-06-09 08:08:00 []</span><br><span class="line">Dropped 0 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br><span class="line">Counts at time 2018-06-09 08:08:10 []</span><br><span class="line">Dropped 0 word(s) totally</span><br><span class="line">Appending to &#x2F;home&#x2F;zj&#x2F;out</span><br><span class="line">Counts at time 2018-06-09 08:08:20 []</span><br><span class="line">...</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>grpc开发入门(python)</title>
      <link href="/2018-06-01-rpc-grpc-python-ping.html"/>
      <url>/2018-06-01-rpc-grpc-python-ping.html</url>
      
        <content type="html"><![CDATA[<p>使用 python 和 grpc库,开发rpc服务</p><a id="more"></a><h3 id="grpc-简介"><a href="#grpc-简介" class="headerlink" title="grpc 简介"></a>grpc 简介</h3><p>grpc 是一个采用Protobuf来做数据的序列化与反序列化,用 http2.0 作为通信协议的rpc框架</p><h4 id="HTTP2-0"><a href="#HTTP2-0" class="headerlink" title="HTTP2.0"></a>HTTP2.0</h4><p>HTTP2.0 相比 HTTP1.1 有很大不同,HTTP1.1 还是基于文本协议的问答有序模式，但是 HTTP2.0 是基于二进制协议的乱序模式 (Duplexing).这意味同一个连接通道上多个请求并行时，服务器处理快的可以先返回而不用因为等待其它请求的响应而排队。HTTP2.0 对请求头的 key/value 做了字典处理，常用的 key/value 无需重复传送,而是通过引用内部字典的索引节省了请求头传输的流量.</p><h4 id="Protobuf-通讯协议"><a href="#Protobuf-通讯协议" class="headerlink" title="Protobuf 通讯协议"></a>Protobuf 通讯协议</h4><p>Protobuf 协议是 Google 开源的二进制 RPC 通讯协议，它可能是互联网开源项目中使用最为广泛的 RPC 协议。</p><h3 id="grpc-简单模式编程示例"><a href="#grpc-简单模式编程示例" class="headerlink" title="grpc 简单模式编程示例"></a>grpc 简单模式编程示例</h3><h4 id="编写协议文件-ping-proto"><a href="#编写协议文件-ping-proto" class="headerlink" title="编写协议文件 ping.proto"></a>编写协议文件 ping.proto</h4><p>在目录下创建并编写文件 <code>ping.proto</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">syntax &#x3D; &quot;proto3&quot;;</span><br><span class="line"></span><br><span class="line">package ping;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping service</span><br><span class="line">service PingCalculator &#123;</span><br><span class="line">    rpc Calc(PingRequest) returns (PingResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping input</span><br><span class="line">message PingRequest &#123;</span><br><span class="line">   string n &#x3D; 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping output</span><br><span class="line">message PingResponse &#123;</span><br><span class="line">    string n &#x3D; 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用grpc-tools工具生成消息序列化类-服务端类和客户端类"><a href="#使用grpc-tools工具生成消息序列化类-服务端类和客户端类" class="headerlink" title="使用grpc_tools工具生成消息序列化类,服务端类和客户端类"></a>使用grpc_tools工具生成消息序列化类,服务端类和客户端类</h4><ul><li>-I.: 指定协议文件的查找目录,这里为当前目录</li><li>–python_out=. 指定消息序列化类文件的输出路径</li><li>–grpc_python_out=. 指定服务端客户端类文件的输出路径</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I. --python_out&#x3D;. --grpc_python_out&#x3D;. ping.proto</span><br></pre></td></tr></table></figure><h4 id="根据服务器类，编写服务器具体逻辑实现"><a href="#根据服务器类，编写服务器具体逻辑实现" class="headerlink" title="根据服务器类，编写服务器具体逻辑实现"></a>根据服务器类，编写服务器具体逻辑实现</h4><p>创建文件 <code>server.py</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import grpc</span><br><span class="line">import time</span><br><span class="line">import ping_pb2</span><br><span class="line">import ping_pb2_grpc</span><br><span class="line">from concurrent import futures</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PingCalculatorServicer(ping_pb2_grpc.PingCalculatorServicer):</span><br><span class="line">    def Calc(self, request, ctx):</span><br><span class="line">        &quot;&quot;&quot;在这里实现业务逻辑&quot;&quot;&quot;</span><br><span class="line">        time.sleep(0.5)</span><br><span class="line">        return ping_pb2.PingResponse(n&#x3D;str(request.n))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    server &#x3D; grpc.server(futures.ThreadPoolExecutor(max_workers&#x3D;10))  # 多线程服务器</span><br><span class="line">    servicer &#x3D; PingCalculatorServicer()   # 实例化 ping 服务类</span><br><span class="line">    ping_pb2_grpc.add_PingCalculatorServicer_to_server(servicer&#x3D;servicer, server&#x3D;server)  # 注册本地服务</span><br><span class="line">    server.add_insecure_port(&#39;127.0.0.1:8080&#39;)  # 监听端口</span><br><span class="line">    server.start()  # 开始接收请求</span><br><span class="line">    try:</span><br><span class="line">        time.sleep(1000)</span><br><span class="line">    except KeyboardInterrupt:</span><br><span class="line">        server.stop(0)  # 使用 ctrl+c 可以退出服务</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="使用客户端-Stub，编写客户端交互代码"><a href="#使用客户端-Stub，编写客户端交互代码" class="headerlink" title="使用客户端 Stub，编写客户端交互代码"></a>使用客户端 Stub，编写客户端交互代码</h4><h5 id="单线程客户端"><a href="#单线程客户端" class="headerlink" title="单线程客户端"></a>单线程客户端</h5><p>创建 <code>client.py</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;单线程客户端&quot;&quot;&quot;</span><br><span class="line">import grpc</span><br><span class="line">import ping_pb2</span><br><span class="line">import ping_pb2_grpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    channel &#x3D; grpc.insecure_channel(&#39;localhost:8080&#39;)</span><br><span class="line">    # 使用 stub</span><br><span class="line">    client &#x3D; ping_pb2_grpc.PingCalculatorStub(channel)</span><br><span class="line">    # 调用吧</span><br><span class="line">    for i in range(10):</span><br><span class="line">        print(i, client.Calc(ping_pb2.PingRequest(n&#x3D;str(i))).n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="多线程客户端"><a href="#多线程客户端" class="headerlink" title="多线程客户端"></a>多线程客户端</h4><p>创建 <code>multithread_client.py</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;并行客户端&quot;&quot;&quot;</span><br><span class="line">import grpc</span><br><span class="line">import ping_pb2</span><br><span class="line">import ping_pb2_grpc</span><br><span class="line">from concurrent import futures</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ping(client, n):</span><br><span class="line">    return client.Calc(ping_pb2.PingRequest(n&#x3D;str(n))).n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    channel &#x3D; grpc.insecure_channel(&quot;127.0.0.1:8080&quot;)</span><br><span class="line">    client &#x3D; ping_pb2_grpc.PingCalculatorStub(channel&#x3D;channel)</span><br><span class="line">    pool &#x3D; futures.ThreadPoolExecutor(max_workers&#x3D;4)  # 客户端使用线程池执行</span><br><span class="line">    results &#x3D; []</span><br><span class="line">    for i in range(10):</span><br><span class="line">        results.append((i, pool.submit(ping, client, str(i))))</span><br><span class="line"></span><br><span class="line">    # 等待所有任务执行完毕</span><br><span class="line">    pool.shutdown()</span><br><span class="line"></span><br><span class="line">    for i, future in results:</span><br><span class="line">        print(i, future.result())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="分别运行服务器和客户端，观察输出结果"><a href="#分别运行服务器和客户端，观察输出结果" class="headerlink" title="分别运行服务器和客户端，观察输出结果"></a>分别运行服务器和客户端，观察输出结果</h4><p>对比单线程和多线程客户端</p><p><img src="/images/prc/rpc-grpc-python-ping-1.png" alt="image"></p><p>从结果可以看出,多线程客户端可以很好的提升效率</p><h3 id="grpc-流模式编程示例"><a href="#grpc-流模式编程示例" class="headerlink" title="grpc 流模式编程示例"></a>grpc 流模式编程示例</h3><p>相较于普通模式,流模式(Streaming) 可以理解为 gRPC 的异步调用</p><h4 id="编写协议文件"><a href="#编写协议文件" class="headerlink" title="编写协议文件"></a>编写协议文件</h4><p>创建 <code>ping.proto</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">syntax &#x3D; &quot;proto3&quot;;</span><br><span class="line"></span><br><span class="line">package ping;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping 服务,注意在输入输出类型上增加了 stream 关键字</span><br><span class="line">service PingCalculator &#123;</span><br><span class="line">    &#x2F;&#x2F; ping method</span><br><span class="line">    rpc Calc(stream PingRequest) returns (stream PingRequest) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping 请求</span><br><span class="line">message PingRequest &#123;</span><br><span class="line">    int32 n &#x3D; 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; ping 响应, 注意要与请求协议中的字段一一对应</span><br><span class="line">message PingResponse &#123;</span><br><span class="line">    int32 n &#x3D; 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用grpc-tools工具生成消息序列化类-服务端类和客户端类-1"><a href="#使用grpc-tools工具生成消息序列化类-服务端类和客户端类-1" class="headerlink" title="使用grpc_tools工具生成消息序列化类,服务端类和客户端类"></a>使用grpc_tools工具生成消息序列化类,服务端类和客户端类</h4><ul><li>-I.: 指定协议文件的查找目录,这里为当前目录</li><li>–python_out=. 指定消息序列化类文件的输出路径</li><li>–grpc_python_out=. 指定服务端客户端类文件的输出路径</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I. --python_out&#x3D;. --grpc_python_out&#x3D;. ping.proto</span><br></pre></td></tr></table></figure><h4 id="根据服务器类，编写服务器具体逻辑实现-1"><a href="#根据服务器类，编写服务器具体逻辑实现-1" class="headerlink" title="根据服务器类，编写服务器具体逻辑实现"></a>根据服务器类，编写服务器具体逻辑实现</h4><p>创建 <code>server.py</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import grpc</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line">from concurrent import futures</span><br><span class="line">import ping_pb2</span><br><span class="line">import ping_pb2_grpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PingCalculatorServer(ping_pb2_grpc.PingCalculatorServicer):</span><br><span class="line">    def Calc(self, request_iterator, ctx):</span><br><span class="line">        # 请求是一个迭代器,响应是一个生成器</span><br><span class="line">        # request 是一个迭代器参数,对应的是一个 stream 请求</span><br><span class="line">        for request in request_iterator:</span><br><span class="line">            # 50% 的概率会有响应, 为了演示请求和响应不是一对一的效果</span><br><span class="line">            if random.randint(0, 1) &#x3D;&#x3D; 1:</span><br><span class="line">                continue</span><br><span class="line">            yield ping_pb2.PingResponse(n&#x3D;request.n)   # 响应是一个生成器, 一个响应对应一个请求</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    server &#x3D; grpc.server(futures.ThreadPoolExecutor(max_workers&#x3D;10))</span><br><span class="line">    servicer &#x3D; PingCalculatorServer()</span><br><span class="line">    ping_pb2_grpc.add_PingCalculatorServicer_to_server(servicer, server)</span><br><span class="line">    server.add_insecure_port(&quot;127.0.0.1:8083&quot;)</span><br><span class="line">    server.start()</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        time.sleep(1000)</span><br><span class="line">    except KeyboardInterrupt:</span><br><span class="line">        server.stop(0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="使用客户端-Stub，编写客户端交互代码-1"><a href="#使用客户端-Stub，编写客户端交互代码-1" class="headerlink" title="使用客户端 Stub，编写客户端交互代码"></a>使用客户端 Stub，编写客户端交互代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import grpc</span><br><span class="line">import ping_pb2</span><br><span class="line">import ping_pb2_grpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_request():</span><br><span class="line">    for i in range(1000):</span><br><span class="line">        yield ping_pb2.PingRequest(n&#x3D;i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    channel &#x3D; grpc.insecure_channel(&quot;127.0.0.1:8083&quot;)</span><br><span class="line">    client &#x3D; ping_pb2_grpc.PingCalculatorStub(channel)</span><br><span class="line">    response_iterator &#x3D; client.Calc(generate_request())</span><br><span class="line">    # 请求是一个生成器,响应是一个迭代器</span><br><span class="line">    for response in response_iterator:</span><br><span class="line">        print(&quot;ping&quot;, response.n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="分别运行服务器和客户端，观察输出结果-1"><a href="#分别运行服务器和客户端，观察输出结果-1" class="headerlink" title="分别运行服务器和客户端，观察输出结果"></a>分别运行服务器和客户端，观察输出结果</h4><p><img src="/images/rpc/rpc-grpc-python-ping-2.png" alt="image"></p><p>通过实验可以看出, 有很多请求被跳过了.</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> RPC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> RPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于zookeeper的分布式RPC(python)</title>
      <link href="/2018-05-27-rpc-distribute-zookeeper-python.html"/>
      <url>/2018-05-27-rpc-distribute-zookeeper-python.html</url>
      
        <content type="html"><![CDATA[<p>基于 zookeeper 实现分布式的 RPC服务,服务节点自由伸缩,客户端可以动态的接收到服务节点的变更</p><a id="more"></a><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line">import random</span><br><span class="line">from kazoo.client import KazooClient</span><br><span class="line"></span><br><span class="line">zk_root &#x3D; &quot;&#x2F;demo&quot;</span><br><span class="line"># 全局变量,RemoteServer 对象列表</span><br><span class="line">G &#x3D; &#123;&quot;servers&quot;: None&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def random_server():</span><br><span class="line">    &quot;&quot;&quot;随机获取一个服务节点&quot;&quot;&quot;</span><br><span class="line">    if G[&quot;servers&quot;] is None:</span><br><span class="line">        # 首次初始化服务列表</span><br><span class="line">        get_servers()</span><br><span class="line">    if not G[&quot;servers&quot;]:</span><br><span class="line">        return</span><br><span class="line">    return random.choice(G[&quot;servers&quot;])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_servers():</span><br><span class="line">    &quot;&quot;&quot;服务发现 获取服务节点列表&quot;&quot;&quot;</span><br><span class="line">    zk &#x3D; KazooClient(hosts&#x3D;&quot;127.0.0.1:2181&quot;)</span><br><span class="line">    zk.start()</span><br><span class="line">    # 当前活跃地址列表</span><br><span class="line">    current_addrs &#x3D; set()</span><br><span class="line"></span><br><span class="line">    def watch_servers(*args):</span><br><span class="line">        &quot;&quot;&quot;服务变更通知 监听服务列表变更&quot;&quot;&quot;</span><br><span class="line">        new_addrs &#x3D; set()</span><br><span class="line">        # 获取新的服务地址列表,并支持监听服务列表变动</span><br><span class="line">        for child in zk.get_children(zk_root, watch&#x3D;watch_servers):</span><br><span class="line">            node &#x3D; zk.get(zk_root + &quot;&#x2F;&quot; + child)</span><br><span class="line">            addr &#x3D; json.loads(node[0])</span><br><span class="line">            new_addrs.add(&quot;&#123;&#125;:&#123;&#125;&quot;.format(addr[&quot;host&quot;], addr[&quot;port&quot;]))</span><br><span class="line"></span><br><span class="line">        del_addrs &#x3D; current_addrs - new_addrs  # # 服务列表变更后,原列表中要删除的服务地址</span><br><span class="line">        del_servers &#x3D; []</span><br><span class="line"></span><br><span class="line">        # 服务列表变更后,原列表要删除的RemoteServerr对象</span><br><span class="line">        for addr in del_addrs:</span><br><span class="line">            for s in G[&quot;servers&quot;]:</span><br><span class="line">                if s.addr &#x3D;&#x3D; addr:</span><br><span class="line">                    del_servers.append(s)</span><br><span class="line">                    break</span><br><span class="line"></span><br><span class="line">        # 删除待删除的RemoteServer</span><br><span class="line">        for server in del_servers:</span><br><span class="line">            G[&quot;servers&quot;].remove(server)</span><br><span class="line">            current_addrs.remove(server.addr)</span><br><span class="line"></span><br><span class="line">        add_addrs &#x3D; new_addrs - current_addrs  # 新增的地址</span><br><span class="line">        # 新增server</span><br><span class="line">        for addr in add_addrs:</span><br><span class="line">            G[&quot;servers&quot;].append(RemoteServer(addr))</span><br><span class="line">            current_addrs.add(addr)</span><br><span class="line"></span><br><span class="line">    # 获取节点列表并持续监听服务列表变更</span><br><span class="line">    for child in zk.get_children(zk_root, watch&#x3D;watch_servers):</span><br><span class="line">        node &#x3D; zk.get(zk_root + &quot;&#x2F;&quot; + child)</span><br><span class="line">        addr &#x3D; json.loads(node[0].decode())</span><br><span class="line">        current_addrs.add(&quot;&#123;&#125;:&#123;&#125;&quot;.format(addr[&quot;host&quot;], addr[&quot;port&quot;]))</span><br><span class="line"></span><br><span class="line">    G[&quot;servers&quot;] &#x3D; [RemoteServer(s) for s in current_addrs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RemoteServer:</span><br><span class="line">    &quot;&quot;&quot;封装rpc套接字对象&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, addr):</span><br><span class="line">        self.addr &#x3D; addr</span><br><span class="line">        self._socket &#x3D; None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def socket(self):</span><br><span class="line">        &quot;&quot;&quot;惰性连接&quot;&quot;&quot;</span><br><span class="line">        if not self._socket:</span><br><span class="line">            self.connect()</span><br><span class="line">        return self._socket</span><br><span class="line"></span><br><span class="line">    def connect(self):</span><br><span class="line">        &quot;&quot;&quot;创建连接&quot;&quot;&quot;</span><br><span class="line">        sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">        host, port &#x3D; self.addr.split(&quot;:&quot;)</span><br><span class="line">        sock.connect((host, int(port)))</span><br><span class="line">        self._socket &#x3D; sock</span><br><span class="line"></span><br><span class="line">    def reconnect(self):</span><br><span class="line">        &quot;&quot;&quot;重连&quot;&quot;&quot;</span><br><span class="line">        self.close()</span><br><span class="line">        self.connect()</span><br><span class="line"></span><br><span class="line">    def close(self):</span><br><span class="line">        &quot;&quot;&quot;关闭连接&quot;&quot;&quot;</span><br><span class="line">        if self._socket:</span><br><span class="line">            self._socket.close()</span><br><span class="line">            self._socket &#x3D; None</span><br><span class="line"></span><br><span class="line">    def rpc(self, in_, params):</span><br><span class="line">        &quot;&quot;&quot;处理请求&quot;&quot;&quot;</span><br><span class="line">        sock &#x3D; self.socket</span><br><span class="line">        request &#x3D; json.dumps(&#123;&quot;in&quot;: in_, &quot;params&quot;: params&#125;)</span><br><span class="line">        length_prefix &#x3D; struct.pack(&quot;I&quot;, len(request))</span><br><span class="line">        sock.send(length_prefix)</span><br><span class="line">        sock.sendall(request.encode())</span><br><span class="line">        length_prefix &#x3D; sock.recv(4)</span><br><span class="line">        length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">        body &#x3D; sock.recv(length)</span><br><span class="line">        response &#x3D; json.loads(body.decode())</span><br><span class="line">        return response[&quot;out&quot;], response[&quot;result&quot;]</span><br><span class="line"></span><br><span class="line">    def ping(self, message):</span><br><span class="line">        return self.rpc(&quot;ping&quot;, message)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    for i in range(100):</span><br><span class="line">        server &#x3D; random_server()</span><br><span class="line">        if not server:</span><br><span class="line">            break  # 如果没有节点存活，就退出</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        try:</span><br><span class="line">            out, result &#x3D; server.ping(&quot;hello &#123;&#125;&quot;.format(i))</span><br><span class="line">            print(server.addr, out, result)</span><br><span class="line">        except Exception as ex:</span><br><span class="line">            server.close()  # 遇到错误，关闭连接</span><br><span class="line">            print(ex)</span><br><span class="line"></span><br><span class="line">        server &#x3D; random_server()</span><br><span class="line">        if not server:</span><br><span class="line">            break</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        try:</span><br><span class="line">            out, result &#x3D; server.pi(i)</span><br><span class="line">            print(server.addr, out, result)</span><br><span class="line">        except Exception as ex:</span><br><span class="line">            server.close()</span><br><span class="line">            print(ex)</span><br></pre></td></tr></table></figure><h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;分布式多进程 RPC 服务&quot;&quot;&quot;</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">import errno</span><br><span class="line">import struct</span><br><span class="line">import signal</span><br><span class="line">import socket</span><br><span class="line">import asyncore</span><br><span class="line">from io import BytesIO</span><br><span class="line">from kazoo.client import KazooClient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RPCServer(asyncore.dispatcher):</span><br><span class="line">    zk_root &#x3D; &quot;&#x2F;demo&quot;</span><br><span class="line">    zk_rpc &#x3D; zk_root + &quot;&#x2F;rpc&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, host, port):</span><br><span class="line">        asyncore.dispatcher.__init__(self)</span><br><span class="line">        self.host &#x3D; host</span><br><span class="line">        self.port &#x3D; port</span><br><span class="line">        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">        self.set_reuse_addr()</span><br><span class="line">        self.bind((host, port))</span><br><span class="line">        self.listen(1)</span><br><span class="line">        self.child_pids &#x3D; []</span><br><span class="line">        # 创建子进程</span><br><span class="line">        if self.prefork(10):</span><br><span class="line">            self.register_zk()  # 父进程 注册 zookeeper 服务</span><br><span class="line">            self.register_parent_signal()  # 父进程善后处理</span><br><span class="line">        else:</span><br><span class="line">            self.register_child_signal()  # 子进程善后处理</span><br><span class="line"></span><br><span class="line">    def prefork(self, n):</span><br><span class="line">        &quot;&quot;&quot;创建子进程 父进程中返回 True, 子进程返回 False&quot;&quot;&quot;</span><br><span class="line">        for i in range(n):</span><br><span class="line">            pid &#x3D; os.fork()</span><br><span class="line">            if pid &lt; 0:</span><br><span class="line">                raise RuntimeError()</span><br><span class="line">            if pid &gt; 0:</span><br><span class="line">                self.child_pids.append(pid)  # 父进程,记录下子进程的pid</span><br><span class="line">                continue</span><br><span class="line">            if pid &#x3D;&#x3D; 0:  # 子进程</span><br><span class="line">                return False</span><br><span class="line">        return True</span><br><span class="line"></span><br><span class="line">    def register_zk(self):</span><br><span class="line">        &quot;&quot;&quot;父进程创建zookeeper连接&quot;&quot;&quot;</span><br><span class="line">        self.zk &#x3D; KazooClient(hosts&#x3D;&#39;127.0.0.1:2181&#39;)</span><br><span class="line">        self.zk.start()</span><br><span class="line">        # 创建根节点</span><br><span class="line">        self.zk.ensure_path(self.zk_root)</span><br><span class="line">        value &#x3D; json.dumps(&#123;&quot;host&quot;: self.host, &quot;port&quot;: self.port&#125;)</span><br><span class="line">        # 创建服务临时子节点, 路径后缀索引</span><br><span class="line">        self.zk.create(self.zk_rpc, value.encode(), ephemeral&#x3D;True, sequence&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def register_parent_signal(self):</span><br><span class="line">        &quot;&quot;&quot;父进程监听信号量&quot;&quot;&quot;</span><br><span class="line">        signal.signal(signal.SIGINT, self.exit_parent)  # 监听父进程退出</span><br><span class="line">        signal.signal(signal.SIGTERM, self.exit_parent)  # 监听父进程退出</span><br><span class="line">        signal.signal(signal.SIGCHLD, self.reap_child)  # 监听子进程退出, 处理意外退出的子进程,避免僵尸进程</span><br><span class="line"></span><br><span class="line">    def exit_parent(self, sig, frame):</span><br><span class="line">        &quot;&quot;&quot;父进程监听到 sigint 和 sigterm 信号, 关闭所有连接所有子进程&quot;&quot;&quot;</span><br><span class="line">        self.zk.stop()  # 关闭 zk 客户端</span><br><span class="line">        self.close()  # 关闭 serversocker</span><br><span class="line">        asyncore.close_all()  # 关闭所有 clientsocket</span><br><span class="line">        pids &#x3D; []</span><br><span class="line">        # 关闭子进程</span><br><span class="line">        for pid in self.child_pids:</span><br><span class="line">            print(&quot;before kill&quot;)</span><br><span class="line">            try:</span><br><span class="line">                os.kill(pid, signal.SIGINT)  # 关闭子进程</span><br><span class="line">                pids.append(pid)</span><br><span class="line">            except OSError as ex:</span><br><span class="line">                # 目标子进程已经提前挂了</span><br><span class="line">                if ex.args[0] &#x3D;&#x3D; errno.ECHILD:</span><br><span class="line">                    continue</span><br><span class="line">                raise ex</span><br><span class="line">            print(&quot;after kill &quot;, pid)</span><br><span class="line">        # 收割目标子进程</span><br><span class="line">        for pid in pids:</span><br><span class="line">            while True:</span><br><span class="line">                try:</span><br><span class="line">                    # 子进程退出后,父进程必须通过 waitpid 收割子进程,否则子进程会称为僵尸进程</span><br><span class="line">                    os.waitpid(pid, 0)  # 收割目标子进程</span><br><span class="line">                    break</span><br><span class="line">                except OSError as ex:</span><br><span class="line">                    # 子进程已经被收割过了</span><br><span class="line">                    if ex.args[0] &#x3D;&#x3D; errno.ECHILD:</span><br><span class="line">                        break</span><br><span class="line">                    if ex.args[0] !&#x3D; errno.EINTR:</span><br><span class="line">                        raise ex  # 被其它信号打断了,要重试</span><br><span class="line">            print(&quot;wait over&quot;, pid)</span><br><span class="line"></span><br><span class="line">    def reap_child(self, sig, frame):</span><br><span class="line">        &quot;&quot;&quot;父进程监听到 sigchld 信号, 退出子进程&quot;&quot;&quot;</span><br><span class="line">        print(&quot;before reap&quot;)</span><br><span class="line">        while True:</span><br><span class="line">            try:</span><br><span class="line">                info &#x3D; os.waitpid(-1, os.WNOHANG)  # 收割任意子进程</span><br><span class="line">                break</span><br><span class="line">            except OSError as ex:</span><br><span class="line">                # 子进程已经被收割</span><br><span class="line">                if ex.args[0] &#x3D;&#x3D; errno.ECHILD:</span><br><span class="line">                    return</span><br><span class="line">                # 被其他信号打断要重试</span><br><span class="line">                if ex.args[0] !&#x3D; errno.EINTR:</span><br><span class="line">                    raise ex</span><br><span class="line">        pid &#x3D; info[0]</span><br><span class="line">        try:</span><br><span class="line">            self.child_pids.remove(pid)</span><br><span class="line">        except ValueError:</span><br><span class="line">            pass</span><br><span class="line">        print(&quot;after reap&quot;, pid)</span><br><span class="line"></span><br><span class="line">    def register_child_signal(self):</span><br><span class="line">        &quot;&quot;&quot;子进程监听信号&quot;&quot;&quot;</span><br><span class="line">        signal.signal(signal.SIGINT, self.exit_child)  # 退出子进程</span><br><span class="line">        signal.signal(signal.SIGTERM, self.exit_child)  # 退出子进程</span><br><span class="line"></span><br><span class="line">    def exit_child(self, sig, frame):</span><br><span class="line">        &quot;&quot;&quot;子进程监听到 sigint 和 sigterm 信号, 关闭子进程所有连接&quot;&quot;&quot;</span><br><span class="line">        self.close()  # 关闭所有 server socket</span><br><span class="line">        asyncore.close_all()  # 关闭所有 client socket</span><br><span class="line">        print(&quot;all closed&quot;)</span><br><span class="line"></span><br><span class="line">    def handle_accept(self):</span><br><span class="line">        &quot;&quot;&quot;接收连接&quot;&quot;&quot;</span><br><span class="line">        pair &#x3D; self.accept()</span><br><span class="line">        if pair is not None:</span><br><span class="line">            sock, addr &#x3D; pair</span><br><span class="line">            RPCHandler(sock, addr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RPCHandler(asyncore.dispatcher_with_send):</span><br><span class="line">    &quot;&quot;&quot;处理请求&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, sock, addr):</span><br><span class="line">        asyncore.dispatcher_with_send.__init__(self, sock)</span><br><span class="line">        self.addr &#x3D; addr</span><br><span class="line">        self.handlers &#x3D; &#123;</span><br><span class="line">            &quot;ping&quot;: self.ping,</span><br><span class="line">        &#125;</span><br><span class="line">        self.rbuf &#x3D; BytesIO()</span><br><span class="line"></span><br><span class="line">    def handle_connect(self):</span><br><span class="line">        print(self.addr, &quot;comes&quot;)</span><br><span class="line"></span><br><span class="line">    def handle_read(self):</span><br><span class="line">        &quot;&quot;&quot;接收请求&quot;&quot;&quot;</span><br><span class="line">        while True:</span><br><span class="line">            connect &#x3D; self.recv(1024)</span><br><span class="line">            if connect:</span><br><span class="line">                self.rbuf.write(connect)</span><br><span class="line">                if len(connect) &lt; 1024:</span><br><span class="line">                    break</span><br><span class="line">        self.handle_rpc()</span><br><span class="line"></span><br><span class="line">    def handle_rpc(self):</span><br><span class="line">        &quot;&quot;&quot;处理一个接收一个完整的请求&quot;&quot;&quot;</span><br><span class="line">        while True:</span><br><span class="line">            self.rbuf.seek(0)</span><br><span class="line">            length_prefix &#x3D; self.rbuf.read(4)</span><br><span class="line">            if len(length_prefix) &lt; 4:</span><br><span class="line">                break</span><br><span class="line">            length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">            body &#x3D; self.rbuf.read(length)</span><br><span class="line">            if len(body) &lt; length:</span><br><span class="line">                break</span><br><span class="line">            request &#x3D; json.loads(body.decode())</span><br><span class="line">            in_ &#x3D; request[&#39;in&#39;]</span><br><span class="line">            params &#x3D; request[&#39;params&#39;]</span><br><span class="line">            print(os.getpid(), in_, params)</span><br><span class="line">            handler &#x3D; self.handlers[in_]</span><br><span class="line">            handler(params)</span><br><span class="line">            left &#x3D; self.rbuf.getvalue()[length+4:]</span><br><span class="line">            self.rbuf &#x3D; BytesIO()</span><br><span class="line">            self.rbuf.write(left)</span><br><span class="line"></span><br><span class="line">    def ping(self, params):</span><br><span class="line">        self.send_result(&quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line">    def send_result(self, out, result):</span><br><span class="line">        &quot;&quot;&quot;给客户端发送消息&quot;&quot;&quot;</span><br><span class="line">        response &#x3D; &#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;</span><br><span class="line">        body &#x3D; json.dumps(response)</span><br><span class="line">        length_prefix &#x3D; struct.pack(&quot;I&quot;, len(body))</span><br><span class="line">        self.send(length_prefix)</span><br><span class="line">        self.send(body.encode())</span><br><span class="line"></span><br><span class="line">    def handle_close(self):</span><br><span class="line">        print(self.addr, &quot;bye&quot;)</span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    host &#x3D; sys.argv[1]</span><br><span class="line">    port &#x3D; int(sys.argv[2])</span><br><span class="line">    RPCServer(host, port)</span><br><span class="line">    # 启动事件循环</span><br><span class="line">    asyncore.loop()</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p><img src="/images/rpc/rpc-distribute-zookeeper-python-1.png" alt="image"></p><ul><li>启动一个服务端 <code>127.0.0.1:8080</code>, 它就会把这个地址注册到 zookeeper</li><li>启动两个客户端, 客户端会从 zookeeper 中取到服务端的地址 <code>127.0.0.1:8080</code>, 然后向服务端发送请求并收到响应</li><li>再启动一个服务端 <code>127.0.0.1&quot;8081</code>, 它同样会向 zookeeper 注册自己的节点</li><li>两个客户端从 zookeeper 接收到新增的服务端 <code>127.0.0.1:8081</code></li><li>两个客户端随机的向两个服务端发送请求且接收到响应</li><li>关闭一个服务端 <code>127.0.0.1:8081</code>, 从 zookeeper 中删除 <code>127.0.0.1:8081</code> 这个笛地址</li><li>两个客户端接收到服务地址的更改,只向 <code>127.0.0.1:8080</code> 发送请求</li><li>重新启动 <code>127.0.0.1:8081</code>, 发现客户端的请求有可以正常的发送到两个服务端</li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> RPC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> RPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPC 模型基础知识(python)</title>
      <link href="/2018-05-23-rpc-basic-python.html"/>
      <url>/2018-05-23-rpc-basic-python.html</url>
      
        <content type="html"><![CDATA[<p>手写python代码介绍各种 RPC 模型, <a href="https://github.com/waterandair/daily-learning/tree/master/learn-rpc/python" target="_blank" rel="noopener">代码地址</a></p><a id="more"></a><h3 id="什么是RPC"><a href="#什么是RPC" class="headerlink" title="什么是RPC"></a>什么是RPC</h3><p>RPC(Remote Procedure Call)即远程过程调用,用于解决分布式系统的通信问题,常见的所有基于 TCP 协议的通信方式都可以看做是一种RPC, 比如http也是一种特殊的RPC, 但通常所说的 RPC 指的是长连接交互.客户端(clien)和服务器端(server)通过 <a href="https://note.youdao.com/" target="_blank" rel="noopener">文件描述符</a> 的读写API访问操作系统内核中的网络模块,在当前套接字(socket)上按照规定好的格式发送(send buffer) 和接收缓存 (recv buffer)来完成数据的传输.RPC可以理解为是对底层通信和交互协议的一个封装.</p><h3 id="RPC-的一般过程"><a href="#RPC-的一般过程" class="headerlink" title="RPC 的一般过程"></a>RPC 的一般过程</h3><p><img src="/images/rpc/rpc-basic-python-1.png" alt="image"></p><ul><li>server 端监听本地端口,等待客户端连接</li><li>客户端连接server端</li><li>客户端发出请求,等待服务端响应</li><li>服务端接收到客户端发送的请求,处理后返回响应</li><li>客户端接收到服务器发送过来响应</li><li>客户端关闭连接</li></ul><p><strong>最简单的代码实现</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># server.py</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # 打开一个ipv4的tcp套接字</span><br><span class="line">    sock.bind((&quot;localhost&quot;, 8083))</span><br><span class="line">    sock.listen(1)  # server 端监听本地端口,等待客户端连接</span><br><span class="line">    try:</span><br><span class="line">        while True:</span><br><span class="line">            conn, addr &#x3D; sock.accept()  # 接收一个客户端连接</span><br><span class="line">            print(conn.recv(1024))  # 从 recv buffer中读取消息, 服务端接收到客户端发送的请求,处理后返回响应</span><br><span class="line">            conn.sendall(b&quot;pong&quot;)  # 将响应发送到 send buffer</span><br><span class="line">            conn.close()</span><br><span class="line">    except KeyboardInterrupt:</span><br><span class="line">        sock.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># client</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    sock.connect((&quot;localhost&quot;, 8083))  # 客户端连接server端</span><br><span class="line">    sock.sendall(b&quot;ping&quot;)  # 将消息发送到 send buffer, 客户端发出请求,等待服务端响应</span><br><span class="line">    print(sock.recv(1024))  # 从 recv buffer 中读取响应, 客户端接收到服务器发送过来响应</span><br><span class="line">    sock.close()  # 客户端关闭连接</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="RPC-客户端"><a href="#RPC-客户端" class="headerlink" title="RPC 客户端"></a>RPC 客户端</h3><p><a href="https://note.youdao.com/" target="_blank" rel="noopener">消息协议</a>的边界使用长度前缀法,使用一个4字节的二进制字符串表示消息体的长度,消息体使用 json 格式进行序列化和发序列化</p><p>消息的结构规定为下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 请求</span><br><span class="line">&#123;</span><br><span class="line">    in: &quot;ping&quot;,</span><br><span class="line">    params: &quot;0&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 响应</span><br><span class="line">&#123;</span><br><span class="line">    out: &quot;pong&quot;</span><br><span class="line">    result: &quot;0&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>客户端代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;rpc客户端&quot;&quot;&quot;</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def rpc(sock, in_, params):</span><br><span class="line">    # 根据协议构造请求</span><br><span class="line">    request &#x3D; json.dumps(&#123;&quot;in&quot;: in_, &quot;params&quot;: params&#125;)  # 序列化请求消息体</span><br><span class="line">    length_prefix &#x3D; struct.pack(&quot;I&quot;, len(request))  # 请求长度前缀, 将一个整数编码成 4 个字节的字符串, &quot;I&quot; 表示无符号的整数</span><br><span class="line">    sock.send(length_prefix)</span><br><span class="line">    sock.sendall(request.encode())  # sendall 会执行 flush</span><br><span class="line"></span><br><span class="line">    # 根据协议解析响应</span><br><span class="line">    res_length_prefix &#x3D; sock.recv(4)  # 接收响应长度前缀</span><br><span class="line">    length, &#x3D; struct.unpack(&quot;I&quot;, res_length_prefix)</span><br><span class="line">    body &#x3D; sock.recv(length)  # 接收指定长度的响应消息体</span><br><span class="line">    response &#x3D; json.loads(body.decode())</span><br><span class="line">    return response[&quot;out&quot;], response[&quot;result&quot;]  # 返回响应类型和结果</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    s &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    s.connect((&quot;localhost&quot;, 8080))</span><br><span class="line">    addr, port &#x3D; s.getsockname()</span><br><span class="line">    print(addr, port)  # 打印出客户端的地址和占用的端口</span><br><span class="line">    # 连续发送10个请求</span><br><span class="line">    for i in range(10):</span><br><span class="line">        out, result &#x3D; rpc(s, &quot;ping&quot;, &quot;hello &#123;&#125;&quot;.format(i))</span><br><span class="line">        print(out, result)</span><br><span class="line">        time.sleep(1)</span><br><span class="line">    s.close()</span><br></pre></td></tr></table></figure><h3 id="RPC-服务端"><a href="#RPC-服务端" class="headerlink" title="RPC 服务端"></a>RPC 服务端</h3><h4 id="同步模型"><a href="#同步模型" class="headerlink" title="同步模型"></a>同步模型</h4><h5 id="单线程同步模型"><a href="#单线程同步模型" class="headerlink" title="单线程同步模型"></a>单线程同步模型</h5><p>单线程同步模型一次只能处理一个连接,这个请求处理完毕并关闭连接后,才能再处理下一个连接.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;单线程同步模型rpc服务器&quot;&quot;&quot;</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def handle_conn(conn, addr, handlers):</span><br><span class="line">    &quot;&quot;&quot;接收并处理请求&quot;&quot;&quot;</span><br><span class="line">    print(addr, &quot;comes&quot;)</span><br><span class="line">    # 循环读写</span><br><span class="line">    while True:</span><br><span class="line">        length_prefix &#x3D; conn.recv(4)  # 接收4个字节的请求长度</span><br><span class="line">        if not length_prefix:  # 连接关闭了</span><br><span class="line">            print(addr, &quot;close&quot;)</span><br><span class="line">            conn.close()</span><br><span class="line">            break  # 退出循环,处理下一个连接</span><br><span class="line"></span><br><span class="line">        length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">        body &#x3D; conn.recv(length)  # 根据接收到的消息体的长度接收请求消息体</span><br><span class="line">        request &#x3D; json.loads(body.decode())</span><br><span class="line">        in_ &#x3D; request[&quot;in&quot;]</span><br><span class="line">        params &#x3D; request[&quot;params&quot;]</span><br><span class="line">        print(in_, params)</span><br><span class="line">        handler &#x3D; handlers[in_]  # 找到响应的handler</span><br><span class="line">        handler(conn, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ping(conn, params):</span><br><span class="line">    send_result(conn, &quot;pong&quot;, params)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">def send_result(conn, out, result):</span><br><span class="line">    &quot;&quot;&quot;发送消息体&quot;&quot;&quot;</span><br><span class="line">    response &#x3D; json.dumps(&#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;)  # 构造响应消息体</span><br><span class="line">    length_prefix &#x3D; struct.pack(&quot;I&quot;, len(response))  # 编码响应长度前缀</span><br><span class="line">    conn.send(length_prefix)</span><br><span class="line">    conn.sendall(response.encode())  # sendall() 会执行 flush</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loop(sock, handlers):</span><br><span class="line">    &quot;&quot;&quot;循环接收请求&quot;&quot;&quot;</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()  # 接收连接</span><br><span class="line">        handle_conn(conn, addr, handlers)  # 单线程处理请求,处理中会阻塞其它请求</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # 创建一个基于 ipv4 的 TCP 套接字</span><br><span class="line">    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)  # 在套接字级别打开 SO_REUSEADDR</span><br><span class="line">    sock.bind((&quot;localhost&quot;, 8080))</span><br><span class="line">    sock.listen(1)  # 监听客户端连接</span><br><span class="line"></span><br><span class="line">    # 注册请求处理器, 这里只用ping服务做演示</span><br><span class="line">    handlers &#x3D; &#123;</span><br><span class="line">        &quot;ping&quot;: ping</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    loop(sock, handlers)</span><br></pre></td></tr></table></figure><p><img src="/images/rpc/rpc-basic-python-2.png" alt="image"></p><p>如图所示,同时启动两个客户端,服务器端只能先处理完第一个连接,第一个连接关闭后,在处理第二个</p><h5 id="多线程同步模型"><a href="#多线程同步模型" class="headerlink" title="多线程同步模型"></a>多线程同步模型</h5><p>相比较单线程同步模型,仅仅修改了 loop 函数, 对每个连接新开一个线程进行处理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;多线程同步模型rpc服务器&quot;&quot;&quot;</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line">from threading import Thread</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def handle_conn(conn, addr, handlers):</span><br><span class="line">    &quot;&quot;&quot;接收并处理请求&quot;&quot;&quot;</span><br><span class="line">    print(addr, &quot;comes&quot;)</span><br><span class="line">    # 循环读写</span><br><span class="line">    while True:</span><br><span class="line">        length_prefix &#x3D; conn.recv(4)  # 接收请求长度</span><br><span class="line">        if not length_prefix:  # 连接关闭了</span><br><span class="line">            print(addr, &quot;close&quot;)</span><br><span class="line">            conn.close()</span><br><span class="line">            break  # 退出循环,处理下一个连接</span><br><span class="line"></span><br><span class="line">        length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">        body &#x3D; conn.recv(length)  # 接收请求消息体</span><br><span class="line">        request &#x3D; json.loads(body.decode())</span><br><span class="line">        in_ &#x3D; request[&quot;in&quot;]</span><br><span class="line">        params &#x3D; request[&quot;params&quot;]</span><br><span class="line">        # 获取当前运行程序的CPU核</span><br><span class="line">        res &#x3D; os.popen(&#39;ps -o psr -p&#39; + str(os.getpid()))</span><br><span class="line">        cpu_id &#x3D; res.readlines()[1].rstrip(&quot;\n&quot;).strip()</span><br><span class="line">        print(in_, params, &quot;|&quot;, &quot;from:&quot;, addr, &quot;|&quot;, &quot;cpu: &quot; + cpu_id)</span><br><span class="line">        handler &#x3D; handlers[in_]  # 找到请求处理器</span><br><span class="line">        handler(conn, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def send_result(conn, out, result):</span><br><span class="line">    &quot;&quot;&quot;发送消息体&quot;&quot;&quot;</span><br><span class="line">    response &#x3D; json.dumps(&#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;)  # 构造响应消息体</span><br><span class="line">    length_prefix &#x3D; struct.pack(&quot;I&quot;, len(response))  # 编码响应长度前缀</span><br><span class="line">    conn.send(length_prefix)</span><br><span class="line">    conn.sendall(response.encode())  # sendall() 会执行 flush</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ping(conn, params):</span><br><span class="line">    send_result(conn, &quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loop(sock, handlers):</span><br><span class="line">    &quot;&quot;&quot;循环接收请求&quot;&quot;&quot;</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()  # 接收连接</span><br><span class="line">        t &#x3D; Thread(target&#x3D;handle_conn, args&#x3D;(conn, addr, handlers))  # 每接收一个新的请求,就会在一个新的线程中处理请求</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # 创建一个基于 ipv4 的 TCP 套接字</span><br><span class="line">    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)  # 在套接字级别打开 SO_REUSEADDR</span><br><span class="line">    sock.bind((&quot;localhost&quot;, 8080))</span><br><span class="line">    sock.listen(1)  # 监听客户端连接</span><br><span class="line"></span><br><span class="line">    # 注册请求处理器</span><br><span class="line">    handlers &#x3D; &#123;</span><br><span class="line">        &quot;ping&quot;: ping</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # 进入服务循环</span><br><span class="line">    loop(sock, handlers)</span><br></pre></td></tr></table></figure><p><img src="/images/rpc/rpc-basic-python-3.png" alt="image"></p><p>从实验中可以看出,前后打开了两个客户端,服务器端不需要等待第一个客户端连接关闭,就可以处理第二个客户端连接.</p><h5 id="多进程同步模型"><a href="#多进程同步模型" class="headerlink" title="多进程同步模型"></a>多进程同步模型</h5><p>Python 的 GIL 导致单个进程只能占满一个 CPU 核心,导致多线程并不能充分利用多核的优势,<br>所以多数 Python 服务器更推荐使用多进程模型 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;多进程同步模型&quot;&quot;&quot;</span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line">import multiprocessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loop_multiprocessing(sock, handlers):</span><br><span class="line">    &quot;&quot;&quot;使用 multiprocessing 的方式为每一个连接创建一个新的进程进行处理&quot;&quot;&quot;</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()</span><br><span class="line">        p &#x3D; multiprocessing.Process(target&#x3D;handler_conn, args&#x3D;(conn, addr, handlers))</span><br><span class="line">        p.start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loop_fork(sock, handlers):</span><br><span class="line">    &quot;&quot;&quot;使用fork的方式为每一个连接创建一个新的进程进行处理&quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    子进程和父进程的关系:</span><br><span class="line">        子进程创建后，父进程拥有的很多操作系统资源，会复制一份给子进程,比如套接字和文件描述符，它们本质上都是对操作系统内核对象的一个引用。</span><br><span class="line">        如果子进程不需要某些引用，一定要即时关闭它，避免操作系统资源得不到释放导致资源泄露。</span><br><span class="line"></span><br><span class="line">        进程 fork 之后，套接字会复制一份套接字连接到子进程，这时父子进程将会各有自己的套接字引用指向内核的同一份套接字对象。</span><br><span class="line">        在子进程里对套接字进程 close，并不是关闭套接字，其本质上只是将内核套接字对象的引用计数减一,只有当引用计数减为零时，才会关闭套接字.</span><br><span class="line">        所以关闭子进程的套接字后,要在父进程里也关闭客户端的套接字。</span><br><span class="line">        否则就会导致服务器套接字引用计数不断增长，同时客户端套接字对象也得不到即时回收，造成资源泄露。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()</span><br><span class="line">        pid &#x3D; os.fork()  # fork 出一个子进程</span><br><span class="line">        if pid &lt; 0:  # 如果 fork 返回值小于零，一般意味着操作系统资源不足，无法创建进程。</span><br><span class="line">            return</span><br><span class="line">        elif pid &gt; 0:  # fork在父进程的返回结果是一个大于0的整数值，这个值是子进程的进程号，父进程可以使用该进程号来控制子进程</span><br><span class="line">            conn.close()  # 关闭父进程的客户端套接字,因为子进程已经复制了一份到子进程</span><br><span class="line">            continue</span><br><span class="line">        else:  # fork 在子进程的返回结果是零</span><br><span class="line">            sock.close()  # 关闭子进程的服务器套接字, 因为只需要父进程保持套接字的监听</span><br><span class="line">            handler_conn(conn, addr, handlers)</span><br><span class="line">            break  # 处理完后一定要退出循环，不然子进程也会继续去 accept 连接</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def handler_conn(conn, addr, handlers):</span><br><span class="line">    print(addr, &quot;comes&quot;)</span><br><span class="line">    while True:</span><br><span class="line">        length_prefix &#x3D; conn.recv(4)</span><br><span class="line">        if not length_prefix:</span><br><span class="line">            print(addr, &quot;bye&quot;)</span><br><span class="line">            conn.close()</span><br><span class="line">            break</span><br><span class="line">        length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">        body &#x3D; conn.recv(length).decode()</span><br><span class="line">        request &#x3D; json.loads(body)</span><br><span class="line">        in_ &#x3D; request[&quot;in&quot;]</span><br><span class="line">        params &#x3D; request[&quot;params&quot;]</span><br><span class="line">        # 获取当前运行程序的CPU核</span><br><span class="line">        res &#x3D; os.popen(&#39;ps -o psr -p&#39; + str(os.getpid()))</span><br><span class="line">        cpu_id &#x3D; res.readlines()[1].rstrip(&quot;\n&quot;).strip()</span><br><span class="line">        print(in_, params, &quot;|&quot;, &quot;from:&quot;, addr, &quot;|&quot;, &quot;cpu: &quot; + cpu_id)</span><br><span class="line">        handler &#x3D; handlers[in_]</span><br><span class="line">        handler(conn, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ping(conn, params):</span><br><span class="line">    send_result(conn, &quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def send_result(conn, out, result):</span><br><span class="line">    response &#x3D; json.dumps(&#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;).encode()</span><br><span class="line">    length_prefix &#x3D; struct.pack(&quot;I&quot;, len(response))</span><br><span class="line">    conn.send(length_prefix)</span><br><span class="line">    conn.sendall(response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)</span><br><span class="line">    sock.bind((&quot;localhost&quot;, 8080))</span><br><span class="line">    sock.listen(1)</span><br><span class="line">    handlers &#x3D; &#123;</span><br><span class="line">        &quot;ping&quot;: ping</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # loop_multiprocessing(sock, handlers)  # python中提供了multiprocessing库去方面的进行多线程编程,这里使用fork是为了方便理解多进程</span><br><span class="line">    loop_fork(sock, handlers)</span><br></pre></td></tr></table></figure><p><img src="/images/rpc/rpc-basic-python-4.png" alt="image"><br>从实验中可以看出,通多线程一样,可以同时处理多个请求,不同的一点是,多进程可以使用到多个CPU,而多线程不行</p><h5 id="Preforking-同步模型"><a href="#Preforking-同步模型" class="headerlink" title="Preforking 同步模型"></a>Preforking 同步模型</h5><p>python多进程虽然可以使用到多核,但是由于进程比进程要消耗更多的资源,所以操作系统可以运行的进程的数量是相当有限的.<br>为了过多进程对系统资源消耗过大,可以采用多进程多线程结合的方式,具体讲来,就是创建指定数量的进程,在每个进程中再创建线程去处理请求</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;preforking 同步模型&quot;&quot;&quot;</span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line">from threading import Thread</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def prefork(nums):</span><br><span class="line">    &quot;&quot;&quot;预先创建指定数量的子进程&quot;&quot;&quot;</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        pid &#x3D; os.fork()</span><br><span class="line">        if pid &lt; 0:</span><br><span class="line">            return</span><br><span class="line">        elif pid &gt; 0:</span><br><span class="line">            continue  # 父进程继续循环,继续fork子进程</span><br><span class="line">        else:  # pid &#x3D;&#x3D; 0</span><br><span class="line">            break  # 子进程退出循环处理请求</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ping(conn, params):</span><br><span class="line">    send_result(conn, &quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def send_result(conn, out, result):</span><br><span class="line">    response &#x3D; json.dumps(&#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;).encode()</span><br><span class="line">    length_prefix &#x3D; struct.pack(&quot;I&quot;, len(response))</span><br><span class="line">    conn.send(length_prefix)</span><br><span class="line">    conn.sendall(response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loop(sock, handlers):</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()</span><br><span class="line">        handle_conn(conn, addr, handlers)</span><br><span class="line">        # Thread(handle_conn, args&#x3D;(conn, addr, handlers))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def handle_conn(conn, addr, hanlers):</span><br><span class="line">    print(addr, &quot;comes&quot;)</span><br><span class="line">    while True:</span><br><span class="line">        length_prefix &#x3D; conn.recv(4)</span><br><span class="line">        if not length_prefix:</span><br><span class="line">            print(addr, &quot;bye&quot;)</span><br><span class="line">            break</span><br><span class="line">        length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">        body &#x3D; conn.recv(length).decode()</span><br><span class="line">        request &#x3D; json.loads(body)</span><br><span class="line">        in_ &#x3D; request[&quot;in&quot;]</span><br><span class="line">        params &#x3D; request[&quot;params&quot;]</span><br><span class="line">        print(in_, params)</span><br><span class="line"></span><br><span class="line">        handler &#x3D; handlers[in_]</span><br><span class="line">        handler(conn, params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    sock &#x3D; socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)</span><br><span class="line">    sock.bind((&quot;localhost&quot;, 8080))</span><br><span class="line">    sock.listen(1)</span><br><span class="line"></span><br><span class="line">    # 开启 10个 子进程</span><br><span class="line">    # prefork(10)</span><br><span class="line">    prefork(1)</span><br><span class="line"></span><br><span class="line">    handlers &#x3D; &#123;</span><br><span class="line">        &quot;ping&quot;: ping</span><br><span class="line">    &#125;</span><br><span class="line">    loop(sock, handlers)</span><br></pre></td></tr></table></figure><p><img src="/images/rpc/rpc-basic-python-5.png" alt="image"></p><p>在实验中,为了方面演示,只创建一个子进程,且只有一个线程,当主进程fork完子进程后,也会去处理请求,这样就相当于只有两个进程可以处理请求,当我依次打开三个客户端,可以看到第三个客户端一开始无法建立连接,当第一个客户端关闭连接后,才建立了连接.</p><p>在下面的试验中,在进程中使用多线程的方式处理请求,就可以处理大于进程数的请求了.</p><p>修改 loop 函数:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def loop(sock, handlers):</span><br><span class="line">    while True:</span><br><span class="line">        conn, addr &#x3D; sock.accept()</span><br><span class="line">        # handle_conn(conn, addr, handlers)</span><br><span class="line">        Thread(handle_conn, args&#x3D;(conn, addr, handlers))</span><br></pre></td></tr></table></figure><p><img src="/images/rpc/rpc-basic-python-6.png" alt="image"></p><h4 id="异步模型"><a href="#异步模型" class="headerlink" title="异步模型"></a>异步模型</h4><p>前面演示的都是同步模型,同步模型在读写数据的时候,如果数据没有就绪,就会阻塞当前线程,这种方式对操作系统资源是一种浪费,所以引进了非阻塞I/O模型,当前线程没有读写数据的时候,线程可以去做其他工作,当有数据需要读写的时候,再进行数据读写操作.</p><p>为了让线程知道何时可以进行读写数据的操作,操作系统提供了事件轮询的机制,比如apache使用的select机制和nginx使用的 epoll 机制.</p><h5 id="单进程异步模型"><a href="#单进程异步模型" class="headerlink" title="单进程异步模型"></a>单进程异步模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot; 单线程异步 RPC 服务器模型 使用 asyncore 包实现 (select 多路复用系统调用)&quot;&quot;&quot;</span><br><span class="line">import asyncore</span><br><span class="line">from io import BytesIO</span><br><span class="line">import socket</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RPCServer(asyncore.dispatcher):</span><br><span class="line">    &quot;&quot;&quot;服务器套接字处理器必须继承dispatcher&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, host, port):</span><br><span class="line">        asyncore.dispatcher.__init__(self)</span><br><span class="line">        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">        self.set_reuse_addr()</span><br><span class="line">        self.bind((host, port))</span><br><span class="line">        self.listen(1)</span><br><span class="line"></span><br><span class="line">    def handle_accept(self):</span><br><span class="line">        pair &#x3D; self.accept()</span><br><span class="line">        if pair is not None:</span><br><span class="line">            sock, addr &#x3D; pair</span><br><span class="line">            PRCHandler(sock, addr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PRCHandler(asyncore.dispatcher_with_send):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    客户端套接字处理器必须继承 dispatcher_with_send</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, sock, addr):</span><br><span class="line">        asyncore.dispatcher_with_send.__init__(self, sock&#x3D;sock)</span><br><span class="line">        self.addr &#x3D; addr</span><br><span class="line">        self.handlers &#x3D; &#123;</span><br><span class="line">            &quot;ping&quot;: self.ping</span><br><span class="line">        &#125;</span><br><span class="line">        # 因为是非阻塞的，所以可能一条消息经历了多次读取，所以这里用一个BytesIO缓冲区存放读取进来的数据</span><br><span class="line">        self.rbuf &#x3D; BytesIO()  # 读缓冲区由用户代码维护，写缓冲区由 asyncore 内部提供</span><br><span class="line"></span><br><span class="line">    def ping(self, params):</span><br><span class="line">        self.send_result(&quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line">    def send_result(self, out, result):</span><br><span class="line">        response &#x3D; &#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;</span><br><span class="line">        body &#x3D; json.dumps(response).encode()</span><br><span class="line">        length_prefix &#x3D; struct.pack(&quot;I&quot;, len(body))</span><br><span class="line">        self.send(length_prefix)</span><br><span class="line">        self.send(body)</span><br><span class="line"></span><br><span class="line">    def handle_connect(self):</span><br><span class="line">        &quot;&quot;&quot;新的连接被accept 回调方法&quot;&quot;&quot;</span><br><span class="line">        print(self.addr, &#39;comes&#39;)</span><br><span class="line"></span><br><span class="line">    def handle_close(self):</span><br><span class="line">        &quot;&quot;&quot;连接关闭之前回调方法&quot;&quot;&quot;</span><br><span class="line">        print(self.addr, &#39;bye&#39;)</span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    def handle_read(self):</span><br><span class="line">        &quot;&quot;&quot;有读事件到来时回调方法&quot;&quot;&quot;</span><br><span class="line">        while True:</span><br><span class="line">            content &#x3D; self.recv(1024)</span><br><span class="line">            if content:</span><br><span class="line">                self.rbuf.write(content)</span><br><span class="line">            # 最后一部分数据,读取后退出循环</span><br><span class="line">            if len(content) &lt; 1024:</span><br><span class="line">                break</span><br><span class="line">        self.handle_rpc()</span><br><span class="line"></span><br><span class="line">    def handle_rpc(self):</span><br><span class="line">        &quot;&quot;&quot;将读到的消息解包并处理&quot;&quot;&quot;</span><br><span class="line">        while True:  # 可能一次性收到了多个请求消息，所以需要循环处理</span><br><span class="line">            self.rbuf.seek(0)</span><br><span class="line">            length_prefix &#x3D; self.rbuf.read(4)</span><br><span class="line">            if len(length_prefix) &lt; 4:  # 不足一个消息</span><br><span class="line">                break</span><br><span class="line">            length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">            body &#x3D; self.rbuf.read(length)</span><br><span class="line">            if len(body) &lt; length:  # 不足一个消息</span><br><span class="line">                break</span><br><span class="line">            request &#x3D; json.loads(body.decode())</span><br><span class="line">            in_ &#x3D; request[&#39;in&#39;]</span><br><span class="line">            params &#x3D; request[&#39;params&#39;]</span><br><span class="line">            print(in_, params)</span><br><span class="line">            handler &#x3D; self.handlers[in_]</span><br><span class="line">            handler(params)  # 处理消息</span><br><span class="line">            left &#x3D; self.rbuf.getvalue()[length + 4:]  # 消息处理完了，缓冲区要截断</span><br><span class="line">            self.rbuf &#x3D; BytesIO()</span><br><span class="line">            self.rbuf.write(left)</span><br><span class="line">        self.rbuf.seek(0, 2)  # 将游标挪到文件结尾，以便后续读到的内容直接追加</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    RPCServer(&quot;localhost&quot;, 8080)</span><br><span class="line">    asyncore.loop()</span><br></pre></td></tr></table></figure><h5 id="Preforking-异步模型"><a href="#Preforking-异步模型" class="headerlink" title="Preforking 异步模型"></a>Preforking 异步模型</h5><p>和单进程同步模型一样,受 Python GIL 的影响,单进程异步模型也只能使用一个CPU,为了利用好多核,可以改用 Preforking 的异步模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;PerForking 异步 RPC 服务器&quot;&quot;&quot;</span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import struct</span><br><span class="line">import socket</span><br><span class="line">import asyncore</span><br><span class="line">from io import BytesIO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RPCServer(asyncore.dispatcher):</span><br><span class="line">    def __init__(self, host, port):</span><br><span class="line">        asyncore.dispatcher.__init__(self)</span><br><span class="line">        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">        self.set_reuse_addr()</span><br><span class="line">        self.bind((host, port))</span><br><span class="line">        self.listen(1)</span><br><span class="line">        self.prefork(10)  # 创建10个子进程</span><br><span class="line"></span><br><span class="line">    def prefork(self, n):</span><br><span class="line">        for i in range(n):</span><br><span class="line">            pid &#x3D; os.fork()</span><br><span class="line">            if pid &lt; 0:  # fork error</span><br><span class="line">                return</span><br><span class="line">            if pid &gt; 0:  # 父进程</span><br><span class="line">                continue</span><br><span class="line">            if pid &#x3D;&#x3D; 0:</span><br><span class="line">                break  # 子进程</span><br><span class="line"></span><br><span class="line">    def handle_accept(self):</span><br><span class="line">        # 获取连接</span><br><span class="line">        pair &#x3D; self.accept()</span><br><span class="line">        if pair is not None:</span><br><span class="line">            sock, addr &#x3D; pair</span><br><span class="line">            # 处理连接</span><br><span class="line">            RPCHandler(sock, addr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RPCHandler(asyncore.dispatcher_with_send):</span><br><span class="line">    def __init__(self, sock, addr):</span><br><span class="line">        asyncore.dispatcher_with_send.__init__(self, sock&#x3D;sock)</span><br><span class="line">        self.addr &#x3D; addr</span><br><span class="line">        self.handlers &#x3D; &#123;</span><br><span class="line">            &quot;ping&quot;: self.ping</span><br><span class="line">        &#125;</span><br><span class="line">        # 因为是非阻塞的，所以可能一条消息经历了多次读取，所以这里用一个BytesIO缓冲区存放读取进来的数据</span><br><span class="line">        self.rbuf &#x3D; BytesIO()</span><br><span class="line"></span><br><span class="line">    def handle_connect(self):</span><br><span class="line">        print(self.addr, &quot;comes&quot;)</span><br><span class="line"></span><br><span class="line">    def handle_close(self):</span><br><span class="line">        print(self.addr, &quot;bye&quot;)</span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    def handle_read(self):</span><br><span class="line">        while True:</span><br><span class="line">            content &#x3D; self.recv(1024)</span><br><span class="line">            if content:</span><br><span class="line">                # 追加到读缓冲</span><br><span class="line">                self.rbuf.write(content)</span><br><span class="line">            # 说明内核缓冲区空了，等待下个事件循环再继续读</span><br><span class="line">            if len(content) &lt; 1024:</span><br><span class="line">                break</span><br><span class="line">        self.handle_rpc()</span><br><span class="line"></span><br><span class="line">    def handle_rpc(self):</span><br><span class="line">        while True:</span><br><span class="line">            self.rbuf.seek(0)</span><br><span class="line">            length_prefix &#x3D; self.rbuf.read(4)</span><br><span class="line">            if len(length_prefix) &lt; 4:</span><br><span class="line">                break</span><br><span class="line">            length, &#x3D; struct.unpack(&quot;I&quot;, length_prefix)</span><br><span class="line">            body &#x3D; self.rbuf.read(length)</span><br><span class="line">            if len(body) &lt; length:</span><br><span class="line">                break</span><br><span class="line">            request &#x3D; json.loads(body.decode())</span><br><span class="line">            in_ &#x3D; request[&#39;in&#39;]</span><br><span class="line">            params &#x3D; request[&#39;params&#39;]</span><br><span class="line">            print(os.getpid(), in_, params)</span><br><span class="line"></span><br><span class="line">            handler &#x3D; self.handlers[in_]</span><br><span class="line">            handler(params)</span><br><span class="line">            # 截断读缓冲</span><br><span class="line">            left &#x3D; self.rbuf.getvalue()[length + 4:]</span><br><span class="line">            self.rbuf &#x3D; BytesIO()</span><br><span class="line">            self.rbuf.write(left)</span><br><span class="line">        self.rbuf.seek(0, 2)  # 移动游标到缓冲区末尾，便于后续内容直接追加</span><br><span class="line"></span><br><span class="line">    def ping(self, params):</span><br><span class="line">        self.send_result(&quot;pong&quot;, params)</span><br><span class="line"></span><br><span class="line">    def send_result(self, out, result):</span><br><span class="line">        response &#x3D; &#123;&quot;out&quot;: out, &quot;result&quot;: result&#125;</span><br><span class="line">        body &#x3D; json.dumps(response)</span><br><span class="line">        length_prefix &#x3D; struct.pack(&quot;I&quot;, len(body))</span><br><span class="line">        self.send(length_prefix)</span><br><span class="line">        self.send(body.encode())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    RPCServer(&quot;localhost&quot;, 8080)</span><br><span class="line">    # 进入事件循环</span><br><span class="line">    asyncore.loop()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> RPC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> RPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四种 python 实现的单例模式的方式</title>
      <link href="/2018-05-22-python-singleton.html"/>
      <url>/2018-05-22-python-singleton.html</url>
      
        <content type="html"><![CDATA[<p>单例模式是一个非常常见的设计模式，</p><h4 id="使用-new-方法"><a href="#使用-new-方法" class="headerlink" title="使用 __new__ 方法"></a>使用 <code>__new__</code> 方法</h4><p><strong>new</strong>方法必须要返回一个实例化的对象</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Singleton(object):</span><br><span class="line">    </span><br><span class="line">    def __new__(cls, *args, **kwargs):</span><br><span class="line">        if not hasattr(cls, &#39;_instance&#39;):</span><br><span class="line">            cls._instance &#x3D; super().__new__(cls, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">        return cls._instance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Test(Singleton):</span><br><span class="line">    a &#x3D; 1</span><br></pre></td></tr></table></figure><h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [6]: a, b &#x3D; Test(), Test()</span><br><span class="line"></span><br><span class="line">In [7]: print(id(a), id(b))</span><br><span class="line">139990582602440 139990582602440</span><br></pre></td></tr></table></figure><h4 id="使用共享属性"><a href="#使用共享属性" class="headerlink" title="使用共享属性"></a>使用共享属性</h4><p>所谓单例就是所有引用(实例、对象)拥有相同的状态(属性)和行为(方法)， 同一个类的所有实例天然拥有相同的行为(方法),只需要保证同一个类的所有实例具有相同的状态(属性)即可,所有实例共享属性的最简单最直接的方法就是使 <code>__dict__</code>属性指向(引用)同一个字典(dict)</p><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Singleton(object):</span><br><span class="line">    _state &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">    def __new__(cls, *args, **kwargs):</span><br><span class="line">        ob &#x3D; super().__new__(cls, *args, **kwargs)</span><br><span class="line">        ob.__dict__ &#x3D; cls._state</span><br><span class="line">        return ob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Test(Singleton):</span><br><span class="line">    a &#x3D; 1</span><br></pre></td></tr></table></figure><h5 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [9]: a, b &#x3D; Test(), Test()</span><br><span class="line"></span><br><span class="line">In [10]: print(a.__dict__)</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">In [11]: print(id(a.__dict__), id(b.__dict__))</span><br><span class="line">139872953600760 139872953600760</span><br></pre></td></tr></table></figure><h4 id="使用元类"><a href="#使用元类" class="headerlink" title="使用元类"></a>使用元类</h4><h5 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Singleton(type):</span><br><span class="line">    def __init__(cls, name, bases, dict):</span><br><span class="line">        super(Singleton, cls).__init__(name, bases, dict)</span><br><span class="line">        cls._instance &#x3D; None</span><br><span class="line"></span><br><span class="line">    def __call__(cls, *args, **kw):</span><br><span class="line">        if cls._instance is None:</span><br><span class="line">            cls._instance &#x3D; super(Singleton, cls).__call__(*args, **kw)   # 这句你怎么理解?</span><br><span class="line">        return cls._instance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Test(metaclass&#x3D;Singleton):</span><br><span class="line">    a &#x3D; 1</span><br></pre></td></tr></table></figure><h5 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [7]: a, b &#x3D; Test(), Test()</span><br><span class="line"></span><br><span class="line">In [8]: print(id(a), id(b))</span><br><span class="line">139856176187376 139856176187376</span><br></pre></td></tr></table></figure><h4 id="使用装饰器"><a href="#使用装饰器" class="headerlink" title="使用装饰器"></a>使用装饰器</h4><h5 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def singleton(cls, *args, **kw):</span><br><span class="line">    </span><br><span class="line">    instances &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">    def _singleton():</span><br><span class="line">        if cls not in instances:</span><br><span class="line">            instances[cls] &#x3D; cls(*args, **kw)</span><br><span class="line">        return instances[cls]</span><br><span class="line"></span><br><span class="line">    return _singleton</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@singleton</span><br><span class="line">class Test(object):</span><br><span class="line">    a &#x3D; 1</span><br></pre></td></tr></table></figure><h5 id="测试-3"><a href="#测试-3" class="headerlink" title="测试"></a>测试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [2]: a, b &#x3D; Test(), Test()</span><br><span class="line"></span><br><span class="line">In [3]: print(id(a), id(b))</span><br><span class="line">140717732304656 140717732304656</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>温情脉脉 —《无条件》</title>
      <link href="/2018-05-17-music-eason-unconditional.html"/>
      <url>/2018-05-17-music-eason-unconditional.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://music.163.com/#/video?id=CCBCB08B4E6293F7749E5B9B55AB7215" target="_blank" rel="noopener">《无条件》 Live 版</a></p><a id="more"></a><h4 id="歌词"><a href="#歌词" class="headerlink" title="歌词"></a>歌词</h4><p><code>你 何以始终不说话 尽管讲出不快吧</code><br><code>事与冀盼有落差 请不必惊怕</code><br><code>我 仍然会冷静聆听</code><br><code>仍然紧守于身边 与你进退也共鸣</code><br><code>时日会 蔓延再蔓延 某些不可改变的改变</code><br><code>与一些不要发现的发现 就这么放大了缺点</code><br><code>来让我 问谁可决定 那些东西叫作完美至善</code><br><code>我只懂得 爱你在每天</code><br><code>当潮流爱新鲜 当旁人爱标签</code><br><code>幸得伴着你我 是窝心的自然</code><br><code>当闲言再尖酸 给他妒忌多点</code><br><code>因世上的至爱 是不计较条件 谁又可清楚看见</code><br><code>美 难免总有些缺憾 如果不甘心去问</code><br><code>问到最尾叫内心 也长出裂痕</code><br><code>笑 何妨与你又重温</code><br><code>仍然我说我庆幸 你永远胜过别人</code><br><code>期待美 没完爱没完 放开不必打算的打算</code><br><code>作一些可以约定的约定 就抱紧以后每一天</code><br><code>其实你定然都发现 我有很多未达完美事情</code><br><code>我只懂得 再努力每天</code><br><code>当潮流爱新鲜 当旁人爱标签</code><br><code>幸得伴着你我 是窝心的自然</code><br><code>当闲言再尖酸 给他妒忌多点</code><br><code>因世上的至爱 是不计较条件 谁又可清楚看见</code></p><h4 id="何谓”无条件的爱”"><a href="#何谓”无条件的爱”" class="headerlink" title="何谓”无条件的爱”"></a>何谓”无条件的爱”</h4><p>　　竟最近才偶然听到这首歌，便马上进入单曲循环。歌声好像将一段心底的往事娓娓道来，平平淡淡中蕴藏着款款深情，给人一种温暖坚定的感觉，让人忍不住想探究歌曲背后的故事。相传 Eason 创作这首歌的灵感来自于他的一只年纪很大的宠物，每天翘首盼着 Eason 回家，这让 Eason 感到了与宠物之间有一条特殊的纽带连着彼此，互为彼此的唯一。</p><p>　　这首歌也带给我一个问题——什么是”无条件的爱”。“无条件”的字面意思有点极端，极端到几乎不可能找到完全符合的例子，如果按照“无条件”本身的含义理解未免有点太不切实际，让人望而生畏。</p><p>　　细细想来，“爱”的产生，其实是一定有条件的。比如，你在花园中看到一株亭亭玉立的牡丹花，心生欢喜，风雨无阻每天都去观赏；比如，你在狗市上看到一只眼睛水汪汪的小二哈，便带回家吃同案、睡同榻。从这两个场景中不难发现，爱的产生的有条件的，你不会无缘无故每天去观赏一支花，也不会无缘无故和动物一起吃饭睡觉。那么“无条件的爱”是什么样的呢，我认为是这样的。当花凋落枯萎，甚至化作尘土后，你依然记着它不论是盛开还是凋零的样子，即使看到一支更艳丽的花，也很明确的知道自己最爱的是哪一支。当你的小二哈渐渐长大（长残～），没有了幼时的机灵可爱，甚至变得“老态龙钟”，你依然对它不离不弃，多年后依然记得它自己最爱的宠物。</p><p>　　结合上述事例，可以试着给“无条件的爱”下一个定义——<code>在一定条件下产生了亲密的关系，但关系的延续不需要任何条件(特别是引起关系产生的条件)</code>。爱一支花，不论它盛开还是衰败；爱一只犬，不论它可爱还是年迈。他爱慕她(他)风华绝代，她喜欢他英俊潇洒，但彼此的关系不会因时空世事的变迁而有丝毫减改变，反而历久弥新，愈陈愈香。</p><p>　　“无条件的爱”就好像是世外桃源，可遇不可求，愈求愈远，强行追寻恐怕只会得到和寻找桃花源一样的结果。对于吾等普罗大众，更重要的是清醒的认识到，现实中大部分的关系是有条件的，并认识到这并不是什么灾难，”无条件“和”有条件“的关系其实都是很好的，”无条件“自不必说，”有条件“的其实更容易做到明明白白，潇潇洒洒。<code>真正的灾难是把”有条件“的当”无条件“的，把”无条件“的当”有条件“的。</code>  </p><p><img src="/images/interesting/music-eason-unconditional-1.jpeg" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> Interesting </category>
          
          <category> Music </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 音乐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPC 消息协议设计注意点</title>
      <link href="/2018-05-16-rpc-protocol-basic.html"/>
      <url>/2018-05-16-rpc-protocol-basic.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>一套 RPC 消息协议包括几个基本内容:消息的边界,消息的编码,消息的结构以及压缩算法</p></blockquote><a id="more"></a><h3 id="消息的边界"><a href="#消息的边界" class="headerlink" title="消息的边界"></a>消息的边界</h3><p>RPC 通信中传递的一串由N条消息组成的消息流,如果一条消息的长度过大,会被网络协议栈拆分为多个数据包,如果一条消息的长度过小,网络协议栈会将多条消息合并为一个数据包再发送,所以必须明确每条消息的边界(分隔符), 接收方才能正确的从消息流中解析出一条条的消息</p><h4 id="特殊分隔符法"><a href="#特殊分隔符法" class="headerlink" title="特殊分隔符法"></a>特殊分隔符法</h4><table><thead><tr><th>message body</th><th>\r\n</th><th>message body</th><th>\r\n</th></tr></thead><tbody><tr><td>特殊分隔符法,很好理解,就是在每条消息最后加一个或几个特殊的字符作为分隔符,比如<code>/r/n</code>,这种方式有两个要求,第一个就是消息中间不能包含分隔符,第二个就是要求消息的内容尽量是文本消息,它的优点是可读性强,缺点是传输效率不高.</td><td></td><td></td><td></td></tr></tbody></table><h4 id="长度前缀法"><a href="#长度前缀法" class="headerlink" title="长度前缀法"></a>长度前缀法</h4><table><thead><tr><th>length</th><th>message body</th><th>length</th><th>message body</th></tr></thead></table><p>发送方在每条消息前面加一个固定字节长度的整数值,用于表示消息体的长度,后面紧跟消息体.接收方在读取消息的时候,首先读取到固定字节长度的整数值,然后在读取相应长度的字节数组,就可以完成的读取出一个消息,这种方式很适合用于二进制消息,它的优点是传输效率高,但可读性差.</p><h4 id="两种方法的混合"><a href="#两种方法的混合" class="headerlink" title="两种方法的混合"></a>两种方法的混合</h4><p>特殊分隔符法和长度前缀法,各有优劣,所以也可以把两者结合使用,扬长避短.比如 HTTP 协议就是这种情况,它的消息头用<code>\r\n</code>分割符,消息体用通过消息头中的<code>Content-Length</code>确定长度.接收方根据分隔符法读取消息头,再根据<code>Content-Length</code>指定的长度读取消息体,消息体可以是文本也可以是二进制数据.</p><h3 id="消息的编码"><a href="#消息的编码" class="headerlink" title="消息的编码"></a>消息的编码</h3><p>消息的编码有两种选择,二进制或文本.<br>二进制可读性差,但效率高,文本可读性好,效率相对差一点</p><h3 id="消息的结构"><a href="#消息的结构" class="headerlink" title="消息的结构"></a>消息的结构</h3><p>协议需要自己拟定消息的结构,客户端和服务端根据这样的结构规则对消息进行序列化和反序列化.  </p><p>比如在rest架构中用的最多的 json,就是一种常用的消息结构,它的优点是可读性非常好,确定是比较啰嗦,json中大量使用了引号,冒号大括号,且每次传递都要发送一遍<code>key</code>,这样的冗余就会造成网络传输中流量的浪费,在一些性能要求高的服务中,这样做是不合适的.  </p><p>在同一条消息通道上,发送方和接收方可以规定好消息格式,传输的时候只传输 <code>value</code>就可以了,接收方可以根据固定的代码将 <code>value</code> 和 <code>key</code> 一一对应. 这样就可以在传输中只传递<code>值</code>,不传递<code>结构</code>, 可以很好的节省流量.</p><h3 id="消息压缩"><a href="#消息压缩" class="headerlink" title="消息压缩"></a>消息压缩</h3><p>为了减轻网络带宽的压力,对消息进行压缩是行之有效的方法,但是这样会加重 CPU 的负担,所以在决定是否使用压缩算法时要进行权衡.最常见的http协议中,就会在消息头中使用 <code>content-encoding</code> 指定压缩算法.</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> RPC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> RPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 与 spark sql 结合实时统计点击量top3商品</title>
      <link href="/2018-05-02-spark-streaming-sql-top3.html"/>
      <url>/2018-05-02-spark-streaming-sql-top3.html</url>
      
        <content type="html"><![CDATA[<p>Spark Streaming 结合 Spark Sql,每隔10秒，统计最近60秒的，每个种类的每个商品的点击次数，然后统计出每个种类top3热门的商品</p><a id="more"></a><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>Spark Streaming最强大的地方在于，可以与SparkCore、SparkSQL整合使用，通过transform、foreachRDD等算子可以将DStream中的RDD使用Spark Core执行批处理操作, RDD 也可以转换为临时表,这样就可以使 Spark Streaming 和 Spark SQL结合使用。</p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>每隔10秒，统计最近60秒的，每个种类的每个商品的点击次数，然后统计出每个种类top3热门的商品</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F; python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark.sql.types import Row</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">案例: top3 热门商品实时统计</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;steaming_sql_top3&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 5)</span><br><span class="line">ssc.checkpoint(&quot;hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;checkpoint&#x2F;steaming_sql_top3&#x2F;&quot;)</span><br><span class="line"></span><br><span class="line"># 接收点击数据 &quot;username product category&quot;</span><br><span class="line">product_click_logs_dstream &#x3D; ssc.socketTextStream(&quot;127.0.0.1&quot;, 9999)</span><br><span class="line"></span><br><span class="line"># 映射为 (category_product, 1), 方便进行window操作统计每种类别的每个商品的点击次数</span><br><span class="line">category_pro_dstream &#x3D; product_click_logs_dstream.map(lambda row: (row.split(&quot; &quot;)[2] + &quot;_&quot; + row.split(&quot; &quot;)[1], 1))</span><br><span class="line"></span><br><span class="line"># 执行 window 操作, 每隔 60 秒统计每个种类每个商品的点击次数</span><br><span class="line">category_pro_counts_dstream &#x3D; category_pro_dstream.reduceByKeyAndWindow(lambda a, b: a+b, lambda a, b: a-b, 60, 10)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def top3(rdd):</span><br><span class="line">    # 使用反射推断Schema</span><br><span class="line">    rdd &#x3D; rdd.map(lambda row: Row(category&#x3D;row[0].split(&quot;_&quot;)[0], product&#x3D;row[0].split(&quot;_&quot;)[1], click_count&#x3D;row[1]))</span><br><span class="line">    spark &#x3D; SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .appName(&quot;steaming_sql_top3&quot;) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    # rdd 转换为 dataFrame</span><br><span class="line">    category_pro_counts_schema &#x3D; spark.createDataFrame(rdd)</span><br><span class="line">    # 注册为临时表</span><br><span class="line">    category_pro_counts_schema.createOrReplaceTempView(&quot;product_click_log&quot;)</span><br><span class="line">    # top3</span><br><span class="line">    top3_product_df &#x3D; spark.sql(</span><br><span class="line">        &quot;SELECT * FROM (&quot;</span><br><span class="line">            &quot;SELECT *, row_number() OVER (PARTITION BY category ORDER BY click_count DESC) rank &quot;</span><br><span class="line">                &quot;FROM product_click_log ) tmp &quot;</span><br><span class="line">        &quot;WHERE rank &lt;&#x3D; 3&quot;)</span><br><span class="line">    top3_product_df.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">category_pro_counts_dstream.foreachRDD(lambda rdd: top3(rdd))</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">测试数据:  nc -lk 9999</span><br><span class="line">zj iphone1 phone </span><br><span class="line">zj iphone2 phone </span><br><span class="line">zj iphone2 phone</span><br><span class="line">zj iphone3 phone</span><br><span class="line">zj iphone3 phone</span><br><span class="line">zj iphone3 phone</span><br><span class="line">zj iphone4 phone</span><br><span class="line">zj iphone4 phone</span><br><span class="line">zj iphone4 phone</span><br><span class="line">zj iphone4 phone</span><br><span class="line">zj car1 car</span><br><span class="line">zj car2 car</span><br><span class="line">zj car2 car</span><br><span class="line">zj car3 car</span><br><span class="line">zj car3 car</span><br><span class="line">zj car3 car</span><br><span class="line">zj car4 car</span><br><span class="line">zj car4 car</span><br><span class="line">zj car4 car</span><br><span class="line">zj car4 car</span><br><span class="line">zj shoes1 shoes</span><br><span class="line">zj shoes2 shoes</span><br><span class="line">zj shoes2 shoes</span><br><span class="line">zj shoes3 shoes</span><br><span class="line">zj shoes3 shoes</span><br><span class="line">zj shoes3 shoes</span><br><span class="line">zj shoes4 shoes</span><br><span class="line">zj shoes4 shoes</span><br><span class="line">zj shoes4 shoes</span><br><span class="line">zj shoes4 shoes</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">返回结果:</span><br><span class="line">+--------+-----------+-------+----+                                             </span><br><span class="line">|category|click_count|product|rank|</span><br><span class="line">+--------+-----------+-------+----+</span><br><span class="line">|   shoes|          4| shoes4|   1|</span><br><span class="line">|   shoes|          3| shoes3|   2|</span><br><span class="line">|   shoes|          2| shoes2|   3|</span><br><span class="line">|   phone|          4|iphone4|   1|</span><br><span class="line">|   phone|          3|iphone3|   2|</span><br><span class="line">|   phone|          2|iphone2|   3|</span><br><span class="line">|     car|          4|   car4|   1|</span><br><span class="line">|     car|          3|   car3|   2|</span><br><span class="line">|     car|          2|   car2|   3|</span><br><span class="line">+--------+-----------+-------+----+</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 热点搜索词滑动统计(pyspark)</title>
      <link href="/2018-04-25-spark-streaming-window.html"/>
      <url>/2018-04-25-spark-streaming-window.html</url>
      
        <content type="html"><![CDATA[<p>使用滑动窗口操作,实现热点搜索词的实时统计</p><a id="more"></a><h4 id="Window-滑动窗口操作"><a href="#Window-滑动窗口操作" class="headerlink" title="Window 滑动窗口操作"></a>Window 滑动窗口操作</h4><p><img src="http://spark.apachecn.org/docs/cn/2.2.0/img/streaming-dstream-window.png" alt="image"></p><p>普通的 DStream 操作都是操作一个 batch, 而 Window 操作用于同时处理多个 batch.把合并在一起的多个 batch 称为一个 Window, Window 中的 batch 个数称为窗口长度(window length), 两次 Window 操作之间的间隔称为滑动间隔(sliding interval). 窗口长度和滑动间隔都要设置为 batch 间隔的整数倍.</p><p>比如一个Spark Streaming编写的 wordcount 程序 batch 间隔为 5 秒, 窗口长度为 30 秒, 滑动间隔为 10 秒. 就意味着每隔 10 秒, 处理一次之前 6 个 batch 的 wordcount 算子.</p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;window_hotword&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 5)</span><br><span class="line">ssc.checkpoint(&quot;hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;window_checkpoint&quot;)</span><br><span class="line"></span><br><span class="line"># 接收实时的搜索词数据, 格式 &quot;name word&quot;</span><br><span class="line">search_logs_dstream &#x3D; ssc.socketTextStream(&quot;127.0.0.1&quot;, 9999)</span><br><span class="line"># 将日志转换为只有一个搜索词</span><br><span class="line">search_words_dstream &#x3D; search_logs_dstream.map(lambda line: line.split(&quot; &quot;)[1])</span><br><span class="line"># 将搜索词映射为 (word, 1) 的格式</span><br><span class="line">search_word_pairs_dstream &#x3D; search_words_dstream.map(lambda word: (word, 1))</span><br><span class="line"></span><br><span class="line"># 对 search_word_pairs_dstream 进行窗口操作</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">python 的 reduceByKeyAndWindow 和 scala&#x2F;java 的略有不同,</span><br><span class="line">    scala&#x2F;java 只传一个进行reduce的函数</span><br><span class="line">    python 需要传两个, 第一个表示对新进入 window 的 batch 进行的reduce 操作, 第二个表示对离开 window 的 batch 进行的 reduce 操作</span><br><span class="line">    第二个参数可以为None, 这样就表示要对window中所有的 batch 执行 reduce 操作,这样相对来说会降低性能,特别是在滑动间隔比较长的时候</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">search_word_count_dstream &#x3D; search_word_pairs_dstream.reduceByKeyAndWindow(lambda a, b: a+b, lambda a, b: a-b, 30, 10)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def transform(rdd):</span><br><span class="line">    # 将 rdd 转换为 (count, word) 格式</span><br><span class="line">    count_word_rdd &#x3D; rdd.map(lambda row: (row[1], row[0]))</span><br><span class="line">    # 对 key 进行倒序排序</span><br><span class="line">    sorted_rdd &#x3D; count_word_rdd.sortByKey(False)</span><br><span class="line">    # 将 rdd 转换为 (word, count) 格式</span><br><span class="line">    word_count_rdd &#x3D; sorted_rdd.map(lambda row: (row[1], row[0]))</span><br><span class="line">    # 取前三</span><br><span class="line">    top3_word_count &#x3D; word_count_rdd.take(3)</span><br><span class="line">    rdd &#x3D; word_count_rdd.filter(lambda row: row in top3_word_count)</span><br><span class="line">    # for word_count in top3_word_count:</span><br><span class="line">    #     print(&quot;***********&quot; + str(word_count) + &quot;*************&quot;)</span><br><span class="line">    return rdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对 window 中的词频进行排序</span><br><span class="line">final_dstream &#x3D; search_word_count_dstream.transform(lambda rdd: transform(rdd))</span><br><span class="line"></span><br><span class="line">final_dstream.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparkStreaming 实时黑名单过滤(pyspark)</title>
      <link href="/2018-04-24-spark-streaming-transform.html"/>
      <url>/2018-04-24-spark-streaming-transform.html</url>
      
        <content type="html"><![CDATA[<p>使用 transform 实现黑名单实时过滤操作</p><a id="more"></a><h4 id="transform-操作"><a href="#transform-操作" class="headerlink" title="transform 操作"></a>transform 操作</h4><p>DStream 提供的 join 操作, 只能合并其他 DStream, 但是在一些场景下,需要 DStream 中 每个 batch RDD 和另外的 RDD 进行 join 操作, 比如在使用 spark Streaming 进行实时访问日志处理的时候,需要过滤掉已知的非法访问</p><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;transformBlacklist&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 5)</span><br><span class="line"></span><br><span class="line">blacklist &#x3D; [</span><br><span class="line">    (&quot;tom&quot;, True),</span><br><span class="line">    (&quot;jerry&quot;, True)</span><br><span class="line">]</span><br><span class="line"># 模拟一个黑名单 (姓名, 是否启用)</span><br><span class="line">blacklistRdd &#x3D; sc.parallelize(blacklist)</span><br><span class="line"># 接收实时访问日志, 这里简化为格式为 (date, username)</span><br><span class="line">logDStream &#x3D; ssc.socketTextStream(&quot;127.0.0.1&quot;, 9999)</span><br><span class="line"></span><br><span class="line"># 把 (data, username) 转为 (username, (date, username))</span><br><span class="line">userLogDStream &#x3D; logDStream.map(lambda row: (row.split(&quot; &quot;)[1], row))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def transform(rdd):</span><br><span class="line">    joinedRdd &#x3D; rdd.leftOuterJoin(blacklistRdd)</span><br><span class="line">    filteredRdd &#x3D; joinedRdd.filter(lambda row: row[1][1] is not True)</span><br><span class="line">    validLogRdd &#x3D; filteredRdd.map(lambda row: row[1])</span><br><span class="line">    return validLogRdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">validLogDStream &#x3D; userLogDStream.transform(lambda rdd: transform(rdd))</span><br><span class="line"></span><br><span class="line">validLogDStream.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive 基础及实践</title>
      <link href="/2018-04-23-hive-base.html"/>
      <url>/2018-04-23-hive-base.html</url>
      
        <content type="html"><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表,并提供类SQL查询功能,它可以将 HQL 转化成 MapReduce 程序在 yarn 上运行。</p><a id="more"></a><p><code>文中命令行默认为 hive&gt; 命令行，其他命令行会特别说明，比如 shell&gt;</code></p><h4 id="Hive-安装"><a href="#Hive-安装" class="headerlink" title="Hive 安装"></a><a href="http://waterandair.top/install-hive.html" target="_blank" rel="noopener">Hive 安装</a></h4><h4 id="DDL-数据定义"><a href="#DDL-数据定义" class="headerlink" title="DDL 数据定义"></a>DDL 数据定义</h4><h5 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h5><p>数据库在HDFS上的默认存储路径是 <code>/user/hive/warehouse/*.db</code></p><h6 id="标准写法"><a href="#标准写法" class="headerlink" title="标准写法"></a>标准写法</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists db_hive;</span><br></pre></td></tr></table></figure><h6 id="指定数据库在-HDFS-中的存储位置"><a href="#指定数据库在-HDFS-中的存储位置" class="headerlink" title="指定数据库在 HDFS 中的存储位置"></a>指定数据库在 HDFS 中的存储位置</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database db_hive2 location &#39;&#x2F;db_hive2.db&#39;;</span><br></pre></td></tr></table></figure><h5 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h5><p>数据库的名称和数据库所在的目录位置是不可修改的，<code>ALTER DATABAS</code> 只能为数据库的 <code>DBPROPERTIES</code> 设置键值对属性值，来描述这个数据库的属性信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter database db_hive set dbproperties(&#39;createtime&#39;&#x3D;&#39;20180630&#39;);</span><br></pre></td></tr></table></figure><h5 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br><span class="line">show databases like &#39;db_hive*&#39;;</span><br></pre></td></tr></table></figure><h5 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc database db_hive;</span><br><span class="line">desc database extended db_hive;</span><br></pre></td></tr></table></figure><h5 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use db_hive;</span><br></pre></td></tr></table></figure><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><h6 id="删除空数据"><a href="#删除空数据" class="headerlink" title="删除空数据"></a>删除空数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop database if exists db_hive2;</span><br></pre></td></tr></table></figure><h6 id="删除空数据库"><a href="#删除空数据库" class="headerlink" title="删除空数据库"></a>删除空数据库</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop database if exists db_hive2 cascade;</span><br></pre></td></tr></table></figure><h5 id="创建表语法"><a href="#创建表语法" class="headerlink" title="创建表语法"></a>创建表语法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><p><strong>部分关键字解释：</strong></p><ul><li>EXTERNAL： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">表示创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）。</span><br><span class="line">Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。</span><br><span class="line">在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</span><br></pre></td></tr></table></figure></li><li>COMMENT： 为表和列添加注释</li><li>PARTITIONED： BY创建分区表</li><li>CLUSTERED BY： 创建分桶</li><li>ROW FORMAT： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] </span><br><span class="line">          [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </span><br><span class="line">          | SERDE serde_name [WITH SERDEPROPERTIES (property_name&#x3D;property_value, property_name&#x3D;property_value, ...)]</span><br><span class="line">    </span><br><span class="line">用户在建表的时候可以自定义 SERDE 或者使用自带的 SERDE。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SERDE。</span><br><span class="line">在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</span><br></pre></td></tr></table></figure></li><li>STORED AS： 指定存储文件类型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</span><br><span class="line">如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</span><br></pre></td></tr></table></figure></li><li>LOCATION：指定表在HDFS上的存储位置</li><li>LIKE允许用户复制现有的表结构，但是不复制数据。</li></ul><h5 id="管理表-内部表"><a href="#管理表-内部表" class="headerlink" title="管理表(内部表)"></a>管理表(内部表)</h5><p>默认创建的表都是管理表(内部表)，Hive 会控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项<code>hive.metastore.warehouse.dir</code>(例如，/user/hive/warehouse)所定义的目录的子目录下。当删除一个管理表时，Hive也会删除这个表中数据，管理表不适合和其他工具共享数据。</p><h6 id="普通创建表"><a href="#普通创建表" class="headerlink" title="普通创建表"></a>普通创建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student(</span><br><span class="line">    id int, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as textfile</span><br><span class="line">location &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student&#39;;</span><br></pre></td></tr></table></figure><h6 id="根据查询结果创建表"><a href="#根据查询结果创建表" class="headerlink" title="根据查询结果创建表"></a>根据查询结果创建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student2 as select id, name from student;</span><br></pre></td></tr></table></figure><h6 id="根据已经存在的表结构创建表"><a href="#根据已经存在的表结构创建表" class="headerlink" title="根据已经存在的表结构创建表"></a>根据已经存在的表结构创建表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student3 like student;</span><br></pre></td></tr></table></figure><h6 id="查询表的类型"><a href="#查询表的类型" class="headerlink" title="查询表的类型"></a>查询表的类型</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted student;</span><br><span class="line"># 输出很多信息，找到 Table Type: MANAGED_TABLE， 表示该表是一个管理表</span><br></pre></td></tr></table></figure><h5 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h5><p>Hive 表不拥有外部表的完整数据，删除外部表不会删除掉这份数据，不过描述表的元数据信息会被删除掉。建议将 HDFS 读入的数据存入外部表，在外部表的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储。</p><h6 id="创建数据库-1"><a href="#创建数据库-1" class="headerlink" title="创建数据库"></a>创建数据库</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists nba;</span><br></pre></td></tr></table></figure><h6 id="创建球队表"><a href="#创建球队表" class="headerlink" title="创建球队表"></a>创建球队表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists nba.team(</span><br><span class="line">    id int,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><h6 id="球员表"><a href="#球员表" class="headerlink" title="球员表"></a>球员表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists nba.player(</span><br><span class="line">    id int,</span><br><span class="line">    name string,</span><br><span class="line">    position string,</span><br><span class="line">    team_id int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><h6 id="查看创建的表"><a href="#查看创建的表" class="headerlink" title="查看创建的表"></a>查看创建的表</h6><p><code>show tables;</code></p><h6 id="向外部表导入数据"><a href="#向外部表导入数据" class="headerlink" title="向外部表导入数据"></a>向外部表导入数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;team.txt&#39; into table nba.team;</span><br><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;player.txt&#39; into table nba.player;</span><br></pre></td></tr></table></figure><h6 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from nba.team;</span><br><span class="line">select * from nba.player;</span><br></pre></td></tr></table></figure><h6 id="查看表元数据"><a href="#查看表元数据" class="headerlink" title="查看表元数据"></a>查看表元数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted team;</span><br><span class="line"># 可以看到 Table Type:         EXTERNAL_TABLE</span><br></pre></td></tr></table></figure><p>注意，删除外部表的时候并不能删除HDFS中存在的文件，如果想彻底删除，必须删除该表指向的文件</p><h6 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h6><p><a href="http://waterandair.top/datas/hive/team.txt" target="_blank" rel="noopener">team.txt</a><br><a href="http://waterandair.top/datas/hive/player.txt" target="_blank" rel="noopener">player.txt</a></p><h5 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h5><p>分区表就是对应 HDFS 上的一个独立的文件夹，该文件夹下是该分区所有的数据文件。<br>Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h6 id="分区表的创建"><a href="#分区表的创建" class="headerlink" title="分区表的创建"></a>分区表的创建</h6><p>注意分区字段名不能和表的字段名重复</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table nba.team_partition(</span><br><span class="line">    id int, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">partitioned by (division string) </span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><h6 id="加载数据到分区表"><a href="#加载数据到分区表" class="headerlink" title="加载数据到分区表"></a>加载数据到分区表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_east.txt&#39; into table nba.team_partition partition(division&#x3D;&#39;east&#39;);</span><br><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_west.txt&#39; into table nba.team_partition partition(division&#x3D;&#39;west&#39;);</span><br></pre></td></tr></table></figure><h6 id="查询分区表中的数据"><a href="#查询分区表中的数据" class="headerlink" title="查询分区表中的数据"></a>查询分区表中的数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from nba.team_partition where division&#x3D;&#39;west&#39;;</span><br><span class="line">select * from nba.team_partition where division&#x3D;&#39;east&#39;;</span><br><span class="line">select * from nba.team_partition where division&#x3D;&#39;east&#39; union select * from nba.team_partition where division&#x3D;&#39;west&#39;;</span><br></pre></td></tr></table></figure><h6 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_partition add partition(division&#x3D;&#39;north&#39;) partition(division&#x3D;&quot;south&quot;);</span><br></pre></td></tr></table></figure><h6 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_partition drop partition(division&#x3D;&#39;north&#39;), partition(division&#x3D;&#39;south&#39;);</span><br></pre></td></tr></table></figure><h6 id="查看分区"><a href="#查看分区" class="headerlink" title="查看分区"></a>查看分区</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show partitions nba.team_partition;</span><br></pre></td></tr></table></figure><h6 id="查看分区表结构"><a href="#查看分区表结构" class="headerlink" title="查看分区表结构"></a>查看分区表结构</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted nba.team_partition;</span><br></pre></td></tr></table></figure><h6 id="创建二级分区表"><a href="#创建二级分区表" class="headerlink" title="创建二级分区表"></a>创建二级分区表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table nba.team_sub_partition(</span><br><span class="line">    id int, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">partitioned by (division string, sub_division string) </span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><h6 id="加载数据到二级分区表中"><a href="#加载数据到二级分区表中" class="headerlink" title="加载数据到二级分区表中"></a>加载数据到二级分区表中</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_west_pacific.txt&#39; into table  nba.team_sub_partition partition(division&#x3D;&#39;west&#39;, sub_division&#x3D;&#39;pacific&#39;);</span><br></pre></td></tr></table></figure><h6 id="查询二级分区数据"><a href="#查询二级分区数据" class="headerlink" title="查询二级分区数据"></a>查询二级分区数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from nba.team_sub_partition where division&#x3D;&#39;west&#39; and sub_division&#x3D;&#39;pacific&#39;;</span><br></pre></td></tr></table></figure><h6 id="附件-1"><a href="#附件-1" class="headerlink" title="附件"></a>附件</h6><p><a href="http://waterandair.top/datas/team_east.txt" target="_blank" rel="noopener">team_east.txt</a><br><a href="http://waterandair.top/datas/team_west.txt" target="_blank" rel="noopener">team_west.txt</a><br><a href="http://waterandair.top/datas/team_west_pacific.txt" target="_blank" rel="noopener">team_west_pacific.txt</a>  </p><h5 id="直接上传文件到分区目录，并使分区表和数据产生关联"><a href="#直接上传文件到分区目录，并使分区表和数据产生关联" class="headerlink" title="直接上传文件到分区目录，并使分区表和数据产生关联"></a>直接上传文件到分区目录，并使分区表和数据产生关联</h5><h6 id="上传数据后修复"><a href="#上传数据后修复" class="headerlink" title="上传数据后修复"></a>上传数据后修复</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建目录</span><br><span class="line">shell&gt; hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team_sub_partition&#x2F;division&#x3D;west&#x2F;sub_division&#x3D;northwest</span><br><span class="line"># 上传源文件</span><br><span class="line">shell&gt; hadoop fs -put &#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_west_northwest.txt &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team_sub_partition&#x2F;division&#x3D;west&#x2F;sub_division&#x3D;northwest</span><br><span class="line"># 查询数据(查询不到数据)</span><br><span class="line">select * from nba.team_sub_partition where division&#x3D;&#39;west&#39; and sub_division&#x3D;&#39;northwest&#39;;</span><br><span class="line"># 执行修复命令</span><br><span class="line">msck repair table nba.team_sub_partition;</span><br><span class="line"># 再次查询可以查到数据</span><br><span class="line">select * from nba.team_sub_partition where division&#x3D;&#39;west&#39; and sub_division&#x3D;&#39;northwest&#39;</span><br></pre></td></tr></table></figure><h6 id="上传数据后添加分区"><a href="#上传数据后添加分区" class="headerlink" title="上传数据后添加分区"></a>上传数据后添加分区</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 创建目录</span><br><span class="line">shell&gt; hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team_sub_partition&#x2F;division&#x3D;west&#x2F;sub_division&#x3D;southwest</span><br><span class="line"># 上传文件</span><br><span class="line">shell&gt; hadoop fs -put &#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_west_southwest.txt &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team_sub_partition&#x2F;division&#x3D;west&#x2F;sub_division&#x3D;southwest</span><br><span class="line"># 添加分区</span><br><span class="line">alter table nba.team_sub_partition add partition(division&#x3D;&#39;west&#39;,sub_division&#x3D;&#39;southwest&#39;);</span><br><span class="line"># 查询数据</span><br><span class="line">select * from nba.team_sub_partition where division&#x3D;&#39;west&#39; and sub_division&#x3D;&#39;southwest&#39;;</span><br></pre></td></tr></table></figure><h6 id="上传数据后-load-数据到分区"><a href="#上传数据后-load-数据到分区" class="headerlink" title="上传数据后 load 数据到分区"></a>上传数据后 load 数据到分区</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 创建目录</span><br><span class="line">shell&gt; hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team_sub_partition&#x2F;division&#x3D;east&#x2F;sub_division&#x3D;atlantic</span><br><span class="line"># load 上传文件</span><br><span class="line">load data local inpath &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_east_atlantic.txt&#39; into table nba.team_sub_partition partition(division&#x3D;&#39;east&#39;,sub_division&#x3D;&#39;atlantic&#39;);</span><br><span class="line"># 查询数据</span><br><span class="line">select * from nba.team_sub_partition where division&#x3D;&#39;east&#39; and sub_division&#x3D;&#39;atlantic&#39;;</span><br></pre></td></tr></table></figure><h6 id="附件-2"><a href="#附件-2" class="headerlink" title="附件"></a>附件</h6><p><a href="http://waterandair.top/datas/team_east_atlantic.txt" target="_blank" rel="noopener">team_east_atlantic.txt</a><br><a href="http://waterandair.top/datas/team_west_southwest.txt" target="_blank" rel="noopener">team_west_southwest.txt</a><br><a href="http://waterandair.top/datas/team_west_northwest.txt" target="_blank" rel="noopener">team_west_northwest.txt</a></p><h5 id="表相关的操作"><a href="#表相关的操作" class="headerlink" title="表相关的操作"></a>表相关的操作</h5><h6 id="表重命名"><a href="#表重命名" class="headerlink" title="表重命名"></a>表重命名</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_sub_partition rename to nba.team_partition2;</span><br></pre></td></tr></table></figure><h6 id="更新列的语法"><a href="#更新列的语法" class="headerlink" title="更新列的语法"></a>更新列的语法</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</span><br></pre></td></tr></table></figure><h6 id="增加和替换列的语法"><a href="#增加和替换列的语法" class="headerlink" title="增加和替换列的语法"></a>增加和替换列的语法</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</span><br><span class="line">ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br></pre></td></tr></table></figure><h6 id="查询表结构"><a href="#查询表结构" class="headerlink" title="查询表结构"></a>查询表结构</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc nba.team_partition;</span><br></pre></td></tr></table></figure><h6 id="添加列"><a href="#添加列" class="headerlink" title="添加列"></a>添加列</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_partition add columns(city string);</span><br></pre></td></tr></table></figure><h6 id="更新列"><a href="#更新列" class="headerlink" title="更新列"></a>更新列</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_partition change column city old_city string;</span><br></pre></td></tr></table></figure><h6 id="替换列"><a href="#替换列" class="headerlink" title="替换列"></a>替换列</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table nba.team_partition replace columns(team_id string, team_name);</span><br></pre></td></tr></table></figure><h6 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists time_partition2;</span><br></pre></td></tr></table></figure><h6 id="清除表中的数据"><a href="#清除表中的数据" class="headerlink" title="清除表中的数据"></a>清除表中的数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">truncate table team;</span><br><span class="line"># 只能删除管理表(内部表)， 不能删除外部表中的数据</span><br></pre></td></tr></table></figure><h4 id="DML-数据操作"><a href="#DML-数据操作" class="headerlink" title="DML 数据操作"></a>DML 数据操作</h4><h5 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h5><h6 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [local] inpath &#39;&#x2F;opt&#x2F;module&#x2F;datas&#x2F;student.txt&#39; [overwrite] into table student [partition (partcol1&#x3D;val1,…)];</span><br></pre></td></tr></table></figure><ul><li>load data: 表示加载数据</li><li>local: 表示从本地加载数据到 hive 表，否则从 HDFS 加载数据到 hive 表 </li><li>inpath: 表示加载数据的路径</li><li>into table: 表示加载到哪张表</li><li>student: 表示具体的表</li><li>overwrite: 表示覆盖表中已有数据，否则表示追加</li><li>partition: 表示上传到指定分区</li></ul><h6 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists team;</span><br><span class="line"># 创建一张分区表</span><br><span class="line">create table team(</span><br><span class="line">    id int,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">partitioned by (division string) </span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line"></span><br><span class="line"># 插入数据</span><br><span class="line">insert into table team partition(division&#x3D;&#39;west&#39;) values (&#39;1&#39;, &#39;Warriors&#39;);</span><br><span class="line"></span><br><span class="line"># 根据单表查询结果插入数据</span><br><span class="line">insert into table team partition(division&#x3D;&#39;east&#39;) select id , name from team where division&#x3D;&#39;west&#39;;</span><br></pre></td></tr></table></figure><h6 id="创建表时用-as-select-加载数据"><a href="#创建表时用-as-select-加载数据" class="headerlink" title="创建表时用 as select 加载数据"></a>创建表时用 as select 加载数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists team2 as select id, name from team;</span><br><span class="line"># 注意，这种方式不能复制分区</span><br></pre></td></tr></table></figure><h6 id="创建表并通过-location-指定加载数据路径"><a href="#创建表并通过-location-指定加载数据路径" class="headerlink" title="创建表并通过 location 指定加载数据路径"></a>创建表并通过 location 指定加载数据路径</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 创建表</span><br><span class="line">drop table if exists team;</span><br><span class="line">create table team(</span><br><span class="line">    id int, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">location &#39;&#x2F;datas&#x2F;team&#39;;</span><br><span class="line"></span><br><span class="line"># 上传数据到 HDFS</span><br><span class="line">shell&gt; hadoop fs -mkdir -p &#x2F;datas&#x2F;team</span><br><span class="line">shell&gt; hadoop fs -put &#x2F;home&#x2F;zj&#x2F;datas&#x2F;team.txt &#x2F;datas&#x2F;team</span><br><span class="line">shell&gt; hadoop fs -put &#x2F;home&#x2F;zj&#x2F;datas&#x2F;team_partition.txt &#x2F;datas&#x2F;team</span><br><span class="line"></span><br><span class="line"># 查询 </span><br><span class="line">select * from team;</span><br></pre></td></tr></table></figure><h6 id="import数据到指定hive表中-需要先-export-导出-…"><a href="#import数据到指定hive表中-需要先-export-导出-…" class="headerlink" title="import数据到指定hive表中(需要先 export 导出)…"></a>import数据到指定hive表中(需要先 export 导出)…</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 导出</span><br><span class="line">export table nba.team to &#39;&#x2F;datas&#x2F;export&#x2F;team&#39;;</span><br><span class="line"># 清空</span><br><span class="line">truncate table student;</span><br><span class="line"># 导入</span><br><span class="line">import table team partition(division&#x3D;&#39;west&#39;) from &#39;&#x2F;datas&#x2F;export&#x2F;team&#39;;</span><br></pre></td></tr></table></figure><h5 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h5><h6 id="Inster-导出"><a href="#Inster-导出" class="headerlink" title="Inster 导出"></a>Inster 导出</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 将查询的结果导出到本地</span><br><span class="line">insert overwrite local directory &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;export&#39;select * from team;</span><br><span class="line"></span><br><span class="line"># 将查询的结果格式化导出到本地</span><br><span class="line">insert overwrite local directory &#39;&#x2F;home&#x2F;zj&#x2F;datas&#x2F;export&#39;</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">collection items terminated by &#39;\n&#39;</span><br><span class="line">select * from team;</span><br><span class="line">             </span><br><span class="line"># 将查询的结果导出到HDFS上(没有local)</span><br><span class="line">insert overwrite directory &#39;&#x2F;datas&#x2F;export&#39;</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">collection items terminated by &#39;\n&#39;</span><br><span class="line">select * from student;</span><br></pre></td></tr></table></figure><p>注意: 导出的是块数据</p><h6 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hadoop fs -get &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;nba.db&#x2F;team2&#x2F;000000_0 &#x2F;home&#x2F;zj&#x2F;datas&#x2F;export&#x2F;team.txt</span><br></pre></td></tr></table></figure><h6 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hive -e &#39;select * from nba.team;&#39; &gt; &#x2F;home&#x2F;zj&#x2F;datas&#x2F;export&#x2F;nba.team.txt;</span><br></pre></td></tr></table></figure><h6 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table nba.team to &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;export&#x2F;team&#39;;</span><br></pre></td></tr></table></figure><h6 id="Sqoop-导出"><a href="#Sqoop-导出" class="headerlink" title="Sqoop 导出"></a>Sqoop 导出</h6><h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><h5 id="查询语法"><a href="#查询语法" class="headerlink" title="查询语法"></a>查询语法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">  | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line">   [LIMIT number]</span><br></pre></td></tr></table></figure><ul><li>SQL 语言大小写不敏感。 </li><li>SQL 可以写在一行或者多行</li><li>关键字不能被缩写也不能分行</li><li>各子句一般要分行写。</li><li>使用缩进提高语句的可读性。</li></ul><h5 id="基本查询"><a href="#基本查询" class="headerlink" title="基本查询"></a>基本查询</h5><h6 id="全表和特定字段查询"><a href="#全表和特定字段查询" class="headerlink" title="全表和特定字段查询"></a>全表和特定字段查询</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from team;</span><br><span class="line">select name from team;</span><br><span class="line">select name as team_name from team;</span><br></pre></td></tr></table></figure><h6 id="limit-语句"><a href="#limit-语句" class="headerlink" title="limit 语句"></a>limit 语句</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from team limit 2;</span><br></pre></td></tr></table></figure><h6 id="where-语句"><a href="#where-语句" class="headerlink" title="where 语句"></a>where 语句</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from team where id &lt;&#x3D; 3;</span><br></pre></td></tr></table></figure><h6 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select * from team where id&#x3D;4;</span><br><span class="line">select * from team where id between 2 and 4;</span><br><span class="line">select * from team where name is null;</span><br><span class="line">select * from team where id in (1, 3, 5);</span><br></pre></td></tr></table></figure><h6 id="逻辑运算符-AND-OR-NOT"><a href="#逻辑运算符-AND-OR-NOT" class="headerlink" title="逻辑运算符(AND/OR/NOT)"></a>逻辑运算符(AND/OR/NOT)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from team where id &gt; 1 and id &lt; 4;</span><br><span class="line">select * from team where id &gt; 4 or name&#x3D;&#39;Lakers&#39;;</span><br><span class="line">select * from team where id not in (1, 2, 3);</span><br></pre></td></tr></table></figure><h6 id="like-和-rlike"><a href="#like-和-rlike" class="headerlink" title="like 和 rlike"></a>like 和 rlike</h6><p><code>% 代表任意个字符， _ 代表一个字符</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 搜查找 name 以 &#39;l&#39; 开头的</span><br><span class="line">select * from team where name like &#39;L%&#39;;</span><br><span class="line"># 查找第二个字符为 &#39;a&#39; 的</span><br><span class="line">select * from team where name like &#39;_a%&#39;;</span><br><span class="line"># 查找含有 &#39;er&#39; 的</span><br><span class="line"># rlike 子句是Hive中这个功能的一个扩展，其可以通过J正则表达式指定匹配条件。</span><br><span class="line">select * from team where name rlike &#39;er&#39;;</span><br></pre></td></tr></table></figure><h5 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h5><h6 id="group-by-语句"><a href="#group-by-语句" class="headerlink" title="group by 语句"></a>group by 语句</h6><p><code>group by</code> 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select division, avg(id) avg_id from team_partition group by division;</span><br></pre></td></tr></table></figure><h6 id="having-语句"><a href="#having-语句" class="headerlink" title="having 语句"></a>having 语句</h6><ul><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li><li>where后面不能写分组函数，而having后面可以使用分组函数</li><li>having只用于group by分组统计语句。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select division, avg(id) avg_id from team_partition group by division having avg_id &gt; 4;</span><br></pre></td></tr></table></figure><h5 id="join-语句"><a href="#join-语句" class="headerlink" title="join 语句"></a>join 语句</h5><h6 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h6>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select player.name, team.name from player join team on player.team_id&#x3D;team.id;</span><br></pre></td></tr></table></figure><h6 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h6><p>JOIN操作符左边表中符合WHERE子句的所有记录将会被返回，不符合条件的位置用 NULL 替代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select p.name, t.name from player p left join team t on p.team_id&#x3D;t.id;</span><br></pre></td></tr></table></figure><h6 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h6><p>JOIN操作符右边表中符合WHERE子句的所有记录将会被返回，不符合条件的位置用 NULL 替代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select p.name, t.name from player p right join team t on p.team_id&#x3D;t.id;</span><br></pre></td></tr></table></figure><h6 id="Full-连接"><a href="#Full-连接" class="headerlink" title="Full 连接"></a>Full 连接</h6><p>将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select p.name, t.name from player p full join team t on p.team_id&#x3D;t.id;</span><br></pre></td></tr></table></figure><h6 id="笛卡尔积join"><a href="#笛卡尔积join" class="headerlink" title="笛卡尔积join"></a>笛卡尔积join</h6><p>省略连接条件或连接条件无效的情况下发生笛卡尔积</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select player.name, team.name from player,team;</span><br></pre></td></tr></table></figure><h5 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h5><h6 id="全局排序-order-by"><a href="#全局排序-order-by" class="headerlink" title="全局排序(order by)"></a>全局排序(order by)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 普通排序</span><br><span class="line">select * from team order by id desc;</span><br><span class="line"># 表达式别名排序</span><br><span class="line">select name, id*2 as ids from team order by ids desc;</span><br><span class="line"># 多列排序</span><br><span class="line">select * from team order by name,id;</span><br></pre></td></tr></table></figure><h6 id="每个MapReduce内部排序-sort-by"><a href="#每个MapReduce内部排序-sort-by" class="headerlink" title="每个MapReduce内部排序(sort by)"></a>每个MapReduce内部排序(sort by)</h6><p>每个MapReduce内部进行排序，对全局结果集来说不是排序。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 设置 reduce 个数</span><br><span class="line">set mapreduce.job.reduces&#x3D;3;</span><br><span class="line"># 查看 reduce 个数</span><br><span class="line">set mapreduce.job.reduces;</span><br><span class="line"># id 降序</span><br><span class="line">select * from team sort by id desc;</span><br></pre></td></tr></table></figure><h6 id="分区排序-distribute-by"><a href="#分区排序-distribute-by" class="headerlink" title="分区排序(distribute by)"></a>分区排序(distribute by)</h6><p>类似MR中partition，进行分区，结合sort by使用<br>注意: Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select * from player distribute by team_id sort by id desc;</span><br><span class="line"></span><br><span class="line">player.idplayer.nameplayer.positionplayer.team_id</span><br><span class="line">6James HardenG3</span><br><span class="line">5Chris PaulG3</span><br><span class="line">2LeBron JamesFG1</span><br><span class="line">1Kobe BryantSG1</span><br><span class="line">4Paul GeorgeF2</span><br><span class="line">3Russell WestbrookG2</span><br></pre></td></tr></table></figure><h6 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h6><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。<br>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from player cluster by team_id;</span><br><span class="line">等价于</span><br><span class="line">select * from player distribute by team_id sort by team_id;</span><br></pre></td></tr></table></figure><p>注意: 分区不一定是按照固定的数据，team_id为 1和2 的也可能分配到一个分区里去</p><h5 id="分桶及抽样调查"><a href="#分桶及抽样调查" class="headerlink" title="分桶及抽样调查"></a>分桶及抽样调查</h5><h6 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h6><p><code>分区针对的是数据的存储路径；分桶针对的是数据文件</code></p><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是无法确定合适的划分大小的时候。  </p><p>分桶是将数据集分解成更容易管理的若干部分的一种技术。  </p><p>桶表不能通过load的方式直接加载数据，只能从另一张表中插入数据。</p><h6 id="创建分桶表"><a href="#创建分桶表" class="headerlink" title="创建分桶表"></a>创建分桶表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table team_bucket(</span><br><span class="line">    id int,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">clustered by (id) into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><h6 id="查看表结构"><a href="#查看表结构" class="headerlink" title="查看表结构"></a>查看表结构</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted team_bucket;</span><br><span class="line"># Num Buckets:        4</span><br></pre></td></tr></table></figure><h6 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.enforce.bucketing&#x3D;true;</span><br><span class="line">set mapreduce.job.reduces&#x3D;-1;</span><br></pre></td></tr></table></figure><h6 id="从-team-表中通过子查询把数据导入到分桶表-team-bucket"><a href="#从-team-表中通过子查询把数据导入到分桶表-team-bucket" class="headerlink" title="从 team 表中通过子查询把数据导入到分桶表 team_bucket"></a>从 team 表中通过子查询把数据导入到分桶表 team_bucket</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">insert into table team_bucket select id, name from team cluster by (id);</span><br><span class="line">select * from team_bucket;</span><br><span class="line"># 分桶成功</span><br></pre></td></tr></table></figure><h6 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h6><p>分桶表非常适合解决抽样查询的需求</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select * from team_bucket tablesample(bucket 1 out of 4 on id);</span><br><span class="line"># tablesample(bucket x out of y)</span><br><span class="line"></span><br><span class="line"># y 必须是 bucket 的倍数或者因子。</span><br><span class="line"># hive 根据 y 的大小，决定抽样的比例。</span><br><span class="line"># 假设 table 分成了 4 份，当 y&#x3D;2 时，抽取 (4&#x2F;2&#x3D;2) 个 bucket 的数据，当 y&#x3D;8时，抽取 (4&#x2F;8&#x3D;1&#x2F;2) 个bucket的数据。</span><br><span class="line"></span><br><span class="line"># x 表示从哪个 bucket 开始抽取。</span><br><span class="line"># 假设 table 从 bucket 为 4，tablesample(bucket 4 out of 4) 表示抽取 (4&#x2F;4&#x3D;1) 个 bucket 的数据，抽取第 4 个 bucket 的数据</span><br><span class="line"></span><br><span class="line"># 注意: x的值必须小于等于y的值</span><br></pre></td></tr></table></figure><h6 id="数据块抽样"><a href="#数据块抽样" class="headerlink" title="数据块抽样"></a>数据块抽样</h6><p>Hive提供了另外一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from team_bucket tablesample(0.1 percent);</span><br></pre></td></tr></table></figure><p>注意: 这种抽样方式不一定适用于所有的文件格式。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><h5 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h5><h5 id="查看系统自带的函数"><a href="#查看系统自带的函数" class="headerlink" title="查看系统自带的函数"></a>查看系统自带的函数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br><span class="line"># 显示函数的用法</span><br></pre></td></tr></table></figure><h6 id="显示自带的函数的用法"><a href="#显示自带的函数的用法" class="headerlink" title="显示自带的函数的用法"></a>显示自带的函数的用法</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc function count;</span><br><span class="line">desc function extended count;</span><br></pre></td></tr></table></figure><h6 id="求总行数-count"><a href="#求总行数-count" class="headerlink" title="求总行数(count)"></a>求总行数(count)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(id) as num  from team;</span><br></pre></td></tr></table></figure><h6 id="求最大值-max"><a href="#求最大值-max" class="headerlink" title="求最大值(max)"></a>求最大值(max)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select max(id) as max_id from team;</span><br></pre></td></tr></table></figure><h6 id="求最小值-min"><a href="#求最小值-min" class="headerlink" title="求最小值(min)"></a>求最小值(min)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select min(id) as min_id from team;</span><br></pre></td></tr></table></figure><h6 id="求总和-sun"><a href="#求总和-sun" class="headerlink" title="求总和(sun)"></a>求总和(sun)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sum(id) as sum_id from team;</span><br></pre></td></tr></table></figure><h6 id="求均值-avg"><a href="#求均值-avg" class="headerlink" title="求均值(avg)"></a>求均值(avg)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select avg(id) as avg_id from team;</span><br></pre></td></tr></table></figure><h5 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h5><p>hive 自定义函数可以分为三种类别:</p><ul><li>UDF(User-Defined-Function): 一进一出</li><li>UDAF(uuer-Defined Aggregation Function): 聚合函数，多进一出</li><li>UDTF(User-Defined Table-Generating Functions): 一进多出</li></ul><p>因为 hive 是java程序，所以自定义函数最好是用java语言编写。</p><h5 id="UDF-Python示例"><a href="#UDF-Python示例" class="headerlink" title="UDF(Python示例)"></a>UDF(Python示例)</h5><h6 id="编写python"><a href="#编写python" class="headerlink" title="编写python"></a>编写python</h6><p>python 编写 hive UDF 是逐行处理标准输入流</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">实现 upper 功能</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    line &#x3D; line.upper()</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><h6 id="添加-python-文件到集群"><a href="#添加-python-文件到集群" class="headerlink" title="添加 python 文件到集群"></a>添加 python 文件到集群</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add file &#x2F;home&#x2F;zj&#x2F;hive&#x2F;udf&#x2F;upper.py</span><br></pre></td></tr></table></figure><h6 id="利用-UDF-查询"><a href="#利用-UDF-查询" class="headerlink" title="利用 UDF 查询"></a>利用 UDF 查询</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 语法</span><br><span class="line">SELECT TRANSFORM (&lt;columns&gt;) USING &#39;python &lt;python_script&gt;&#39; AS (&lt;columns&gt;) FROM &lt;table&gt;;</span><br><span class="line"># 示例</span><br><span class="line">select transform(name) using &#39;python3 upper.py&#39; as upper_name from team;</span><br><span class="line"># 注意: 使用transform的时候不能查询别的列</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UpdateStateByKey 实时统计全局wordcount(pyspark)</title>
      <link href="/2018-04-23-spark-updatestatebykey.html"/>
      <url>/2018-04-23-spark-updatestatebykey.html</url>
      
        <content type="html"><![CDATA[<p>使用 updateStateByKey 操作实现基于缓存的实时wordcount程序</p><a id="more"></a><h4 id="updateStateByKey-介绍"><a href="#updateStateByKey-介绍" class="headerlink" title="updateStateByKey 介绍"></a>updateStateByKey 介绍</h4><p>updateStateByKey操作，可以为每个key维护一份state，并持续不断的更新该state。</p><ol><li>首先，要定义一个state，可以是任意的数据类型；</li><li>其次，要定义state更新函数——指定一个函数如何使用之前的state和新值来更新state。</li></ol><p>对于每个batch，Spark都会为每个之前已经存在的key去应用一次state更新函数，无论这个key在batch中是否有新的数据。如果state更新函数返回none，那么key对应的state就会被删除。当然，对于每个新出现的key，也会执行state更新函数。</p><p><strong>注意，updateStateByKey操作，要求必须开启Checkpoint机制。</strong></p><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;streaming_socket&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 10)</span><br><span class="line"># 必须开启 checkpoint 机制,这样才能把每个 key 对应的 state 存到内存和磁盘中, 以便于在内存数据丢失的时候, 可以从 checkpoint 中恢复</span><br><span class="line">ssc.checkpoint(&quot;hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;wordcount_checkpoint&quot;)</span><br><span class="line"></span><br><span class="line">lines &#x3D; ssc.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class="line">words &#x3D; lines\</span><br><span class="line">    .flatMap(lambda l: l.split())\</span><br><span class="line">    .map(lambda w: (w, 1))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 第一个参数表示当前 batch 中这个 key 的值(可能有多个)</span><br><span class="line"># 第二个参数表示 key 之前的 state</span><br><span class="line">def word_count(values, state):</span><br><span class="line">    if not state:</span><br><span class="line">        state &#x3D; 0</span><br><span class="line">    for value in values:</span><br><span class="line">        state +&#x3D; value</span><br><span class="line">    return state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wordcount &#x3D; words.updateStateByKey(word_count)</span><br><span class="line">wordcount.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>各区域热门商品统计(pyspark)</title>
      <link href="/2018-04-22-spark-app-area-top3-product.html"/>
      <url>/2018-04-22-spark-app-area-top3-product.html</url>
      
        <content type="html"><![CDATA[<p>本文可看做是 <a href="http://waterandair.top/spark-app-session-analysis.html" target="_blank" rel="noopener">用户行为分析(pyspark)</a> 的后续篇, 主要介绍使用spark统计各区域热门商品,完整代码可以点击参考这里,代码中有详细的注释,所以可以也可以略过文章,直接看代码<a href="https://github.com/waterandair/daily-learning/blob/master/learn-spark/python/emulate_project/area_top_3_product_app.py" target="_blank" rel="noopener">项目源码地址</a></p><a id="more"></a>  <h3 id="模拟数据"><a href="#模拟数据" class="headerlink" title="模拟数据"></a>模拟数据</h3><p>模拟数据与 <a href="http://waterandair.top/spark-app-session-analysis.html" target="_blank" rel="noopener">用户行为分析(pyspark)</a> 中的模拟数据一样</p><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>根据用户指定的日期范围,统计各个区域下最热门的3个产品</p><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p>产品的热度根据产品的点击量评价</p><ol><li>根据筛选条件,查询用户行为记录表中的区域id和点击的产品id</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sql_str &#x3D; &quot;SELECT city_id, click_product_id as product_id  </span><br><span class="line">           FROM user_visit_action </span><br><span class="line">           WHERE click_product_id !&#x3D; &#39;&#39; AND  date &gt;&#x3D;&#39;&quot; + start_date + &quot;&#39; AND date&lt;&#x3D;&#39;&quot; + end_date + &quot;&#39;&quot;</span><br></pre></td></tr></table></figure><ol start="2"><li>获取城市信息</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 实际应用中,这一步可能是从关系型数据库中获取城市信息,这里为了方便测试,模拟一份数据</span><br><span class="line">def get_caty_id_to_caty_info_rdd(sc):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取城市详细信息</span><br><span class="line">    :param sc:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 城市详细信息(id,(id,name,area))</span><br><span class="line">    caty_info &#x3D; [</span><br><span class="line">        (0, (0, &#39;北京&#39;, &#39;华北&#39;)),</span><br><span class="line">        (1, (1, &#39;上海&#39;, &#39;华东&#39;)),</span><br><span class="line">        (2, (2, &#39;南京&#39;, &#39;华东&#39;)),</span><br><span class="line">        (3, (3, &#39;广州&#39;, &#39;华南&#39;)),</span><br><span class="line">        (4, (4, &#39;三亚&#39;, &#39;华南&#39;)),</span><br><span class="line">        (5, (5, &#39;武汉&#39;, &#39;华中&#39;)),</span><br><span class="line">        (6, (6, &#39;长沙&#39;, &#39;华中&#39;)),</span><br><span class="line">        (7, (7, &#39;西安&#39;, &#39;西北&#39;)),</span><br><span class="line">        (8, (8, &#39;成都&#39;, &#39;西南&#39;)),</span><br><span class="line">        (9, (9, &#39;哈尔滨&#39;, &#39;东北&#39;))</span><br><span class="line"></span><br><span class="line">    ]</span><br><span class="line">    return sc.parallelize(caty_info)</span><br></pre></td></tr></table></figure><ol start="3"><li>创建包含 city_id, city_name, area, product_id 的临时表 <code>tmp_clk_prod_basie</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def get_temp_click_product_table(spark, click_action_rdd, city_id_to_city_info_rdd):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    把 rdd 合并得到每次action的city_id和product_id, 并转化为临时表</span><br><span class="line">    :param spark:</span><br><span class="line">    :param click_action_rdd:</span><br><span class="line">    :param city_id_to_city_info_rdd:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    temp_click_product_rdd &#x3D; click_action_rdd\</span><br><span class="line">        .join(city_id_to_city_info_rdd)\</span><br><span class="line">        .map(lambda row: Row(city_id&#x3D;row[0], city_name&#x3D;row[1][1][1], area&#x3D;row[1][1][2], product_id&#x3D;row[1][0][1]))</span><br><span class="line"></span><br><span class="line">    schema_click_product &#x3D; spark.createDataFrame(temp_click_product_rdd)</span><br><span class="line">    schema_click_product.createOrReplaceTempView(&quot;tmp_clk_prod_basic&quot;)</span><br><span class="line">    # schema_click_product.show()</span><br></pre></td></tr></table></figure><ol start="4"><li>按照 area, product_id 分组, 计算 click_count 创建 <code>tmp_area_product_click_count</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def schema_temp_area_prdocut_click_count_table(spark):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    生成各区域各商品点击次数临时表</span><br><span class="line">    :param spark:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sql &#x3D; &quot;SELECT &quot; \</span><br><span class="line">          &quot;area, &quot; \</span><br><span class="line">          &quot;product_id, &quot; \</span><br><span class="line">          &quot;count(*) as click_count, &quot; \</span><br><span class="line">          &quot;collect_set(concat(city_id, &#39;:&#39;, city_name)) as city_info &quot; \</span><br><span class="line">          &quot;FROM tmp_clk_prod_basic &quot; \</span><br><span class="line">          &quot;GROUP BY area, product_id&quot;</span><br><span class="line">    df &#x3D; spark.sql(sql)</span><br><span class="line">    df.createOrReplaceTempView(&quot;tmp_area_product_click_count&quot;)</span><br></pre></td></tr></table></figure><ol start="5"><li>利用开窗函数从 <code>tmp_area_product_click_count</code> 取出每个区域中点击量排名前三的 product_id</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def get_area_top3_product_rdd(spark):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取各区域 top3 商品</span><br><span class="line">    先使用开窗函数进行一个子查询, 查询出一个 每个区域内 商品的 click_count 按照倒序排序的结果集</span><br><span class="line">    然后在得到的结果集中,取每个区域中 行号 1-3 的行</span><br><span class="line">    :param spark:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sql &#x3D; &quot;SELECT &quot; \</span><br><span class="line">            &quot;area, &quot; \</span><br><span class="line">            &quot;CASE &quot; \</span><br><span class="line">                &quot;WHEN area&#x3D;&#39;华北&#39; OR area&#x3D;&#39;华东&#39; THEN &#39;A级&#39; &quot; \</span><br><span class="line">                &quot;WHEN area&#x3D;&#39;华中&#39; OR area&#x3D;&#39;华南&#39;  THEN &#39;B级&#39; &quot; \</span><br><span class="line">                &quot;WHEN area&#x3D;&#39;西北&#39; OR area&#x3D;&#39;西南&#39;  THEN &#39;C级&#39; &quot; \</span><br><span class="line">                &quot;ELSE &#39;D级&#39;&quot; \</span><br><span class="line">            &quot;END area_level, &quot; \</span><br><span class="line">          &quot;product_id, &quot; \</span><br><span class="line">          &quot;click_count, &quot; \</span><br><span class="line">          &quot;city_info &quot; \</span><br><span class="line">          &quot;FROM &quot; \</span><br><span class="line">            &quot;(&quot; \</span><br><span class="line">                &quot;SELECT &quot; \</span><br><span class="line">                    &quot;area, &quot; \</span><br><span class="line">                    &quot;product_id, &quot; \</span><br><span class="line">                    &quot;click_count, &quot; \</span><br><span class="line">                    &quot;city_info, &quot; \</span><br><span class="line">                    &quot;row_number() OVER(PARTITION BY area ORDER BY click_count DESC ) as rank &quot; \</span><br><span class="line">                    &quot;FROM tmp_area_product_click_count) AS t &quot; \</span><br><span class="line">          &quot;WHERE rank &lt;&#x3D;3&quot;</span><br><span class="line"></span><br><span class="line">    df &#x3D; spark.sql(sql)</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算页面单跳转化率(pyspark)</title>
      <link href="/2018-04-20-spark-app-page-convert-rate.html"/>
      <url>/2018-04-20-spark-app-page-convert-rate.html</url>
      
        <content type="html"><![CDATA[<p>本文可看做是 <a href="http://waterandair.top/spark-app-session-analysis.html" target="_blank" rel="noopener">用户行为分析(pyspark)</a> 的后续篇, 主要意在介绍使用spark如何计算页面单跳转化率,完整代码可以点击参考这里,代码中有详细的注释,所以可以也可以略过文章,直接看代码<a href="https://github.com/waterandair/daily-learning/blob/master/learn-spark/python/emulate_project/page_one_step_convert_rate_app.py" target="_blank" rel="noopener">项目源码地址</a></p><a id="more"></a>  <h3 id="模拟数据"><a href="#模拟数据" class="headerlink" title="模拟数据"></a>模拟数据</h3><p>模拟数据与 <a href="http://waterandair.top/spark-app-session-analysis.html" target="_blank" rel="noopener">用户行为分析(pyspark)</a> 中的模拟数据一样</p><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>根据指定的页码,计算各个网页之间的跳转转化率,比如用户输入页码 <code>1, 3, 5, 7</code>, 就要返回<br><code>1-&gt;3, 3-&gt;5, 5-&gt;7</code> 这三个页面之间的跳转率</p><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ol><li>首先要按照session粒度聚合用户行为数据,这一点,同<a href="http://waterandair.top/spark-app-session-analysis.html" target="_blank" rel="noopener">用户行为分析(pyspark)</a></li><li>对session rdd 做 flatmap 操作, 对session 下的所有行为记录按照时间做升序排序,对于这些连续的行为记录,认为后一个页面是从前一个页面跳转来的,比如按照时间排序后访问的页码为 <code>1, 3</code>, 那么就认为页面 3 是从页面 1 跳转过来的.遍历排好序的页码,生成格式为 <code>(&quot;1_3&quot;, 1)</code> 的关于页面切片的 rdd,这里要特别注意需要生成<code>起始页</code>的访问数,可以记作<code>0_1</code></li><li>对上面生成的 rdd 执行 countByKey 操作,可以计算出每个切片的数量</li><li>根据用户输入的页码,生成页码切片,比如用户输入的页码为 ``1, 3, 5, 7<code>, 就要生成</code>0-&gt;1,1-&gt;3, 3-&gt;5, 5-&gt;7<code>这四个切片,从上面 countByKey 中取出相应切片的数量,量量相除,可得转化率,比如用</code>1-&gt;3<code>的数量除以</code>0-&gt;1` 的数量,就可以计算出从页面 1 到 3 的跳转率</li></ol><p>具体实现,参考<a href="https://github.com/waterandair/daily-learning/blob/master/learn-spark/python/emulate_project/page_one_step_convert_rate_app.py" target="_blank" rel="noopener">代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用户行为分析(pyspark)</title>
      <link href="/2018-04-19-spark-app-session-analysis.html"/>
      <url>/2018-04-19-spark-app-session-analysis.html</url>
      
        <content type="html"><![CDATA[<p>本文主要意在介绍spark用户行为分析的流程和主要功点,仅会贴出部分代码,完整代码可以点击参考这里,代码中有详细的注释,所以可以也可以略过文章,直接看代码<a href="https://github.com/waterandair/daily-learning/blob/master/learn-spark/python/emulate_project/session_analysis_app.py" target="_blank" rel="noopener">项目源码地址</a></p><a id="more"></a><h3 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h3><p>用户行为分析,是所有互联网公司都会进行的一个项目.通过用户的访问行为记录,从多种维度分析用户行为,帮助公司了解用户,辅助决策.</p><h3 id="项目模拟数据"><a href="#项目模拟数据" class="headerlink" title="项目模拟数据"></a>项目模拟数据</h3><p>为了方便的测试代码,这里使用脚本生成类似生产环境的模拟数据</p><h4 id="生成模拟数据"><a href="#生成模拟数据" class="headerlink" title="生成模拟数据"></a>生成模拟数据</h4><p>执行脚本,生成两个文件,一个是用户行为记录数据,一个是用户详细信息数据.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">import random</span><br><span class="line">import uuid</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imitate_data():</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    生成模拟数据</span><br><span class="line">    用户行为表: date, userid, sessionid, page_id, action_time, search_key_word, click_category_id,click_product_id, order_category_ids, order_product_ids, pay_category_ids, pay_product_ids, city_id</span><br><span class="line">    用户表: userid, username, name, age, professional, city, gender</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rows &#x3D; []</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;初始化 user_visit_action&quot;&quot;&quot;</span><br><span class="line">    search_key_words &#x3D; [&quot;火锅&quot;, &quot;蛋糕&quot;, &quot;重庆辣子鸡&quot;, &quot;重庆小面&quot;, &quot;呷哺呷哺&quot;, &quot;新辣道鱼火锅&quot;, &quot;国贸大厦&quot;, &quot;太古商场&quot;, &quot;日本料理&quot;, &quot;温泉&quot;]</span><br><span class="line">    date &#x3D; datetime.date.today().strftime(&quot;%Y-%m-%d&quot;)</span><br><span class="line">    actions &#x3D; [&quot;search&quot;, &quot;click&quot;, &quot;order&quot;, &quot;pay&quot;]</span><br><span class="line"></span><br><span class="line">    for i in range(100):</span><br><span class="line">        # 用户id</span><br><span class="line">        userid &#x3D; str(random.randint(1, 100))</span><br><span class="line">        city_id &#x3D; str(random.randint(0, 9))</span><br><span class="line"></span><br><span class="line">        for j in range(10):</span><br><span class="line">            # 使用随机 uuid 模拟 session_id, 代表某次访问事件</span><br><span class="line">            sessionid &#x3D; str(uuid.uuid4()).replace(&quot;-&quot;, &quot;&quot;)</span><br><span class="line">            base_action_time &#x3D; date + &quot; &quot; + str(random.randint(0, 23)).zfill(2)</span><br><span class="line"></span><br><span class="line">            for k in range(random.randint(1, 100)):</span><br><span class="line">                # 分页id</span><br><span class="line">                page_id &#x3D; str(random.randint(1, 10))</span><br><span class="line">                # 点击行为发生的具体时间</span><br><span class="line">                action_time &#x3D; base_action_time + &quot;:&quot; + \</span><br><span class="line">                    str(random.randint(0, 59)).zfill(2) + &quot;:&quot; + \</span><br><span class="line">                    str(random.randint(0, 59)).zfill(2)</span><br><span class="line">                # 如果是搜索行为,这里是搜索关键字</span><br><span class="line">                search_key_word &#x3D; &#39;&#39;</span><br><span class="line">                # 在网站首页点击了某个品类</span><br><span class="line">                click_category_id &#x3D; &#39;&#39;</span><br><span class="line">                # 可能是在网站首页或商品列表点击了某个商品</span><br><span class="line">                click_product_id &#x3D; &#39;&#39;</span><br><span class="line">                # 订单中的商品的品类</span><br><span class="line">                order_category_ids &#x3D; &#39;&#39;</span><br><span class="line">                # 订单中的商品</span><br><span class="line">                order_product_ids &#x3D; &#39;&#39;</span><br><span class="line">                # 某次支付行为下所有商品的品类</span><br><span class="line">                pay_category_ids &#x3D; &#39;&#39;</span><br><span class="line">                # 某次支付行为中的所有商品</span><br><span class="line">                pay_product_ids &#x3D; &#39;&#39;</span><br><span class="line">                action &#x3D; actions[random.randint(0, 3)]</span><br><span class="line"></span><br><span class="line">                if action &#x3D;&#x3D; &#39;search&#39;:</span><br><span class="line">                    search_key_word &#x3D; search_key_words[random.randint(0, 9)]</span><br><span class="line">                elif action &#x3D;&#x3D; &#39;click&#39;:</span><br><span class="line">                    click_category_id &#x3D; str(random.randint(1, 100))</span><br><span class="line">                    click_product_id &#x3D; str(random.randint(1, 100))</span><br><span class="line">                elif action &#x3D;&#x3D; &#39;order&#39;:</span><br><span class="line">                    order_category_ids &#x3D; str(random.randint(1, 100))</span><br><span class="line">                    order_product_ids &#x3D; str(random.randint(1, 100))</span><br><span class="line">                elif action &#x3D;&#x3D; &#39;pay&#39;:</span><br><span class="line">                    pay_category_ids &#x3D; str(random.randint(1, 100))</span><br><span class="line">                    pay_product_ids &#x3D; str(random.randint(1, 100))</span><br><span class="line">                else:</span><br><span class="line">                    raise Exception(&quot;action error&quot;)</span><br><span class="line"></span><br><span class="line">                row &#x3D; (date, userid, sessionid, page_id, action_time, search_key_word, click_category_id,</span><br><span class="line">                       click_product_id, order_category_ids, order_product_ids, pay_category_ids, pay_product_ids, city_id)</span><br><span class="line">                row &#x3D; &quot;,&quot;.join(row)</span><br><span class="line">                rows.append(row)</span><br><span class="line"></span><br><span class="line">    path &#x3D; os.path.dirname(os.path.realpath(__file__))</span><br><span class="line">    with open(path + &quot;&#x2F;session_analysis_data.txt&quot;, &#39;w&#39;) as f:</span><br><span class="line">        for line in rows:</span><br><span class="line">            f.write(line + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;初始化 user_info&quot;&quot;&quot;</span><br><span class="line">    rows.clear()</span><br><span class="line">    genders &#x3D; [&quot;male&quot;, &quot;female&quot;]</span><br><span class="line">    for i in range(1, 101):</span><br><span class="line">        userid &#x3D; str(i)</span><br><span class="line">        username &#x3D; &quot;user&quot; + str(i)</span><br><span class="line">        name &#x3D; &quot;name&quot; + str(i)</span><br><span class="line">        age &#x3D; str(random.randint(1, 60))</span><br><span class="line">        professional &#x3D; &quot;professional&quot; + str(i)</span><br><span class="line">        city &#x3D; &quot;city&quot; + str(random.randint(0, 9))</span><br><span class="line">        gender &#x3D; genders[random.randint(0, 1)]</span><br><span class="line">        row &#x3D; (userid, username, name, age, professional, city, gender)</span><br><span class="line">        row &#x3D; &quot;,&quot;.join(row)</span><br><span class="line">        rows.append(row)</span><br><span class="line"></span><br><span class="line">    with open(path + &quot;&#x2F;user_info.txt&quot;, &#39;w&#39;) as f:</span><br><span class="line">        for line in rows:</span><br><span class="line">            f.write(line + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    imitate_data()</span><br></pre></td></tr></table></figure><h4 id="生成模拟表"><a href="#生成模拟表" class="headerlink" title="生成模拟表"></a>生成模拟表</h4><p>spark 读取文件中的数据,并转换为hive表的形式供后续使用</p><h3 id="功能点"><a href="#功能点" class="headerlink" title="功能点"></a>功能点</h3><h4 id="按照-session-粒度聚合数据"><a href="#按照-session-粒度聚合数据" class="headerlink" title="按照 session 粒度聚合数据"></a>按照 session 粒度聚合数据</h4><h5 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h5><p>要求可以灵活的针对不同的筛选条件对用户进行分析,比如查询年龄在20~30岁的教师在当天浏览过哪些网页</p><h5 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h5><p>分析用户行为,首先应该考虑两个问题:  </p><ol><li>对于大部分互联网产品,记录用户行为的表或日志都会非常大,比如一个日获千万的应用,用户行为表可能一天就有 10 亿左右条新增记录.</li><li>“过滤”的条件的粒度是不同的,比如按照访问时长筛选是 session 粒度的,按照点击品类是 action 粒度的,按照年龄,性别是 用户粒度的,如果不做特别处理,每次查询都要做全表扫表,非常消耗性能</li></ol><p>针对上述问题,一个可靠的解决方案是对原始按照session粒度聚合,聚合后的每一行表示某一用户在某个会话期间所进行的操作,比如访问过的网页,点击过的产品,会话的时长,以及当前用户的年龄,职业等用户信息.</p><h4 id="统计访问时长和访问步长"><a href="#统计访问时长和访问步长" class="headerlink" title="统计访问时长和访问步长"></a>统计访问时长和访问步长</h4><h5 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h5><p>统计出符合条件的session中访问时长在1s<del>3s、4s</del>6s、7s<del>9s、10s</del>30s、30s<del>60s、1m</del>3m、3m<del>10m、10m</del>30m、30m以上各个范围内的session占比；访问步长在1<del>3、4</del>6、7<del>9、10</del>30、30<del>60、60以上各个范围内的session占比.假设有1000万个session记录,访问时长在 1</del>3s的有100万个,那么1~3s访问步长就占比10%.通过这里功能,可以了解到用户使用产品的一些习惯</p><h5 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h5><p><strong>访问时长:</strong> 某个用户某次访问会话的时长(用最后一个行为记录的时间减去第一个行为记录的时间)<br><strong>访问步长:</strong> 某个用户某次访问会话中点击过的页面数量</p><p>用于 spark 是分布式计算,所以在做全局统计的时候,不能使用对自定义变量做累加操作的方式进行统计,针对这样的需求,spark提供了 Accumulators（累加器）.  </p><p>由于累加器仅仅支持int的累加,如果直接使用 Accumulator 的方式,需要很繁琐的定义十几个累加器,这样做不便于管理,会使代码显得繁琐,很容易出错,理想的方式是自定义一个累加器,对各个访问时长和访问步长统一进行累加操作</p><h5 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h5><p>自定义累加器需要实现一个继承 AccumulatorParam 的类, 并实现 <code>zero</code>方法和 <code>addInPlace</code>方法, <code>zero</code>方法返回一个自定义类型的初始值, <code>addInPlace</code> 方法定义累加的方式.</p><p><strong>注意:</strong>要时刻清楚spark是分布式计算的, 可能会把在一个 executor 中累加好的结果累加到另一个结果上,所以对于下面的代码,在进行累加的时候,需要判断value2(累加值)是单个值还是多个值累加的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># 定义常量</span><br><span class="line">TIME_PERIOD_1s_3s &#x3D; &quot;1s_3s&quot;</span><br><span class="line">TIME_PERIOD_4s_6s &#x3D; &quot;4s_6s&quot;</span><br><span class="line">TIME_PERIOD_7s_9s &#x3D; &quot;7s_9s&quot;</span><br><span class="line">TIME_PERIOD_10s_30s &#x3D; &quot;10s_30s&quot;</span><br><span class="line">TIME_PERIOD_30s_60s &#x3D; &quot;30s_60s&quot;</span><br><span class="line">TIME_PERIOD_1m_3m &#x3D; &quot;1m_3m&quot;</span><br><span class="line">TIME_PERIOD_3m_10m &#x3D; &quot;3m_10m&quot;</span><br><span class="line">TIME_PERIOD_10m_30m &#x3D; &quot;10m_30m&quot;</span><br><span class="line">TIME_PERIOD_30m &#x3D; &quot;30m&quot;</span><br><span class="line"></span><br><span class="line">STEP_PERIOD_1_3 &#x3D; &quot;1_3&quot;</span><br><span class="line">STEP_PERIOD_4_6 &#x3D; &quot;4_6&quot;</span><br><span class="line">STEP_PERIOD_7_9 &#x3D; &quot;7_9&quot;</span><br><span class="line">STEP_PERIOD_10_30 &#x3D; &quot;10_30&quot;</span><br><span class="line">STEP_PERIOD_30_60 &#x3D; &quot;30_60&quot;</span><br><span class="line">STEP_PERIOD_60 &#x3D; &quot;60&quot;</span><br><span class="line"></span><br><span class="line"># 自定义累加器</span><br><span class="line">from .constant import *</span><br><span class="line">from pyspark import AccumulatorParam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SessionAggrAccumulator(AccumulatorParam):</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    自定义累加器</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def zero(self, value):</span><br><span class="line">        return &#123;</span><br><span class="line">            SESSION_COUNT: 0,</span><br><span class="line">            TIME_PERIOD_1s_3s: 0,</span><br><span class="line">            TIME_PERIOD_4s_6s: 0,</span><br><span class="line">            TIME_PERIOD_7s_9s: 0,</span><br><span class="line">            TIME_PERIOD_10s_30s: 0,</span><br><span class="line">            TIME_PERIOD_30s_60s: 0,</span><br><span class="line">            TIME_PERIOD_1m_3m: 0,</span><br><span class="line">            TIME_PERIOD_3m_10m: 0,</span><br><span class="line">            TIME_PERIOD_10m_30m: 0,</span><br><span class="line">            TIME_PERIOD_30m: 0,</span><br><span class="line">            STEP_PERIOD_1_3: 0,</span><br><span class="line">            STEP_PERIOD_4_6: 0,</span><br><span class="line">            STEP_PERIOD_7_9: 0,</span><br><span class="line">            STEP_PERIOD_10_30: 0,</span><br><span class="line">            STEP_PERIOD_30_60: 0,</span><br><span class="line">            STEP_PERIOD_60: 0</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    def addInPlace(self, value1, value2):</span><br><span class="line">        # print(value1, value2)</span><br><span class="line">        if value1 &#x3D;&#x3D; &quot;&quot;:</span><br><span class="line">            return value2</span><br><span class="line">        if isinstance(value2, dict):</span><br><span class="line">            # rdd 可能会被分割成多分并行计算,所以这里处理当 value2 传入的是某个rdd某个部分计算的值</span><br><span class="line">            value &#x3D; &#123;k: v + value2[k] for k, v in value1.items()&#125;</span><br><span class="line">            return value</span><br><span class="line">        else:</span><br><span class="line">            value1[value2] +&#x3D; 1</span><br><span class="line">            return value1</span><br></pre></td></tr></table></figure><h4 id="按时间比例随机抽取指定数量的session"><a href="#按时间比例随机抽取指定数量的session" class="headerlink" title="按时间比例随机抽取指定数量的session"></a>按时间比例随机抽取指定数量的session</h4><h5 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h5><p>需要从每天的访问记录中随机抽取100次访问记录,要求按照每小时session量的比例抽取,假设在00:00 ~ 01:00 有1000session记录,一天总session记录为10000条,那么就要在00:00 ~ 01:00中抽取10条,这样的方式可以保证抽取的公平性</p><h5 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h5><ol><li>将 session的rdd 中的日期小时(yyyy-MM-dd_HH) 提取出来,转换成格式为<code>&lt;yyyy-MM-dd_HH,&lt;row&gt;&gt;</code>的rdd, 记作 <code>time_session_id_rdd</code></li><li>对 <code>time_session_id_rdd</code> 做 <code>countByKey</code> 操作,算出每小时session的数量, rdd格式转换为  <code>&lt;yyyy-MM-dd_HH,count&gt;</code>, 记作 <code>count_session_by_hour</code></li><li>将<code>count_session_by_hour</code>用分隔符做处理,转换为  {yyyy-MM-dd:{HH:count,HH:count, …},}的字典, 记作 <code>date_hour_counts</code></li><li>根据 <code>date_hour_counts</code>的记录,就可以算出一天中每小时需求抽取多少条数据,然后生成每小时要抽取的 session 的随机索引</li><li>对从第一步中得到的<code>time_session_id_rdd</code>做groupByKey()操作,遍历并按照随机索引抽取session_id</li></ol><h4 id="获取点击-下单-支付数量排名前-N-的品类"><a href="#获取点击-下单-支付数量排名前-N-的品类" class="headerlink" title="获取点击,下单,支付数量排名前 N 的品类"></a>获取点击,下单,支付数量排名前 N 的品类</h4><h5 id="需求-3"><a href="#需求-3" class="headerlink" title="需求"></a>需求</h5><p>为了比较产品的受欢迎程度,对所有 session 中对每个品类的按照点击次数,下单数量和支付数量进行倒序排序,取出排名前N的品类</p><h5 id="方案-3"><a href="#方案-3" class="headerlink" title="方案"></a>方案</h5><p>因为要对三个字段进行排序,即需要进行多次排序,比如两个session点击量相同,就去比较下单数量,如果下数量也相同,就再比较支付数量.spark 中使用 <code>sortByKey</code> 对rdd排序,为了满足多次排序,如果把 rdd 构造为 <code>&lt;(click_count,order_count,pay_count), &lt;row&gt;</code>的形式.</p><h4 id="取出排名前-N-的品类中-被点击次数排名前-N-的session"><a href="#取出排名前-N-的品类中-被点击次数排名前-N-的session" class="headerlink" title="取出排名前 N 的品类中,被点击次数排名前 N 的session"></a>取出排名前 N 的品类中,被点击次数排名前 N 的session</h4><h5 id="需求-4"><a href="#需求-4" class="headerlink" title="需求"></a>需求</h5><p>为了了解每个受欢迎的品类中的优质客户,计算每个品类中点击量排名前N的session</p><h5 id="方案-4"><a href="#方案-4" class="headerlink" title="方案"></a>方案</h5><ol><li>取出所有 category_id, 并转换为  &lt;category_id, category_id&gt; 形式的rdd</li><li>将 session 原始rdd 转换为  &lt;category_id, &lt;session_id, click_count&gt;&gt; 形式的rdd</li><li>合并上面的两个 rdd, 进行groupByKey 操作,再对每个分组中的 click_count, 进行排序,取出排名前 N 的记录</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pyspark 自定义累加器(python)</title>
      <link href="/2018-04-03-pyspark-custom-accumulator.html"/>
      <url>/2018-04-03-pyspark-custom-accumulator.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>spark 的累加器只支持数值型和浮点型的数据类型, 可以使用自定义的累加器完成不同的累加计算  </p></blockquote><a id="more"></a>  <p>当需要定义很多累加操作的时候,需要定义很多个累加器,这样有的时候不便于管理,可以使用自定义累加器,把所有要累加的字段加入到一个累加器中,进行累加计算.</p><p><strong>举个例子</strong>: 假设要累加 rdd 数据中 a, b, c, d的个数,普通的做法是定义四个累加器, 在算子中判断数据是否等于期望的值,满足条件对相应的累加器进行累加, 这样的话,假设要累加更多的字段, 就需要定义更多的累加器.  </p><p><strong>解决:</strong> 针对这种情况,自定义一个累加器,让所有的累加操作在一个累加器中完成</p><p><strong>*代码如下:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark import AccumulatorParam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyAccum(AccumulatorParam):</span><br><span class="line"></span><br><span class="line">    def zero(self, value):</span><br><span class="line">        return &#123;&quot;a&quot;: 0, &quot;b&quot;: 0, &quot;c&quot;: 0, &quot;d&quot;: 0&#125;</span><br><span class="line"></span><br><span class="line">    def addInPlace(self, value1, value2):</span><br><span class="line">        if value1 &#x3D;&#x3D; &quot;&quot;:</span><br><span class="line">            return value2</span><br><span class="line">        if isinstance(value2, dict):</span><br><span class="line">            # rdd 可能会被分割成多份并行计算,所以这里处理当 value2 为某部分 rdd 计算得到的值 </span><br><span class="line">            value &#x3D; &#123;k: v + value2[k] for k, v in value1.items()&#125;</span><br><span class="line">            return value</span><br><span class="line">        else:</span><br><span class="line">            if value1.get(value2) is not None:</span><br><span class="line">                value1[value2] +&#x3D; 1</span><br><span class="line">        return value1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local&quot;)</span><br><span class="line">accum &#x3D; sc.accumulator(&quot;&quot;, accum_param&#x3D;MyAccum())</span><br><span class="line"></span><br><span class="line">rdd &#x3D; sc.parallelize([&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;, &quot;e&quot;, &quot;d&quot;, &quot;c&quot;])</span><br><span class="line">rdd &#x3D; rdd.map(lambda x: accum.add(x))</span><br><span class="line">rdd.count()</span><br><span class="line">print(accum.value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 输出结果为: </span><br><span class="line"># &#123;&#39;d&#39;: 1, &#39;c&#39;: 2, &#39;b&#39;: 1, &#39;a&#39;: 2&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis Cluster 介绍</title>
      <link href="/2018-03-21-redis-cluster.html"/>
      <url>/2018-03-21-redis-cluster.html</url>
      
        <content type="html"><![CDATA[<p>Redis 的高可用策略包括持久化,复制,哨兵和集群,本文讲解redis的集群。主从复制+哨兵的架构适合数据量比较小的场景，数据量受到master节点的约束，但如果是海量数据，就要考虑多master+多slave的架构，Redis提供的Cluster就是应用在这样的需要灵活扩展集群容量的场景下。</p><a id="more"></a><h3 id="Redis-Cluster-介绍"><a href="#Redis-Cluster-介绍" class="headerlink" title="Redis Cluster 介绍"></a>Redis Cluster 介绍</h3><p> Redis Cluster 是Redis提供的一个分布式存储方案，它可以自动将数据进行分片，每个master放一部分数据，实现海量存储，它提供了高可用机制，不需要手动配置复制、哨兵就可以实现故障转移，配置更新。 Redis Cluster 模式下，每个Redis会开放两个端口，比如一个是6379，一个是16379，前者用来对外提供服务，后者用于节点间通信，完成故障发现和转移。</p><h3 id="数据分布算法介绍"><a href="#数据分布算法介绍" class="headerlink" title="数据分布算法介绍"></a>数据分布算法介绍</h3><h4 id="原始的-hash-算法"><a href="#原始的-hash-算法" class="headerlink" title="原始的 hash 算法"></a>原始的 hash 算法</h4><p>这种算法是根据节点数量对<code>key</code>取模，然后存到相应的节点，这种算法很不可取，因为一旦有一个节点的故障了，大部分数据就要重新取模写入缓存，这是不能接受的。</p><h4 id="一致性-hash"><a href="#一致性-hash" class="headerlink" title="一致性 hash"></a>一致性 hash</h4><p><img src="/images/redis/cluster-01.png" alt="image"></p><ul><li>想象有一个圆环，计算每个节点的hash值后放置到圆环上</li><li>当有一个<code>key</code>要存储的时候，就计算<code>key</code>的hash值并放置在圆环上，然后顺时针去找到距离最近的节点存储</li><li>当某一个节点宕机的时候，这个节点上的请求就会转移到下一个节点。</li></ul><p>这样的算法优点是只有宕机节点中的数据会受到影响，缺点是容易造成单节点瓶颈。</p><p><strong>虚拟节点：</strong><br><img src="/images/redis/cluster-02.png" alt="image"></p><p>针对普通一致性哈希算法的缺点，引入了虚拟节点，其含义如图所示，对每个节点都做了均匀的分布，这个每个区间的<code>key</code>， 就可以均匀的分配到各个节点，这样，当某个节点宕机后，就可以把宕机节点的数据均匀的分布到正常工作的节点</p><h4 id="Redis-cluster-的-hash-solt-算法"><a href="#Redis-cluster-的-hash-solt-算法" class="headerlink" title="Redis cluster 的 hash solt 算法"></a>Redis cluster 的 hash solt 算法</h4><p>redis cluster 有固定的 16384 个 <code>hash slot</code>，当接收到写请求后，会对 <code>key</code>计算CRC16值，然后再对16384取模，这样就可以获取<code>key</code>对应的<code>hash slot</code>。redis cluster 中每个 master 都会持有部分slot，当增加一个master时，就将其他master的<code>hash slot</code>部分移动过去，减少一个master，就将它的<code>hash slot</code>移动到其他master上去。</p><p>这样就可以高效的解决节点变化时造成的数据转移开销。</p><h3 id="实践演练"><a href="#实践演练" class="headerlink" title="实践演练"></a>实践演练</h3><h4 id="配置项说明"><a href="#配置项说明" class="headerlink" title="配置项说明"></a>配置项说明</h4><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>cluster-enabled yes</td><td>开启cluster</td></tr><tr><td>cluster-config-file <filename></td><td>指定一个文件，cluster自动更新集群状态保存在那里，包括集群中其他节点的状态，故障转移</td></tr><tr><td>cluster-node-timeout 5000</td><td>节点存活超时时长，超时后会被认为宕机并触发主备切换</td></tr></tbody></table><h4 id="搭建环境过程"><a href="#搭建环境过程" class="headerlink" title="搭建环境过程"></a>搭建环境过程</h4><h5 id="准备节点"><a href="#准备节点" class="headerlink" title="准备节点"></a>准备节点</h5><p>cluster 建议最少使用6个节点去搭建环境，其中有三个master和三个salve</p><p>这里准备的6个节点的地址为：172.17.0.2 — 172.17.0.7</p><h5 id="修改所有节点的配置"><a href="#修改所有节点的配置" class="headerlink" title="修改所有节点的配置"></a>修改所有节点的配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cluster-enabled yes</span><br><span class="line">cluster-config-file &#x2F;etc&#x2F;redis&#x2F;cluster.conf</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line">appendonly yes</span><br></pre></td></tr></table></figure><p>注意不要配置中不要配置 slave， cluster 会自动管理</p><h5 id="启动6个节点"><a href="#启动6个节点" class="headerlink" title="启动6个节点"></a>启动6个节点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server &#x2F;etc&#x2F;redis&#x2F;6379.conf &amp;</span><br></pre></td></tr></table></figure><h5 id="查看是否开启-cluster"><a href="#查看是否开启-cluster" class="headerlink" title="查看是否开启 cluster"></a>查看是否开启 cluster</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zj@zj-pc:~$ redis-cli -h 172.17.0.2      </span><br><span class="line">172.17.0.2:6379&gt; info cluster</span><br><span class="line"># Cluster</span><br><span class="line">cluster_enabled:1</span><br></pre></td></tr></table></figure><h5 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h5><p>Redis 提供了创建集群的辅助工具 <code>redis-trib.rb</code>，这个工具是<code>ruby</code>编写的且依赖 <code>gem</code> 包 reids，所以先配置<code>ruby</code>环境，并执行 <code>gem install redis</code></p><p>创建集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">zj@zj-pc:bin$ redis-trib.rb create --replicas 1 172.17.0.2:6379 172.17.0.3:6379 172.17.0.4:6379 172.17.0.5:6379  172.17.0.6:6379 172.17.0.7:6379</span><br><span class="line">&gt;&gt;&gt; Creating cluster</span><br><span class="line">&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...</span><br><span class="line">Using 3 masters:</span><br><span class="line">172.17.0.2:6379</span><br><span class="line">172.17.0.3:6379</span><br><span class="line">172.17.0.4:6379</span><br><span class="line">Adding replica 172.17.0.6:6379 to 172.17.0.2:6379</span><br><span class="line">Adding replica 172.17.0.7:6379 to 172.17.0.3:6379</span><br><span class="line">Adding replica 172.17.0.5:6379 to 172.17.0.4:6379</span><br><span class="line">M: 2b6e3598a7d0d5610c42a402764a8bd19b2e0b8f 172.17.0.2:6379</span><br><span class="line">   slots:0-5460 (5461 slots) master</span><br><span class="line">M: aeb8d6cd17e33b7aa65a0b7c21630aabbcce7da9 172.17.0.3:6379</span><br><span class="line">   slots:5461-10922 (5462 slots) master</span><br><span class="line">M: 321427ab1b0c481307468d877faa645949da35f3 172.17.0.4:6379</span><br><span class="line">   slots:10923-16383 (5461 slots) master</span><br><span class="line">S: 9adfc49d11550b8acba2cf5c48597849adb29bb6 172.17.0.5:6379</span><br><span class="line">   replicates 321427ab1b0c481307468d877faa645949da35f3</span><br><span class="line">S: e55ca5a127cae6a006a400215e559ab66dd94d24 172.17.0.6:6379</span><br><span class="line">   replicates 2b6e3598a7d0d5610c42a402764a8bd19b2e0b8f</span><br><span class="line">S: 06244da39a951a856c449fcd5343d86a0c7e2a7e 172.17.0.7:6379</span><br><span class="line">   replicates aeb8d6cd17e33b7aa65a0b7c21630aabbcce7da9</span><br><span class="line">Can I set the above configuration? (type &#39;yes&#39; to accept): yes</span><br><span class="line">&gt;&gt;&gt; Nodes configuration updated</span><br><span class="line">&gt;&gt;&gt; Assign a different config epoch to each node</span><br><span class="line">&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span><br><span class="line">Waiting for the cluster to join..</span><br><span class="line">&gt;&gt;&gt; Performing Cluster Check (using node 172.17.0.2:6379)</span><br><span class="line">M: 2b6e3598a7d0d5610c42a402764a8bd19b2e0b8f 172.17.0.2:6379</span><br><span class="line">   slots:0-5460 (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">M: 321427ab1b0c481307468d877faa645949da35f3 172.17.0.4:6379</span><br><span class="line">   slots:10923-16383 (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">M: aeb8d6cd17e33b7aa65a0b7c21630aabbcce7da9 172.17.0.3:6379</span><br><span class="line">   slots:5461-10922 (5462 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 9adfc49d11550b8acba2cf5c48597849adb29bb6 172.17.0.5:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 321427ab1b0c481307468d877faa645949da35f3</span><br><span class="line">S: e55ca5a127cae6a006a400215e559ab66dd94d24 172.17.0.6:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 2b6e3598a7d0d5610c42a402764a8bd19b2e0b8f</span><br><span class="line">S: 06244da39a951a856c449fcd5343d86a0c7e2a7e 172.17.0.7:6379</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates aeb8d6cd17e33b7aa65a0b7c21630aabbcce7da9</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line">&gt;&gt;&gt; Check for open slots...</span><br><span class="line">&gt;&gt;&gt; Check slots coverage...</span><br><span class="line">[OK] All 16384 slots covered.</span><br></pre></td></tr></table></figure><h5 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看集群信息</span><br><span class="line">redis-trib.rb check &lt;ip:port&gt;</span><br><span class="line"># 添加节点</span><br><span class="line">redis-trib.rb add-node &lt;ip:port&gt; &lt;ip:port&gt;</span><br><span class="line"># 给某个节点转移 hashslot</span><br><span class="line">redis-trib.rb reshard &lt;ip:port&gt;</span><br><span class="line"># 给指定master添加slave</span><br><span class="line">redis-trib.rb add-node --slave --master-id &lt;master-id&gt; &lt;ip:port&gt; &lt;ip:port&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 哨兵(sentinal)</title>
      <link href="/2018-03-16-redis-sentinel.html"/>
      <url>/2018-03-16-redis-sentinel.html</url>
      
        <content type="html"><![CDATA[<p>Redis 的高可用策略包括持久化,复制,哨兵和集群,本文讲解redis的哨兵.</p><a id="more"></a><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>哨兵是redis分布式集群架构中非常重要的一个组件，主要有集群监控,消息通知,故障转移等功能.</p><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><h4 id="sdown和odown转换机制"><a href="#sdown和odown转换机制" class="headerlink" title="sdown和odown转换机制"></a>sdown和odown转换机制</h4><ul><li>sdown是主观宕机，哨兵ping一个master，如果超过了 <code>is-master-down-after-milliseconds</code>指定的毫秒数，就主观认为master宕机</li><li>odown是客观宕机，在指定时间内, 哨兵收到了quorum指定数量的哨兵也认为 master 是 sdown 了，那么就认为是 odown 了</li></ul><h4 id="哨兵集群的自动发现机制"><a href="#哨兵集群的自动发现机制" class="headerlink" title="哨兵集群的自动发现机制"></a>哨兵集群的自动发现机制</h4><p>哨兵的自动发现是通过pub/sub实现的，每两秒钟哨兵都会往自己监控的集群中的 <code>__sentinel__:hello</code> channel 里发送一个包含自身信息的消息,<br>每个哨兵都会监听这个 <code>__sentinel__:hello</code> channel，以此确定其他哨兵的存在,并且会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步.</p><h4 id="slave-配置的自动纠正"><a href="#slave-配置的自动纠正" class="headerlink" title="slave 配置的自动纠正"></a>slave 配置的自动纠正</h4><p>哨兵会负责自动纠正 slave 的一些配置，比如哨兵会确保将要成为 master 的 slave 在复制现有 master 的数据，故障转移之后，哨兵会确保 slave 连接到正确的 master 上</p><h4 id="master-选举算法"><a href="#master-选举算法" class="headerlink" title="master 选举算法"></a>master 选举算法</h4><p>当一个 msater 被判定为 odown 并且有 majority 的哨兵允许了主备切换，那么就会从 slave 中选出一个master。</p><p><strong>master 的选取 slave 的步骤：</strong></p><ol><li>首先去掉不合适的slave， 如果与 master 断开连接的时长超过了 <code>(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state</code>，就不能被选为新的master</li><li>根据 slave priority 选举，slave priority越低优先级越高</li><li>如果 slave priority 相同，就比较 replica offset， 复制数据越多的slave优先级越高</li><li>如果 slave priority 和 replica offset 都相同，就选择 run id 最小的</li></ol><h4 id="quorum-和-majority"><a href="#quorum-和-majority" class="headerlink" title="quorum 和 majority"></a>quorum 和 majority</h4><p>做主备切换前，首先需要 quorum 数量的哨兵认为master 是 odown，然后选举出一个哨兵来做切换，这个哨兵还得得到 majority 哨兵的授权，才能正式执行切换。</p><p>假设 quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换，</p><p>如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换</p><h4 id="configuration-epoch"><a href="#configuration-epoch" class="headerlink" title="configuration epoch"></a>configuration epoch</h4><p>哨兵会对 master+slave 进行监控，有相应的监控的配置，执行切换的那个哨兵，会从要切换到的新 master 那里得到一个 configuration epoch ，这就是一个version号，每次切换的 version 号都必须是唯一的<br>如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间后继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号。</p><h4 id="configuraiton传播"><a href="#configuraiton传播" class="headerlink" title="configuraiton传播"></a>configuraiton传播</h4><p>哨兵完成切换之后，会在本地生成最新的master配置包括新的version，然后通过pub/sub消息机制同步给其他的哨兵，其他哨兵根据版本号的大小判断是否更新自己的 master 配置。</p><h4 id="数据丢失问题"><a href="#数据丢失问题" class="headerlink" title="数据丢失问题"></a>数据丢失问题</h4><p>在故障转移,主备切换的时候,可能会发生数据丢失的情况</p><h5 id="异步复制导致的数据丢失"><a href="#异步复制导致的数据丢失" class="headerlink" title="异步复制导致的数据丢失"></a>异步复制导致的数据丢失</h5><p>redis的复制功能是异步的,所以如果master在数据复制的过程中宕机,就会发生数据丢失</p><h5 id="脑裂导致的数据丢失"><a href="#脑裂导致的数据丢失" class="headerlink" title="脑裂导致的数据丢失"></a>脑裂导致的数据丢失</h5><p>master有时会因为网络问题暂时无法连接 salve,这时哨兵会误认为master宕机了并且开始进行故障转移,这会造成哨兵集群中出现两个 master.因为之前的master还在正常的接收请求,所以这时就会发生数据不一致数据丢失的情况</p><h5 id="解决数据丢失问题"><a href="#解决数据丢失问题" class="headerlink" title="解决数据丢失问题"></a>解决数据丢失问题</h5><p>配置一下两个选项可以减少异步复制和脑裂导致的数据丢失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 表示至少有1个slave，数据复制和同步的延迟不能超过10秒,</span><br><span class="line"># 如果所有slave数据同步都超过10秒,则master停止接收请求</span><br><span class="line">min-slaves-to-write 1</span><br><span class="line">min-slaves-max-lag 10</span><br></pre></td></tr></table></figure><p><strong>详细说明:</strong></p><p><code>min-slaves-max-lag</code> 可以确保一旦slave同步数据延迟过长，就认为master宕机后大量丢失数据，这时就拒绝写请求，这样可以把由异步复制导致的数据丢失问题控制到最小范围内.</p><p>当master出现了脑裂，会逐渐和其他slave失去连接,配置<code>min-slaves-to-write</code>就可以给当前连接的slave设置一个阈值,如果小于了设置值,master就直接拒绝客户端的写请求.</p><h3 id="实践演练"><a href="#实践演练" class="headerlink" title="实践演练"></a>实践演练</h3><h4 id="配置项说明"><a href="#配置项说明" class="headerlink" title="配置项说明"></a>配置项说明</h4><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>sentinel monitor <mastername> <host> <port> <quorum></td><td>哨兵指定要监控的master</td></tr><tr><td>down-after-milliseconds <mastername> 1000</td><td>哨兵与redis节点的超时阈值</td></tr><tr><td>sentinel failover-timeout <mastername> 180000</td><td>执行故障转移的timeout超时时长</td></tr><tr><td>sentinel parallel-syncs <mastername> 1</td><td>slave切换为master后同时进行同步的slave数量</td></tr></tbody></table><h4 id="完整配置示例"><a href="#完整配置示例" class="headerlink" title="完整配置示例"></a>完整配置示例</h4><h5 id="准备三个Redis节点"><a href="#准备三个Redis节点" class="headerlink" title="准备三个Redis节点"></a>准备三个Redis节点</h5><p>一主两从，参考<a href="http://waterandair.top/redis-replication.html" target="_blank" rel="noopener">Redis复制</a></p><table><thead><tr><th>ip</th><th>角色</th></tr></thead><tbody><tr><td>172.17.0.2</td><td>master</td></tr><tr><td>172.17.0.3</td><td>slave</td></tr><tr><td>172.17.0.4</td><td>slave</td></tr></tbody></table><h5 id="修改-sentinel-配置文件"><a href="#修改-sentinel-配置文件" class="headerlink" title="修改 sentinel 配置文件"></a>修改 sentinel 配置文件</h5><p>三个节点是一样的配置<code>/etc/redis/sentinel.conf</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sentinel monitor mymaster 172.17.0.2 6379 2</span><br><span class="line">sentinel down-after-milliseconds mymaster 30000</span><br><span class="line">sentinel failover-timeout mymaster 60000</span><br><span class="line">sentinel parallel-syncs mymaster 1</span><br></pre></td></tr></table></figure><h5 id="依次启动三个哨兵"><a href="#依次启动三个哨兵" class="headerlink" title="依次启动三个哨兵"></a>依次启动三个哨兵</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-sentinel &#x2F;etc&#x2F;redis&#x2F;sentinel.conf</span><br></pre></td></tr></table></figure><h5 id="客户端登录查看状态"><a href="#客户端登录查看状态" class="headerlink" title="客户端登录查看状态"></a>客户端登录查看状态</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zj@zj-pc:~$ redis-cli -h 172.17.0.2 -p 26379 </span><br><span class="line">172.17.0.2:26379&gt; sentinel get-master-addr-by-name mymaster</span><br><span class="line">1) &quot;172.17.0.2&quot;</span><br><span class="line">2) &quot;6379&quot;</span><br></pre></td></tr></table></figure><p>其他状态命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sentinel master mymaster</span><br><span class="line">sentinel slaves mymaster</span><br><span class="line">sentinel sentinels mymaster</span><br></pre></td></tr></table></figure><h4 id="容灾演练"><a href="#容灾演练" class="headerlink" title="容灾演练"></a>容灾演练</h4><h5 id="查看当前master节点信息"><a href="#查看当前master节点信息" class="headerlink" title="查看当前master节点信息"></a>查看当前master节点信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.17.0.2:26379&gt; sentinel get-master-addr-by-name mymaster</span><br><span class="line">1) &quot;172.17.0.2&quot;</span><br><span class="line">2) &quot;6379&quot;</span><br></pre></td></tr></table></figure><h5 id="手动使master宕机"><a href="#手动使master宕机" class="headerlink" title="手动使master宕机"></a>手动使master宕机</h5><h6 id="查看-pid"><a href="#查看-pid" class="headerlink" title="查看 pid"></a>查看 pid</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@507b5013f669:~# ps -aux</span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root         1  0.0  0.0  65508  5288 ?        Ss   Jul16   0:00 &#x2F;usr&#x2F;sbin&#x2F;sshd -D</span><br><span class="line">root        47  0.0  0.0  65508  6308 ?        Rs   Jul16   0:00 sshd: root@pts&#x2F;0</span><br><span class="line">root        49  0.0  0.0  18320  3324 pts&#x2F;0    Ss   Jul16   0:00 -bash</span><br><span class="line">root        94  0.3  0.0  39552  4936 pts&#x2F;0    Sl   06:13   0:41 redis-server 0.0.0.0:6379</span><br><span class="line">root       113  0.4  0.0  38412  4004 pts&#x2F;0    Sl   06:31   0:50 redis-sentinel 0.0.0.0:26379 [sentinel]</span><br><span class="line">root       118  0.0  0.0  34424  2892 pts&#x2F;0    R+   09:56   0:00 ps -aux</span><br></pre></td></tr></table></figure><h6 id="kill-命令"><a href="#kill-命令" class="headerlink" title="kill 命令"></a>kill 命令</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@507b5013f669:~# kill -9 94</span><br></pre></td></tr></table></figure><h6 id="删除-pid-文件"><a href="#删除-pid-文件" class="headerlink" title="删除 pid 文件"></a>删除 pid 文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm &#x2F;var&#x2F;run&#x2F;redis_6379.pid</span><br></pre></td></tr></table></figure><h5 id="查看三个哨兵的日志"><a href="#查看三个哨兵的日志" class="headerlink" title="查看三个哨兵的日志"></a>查看三个哨兵的日志</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"># 172.17.0.2 节点</span><br><span class="line">113:X 17 Jul 09:57:04.675 # +sdown master mymaster 172.17.0.2 6379</span><br><span class="line">113:X 17 Jul 09:57:04.711 # +new-epoch 1</span><br><span class="line">113:X 17 Jul 09:57:04.719 # +vote-for-leader b9932b720e70c34059617068763213c5459516aa 1</span><br><span class="line">113:X 17 Jul 09:57:04.758 # +odown master mymaster 172.17.0.2 6379 #quorum 3&#x2F;2</span><br><span class="line">113:X 17 Jul 09:57:04.758 # Next failover delay: I will not start a failover before Tue Jul 17 10:03:05 2018</span><br><span class="line">113:X 17 Jul 09:57:05.552 # +config-update-from sentinel b9932b720e70c34059617068763213c5459516aa 172.17.0.4 26379 @ mymaster 172.17.0.2 6379</span><br><span class="line">113:X 17 Jul 09:57:05.552 # +switch-master mymaster 172.17.0.2 6379 172.17.0.4 6379</span><br><span class="line">113:X 17 Jul 09:57:05.552 * +slave slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">113:X 17 Jul 09:57:05.552 * +slave slave 172.17.0.2:6379 172.17.0.2 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line"></span><br><span class="line"># 172.17.0.3 节点</span><br><span class="line">152:S 17 Jul 09:56:34.576 # Connection with master lost.</span><br><span class="line">152:S 17 Jul 09:56:34.576 * Caching the disconnected master state.</span><br><span class="line">152:S 17 Jul 09:56:35.379 * Connecting to MASTER 172.17.0.2:6379</span><br><span class="line">152:S 17 Jul 09:56:35.379 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">152:S 17 Jul 09:56:35.379 # Error condition on socket for SYNC: Connection refused</span><br><span class="line">152:S 17 Jul 09:56:36.382 * Connecting to MASTER 172.17.0.2:6379</span><br><span class="line">152:S 17 Jul 09:56:36.382 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">152:S 17 Jul 09:56:36.382 # Error condition on socket for SYNC: Connection refused</span><br><span class="line">152:S 17 Jul 09:56:37.386 * Connecting to MASTER 172.17.0.2:6379</span><br><span class="line">152:S 17 Jul 09:56:37.386 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">152:S 17 Jul 09:56:37.386 # Error condition on socket for SYNC: Connection refused</span><br><span class="line">......</span><br><span class="line">161:X 17 Jul 09:57:04.639 # +sdown master mymaster 172.17.0.2 6379</span><br><span class="line">161:X 17 Jul 09:57:04.710 # +new-epoch 1</span><br><span class="line">161:X 17 Jul 09:57:04.719 # +vote-for-leader b9932b720e70c34059617068763213c5459516aa 1</span><br><span class="line">152:S 17 Jul 09:57:05.460 * Connecting to MASTER 172.17.0.2:6379</span><br><span class="line">152:S 17 Jul 09:57:05.461 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">152:S 17 Jul 09:57:05.461 # Error condition on socket for SYNC: Connection refused</span><br><span class="line">152:S 17 Jul 09:57:05.552 * SLAVE OF 172.17.0.4:6379 enabled (user request from &#39;id&#x3D;15 addr&#x3D;172.17.0.4:32787 fd&#x3D;14 name&#x3D;sentinel-b9932b72-cmd age&#x3D;12159 idle&#x3D;0 flags&#x3D;x db&#x3D;0 sub&#x3D;0 psub&#x3D;0 multi&#x3D;3 qbuf&#x3D;135 qbuf-free&#x3D;32633 obl&#x3D;36 oll&#x3D;0 omem&#x3D;0 events&#x3D;r cmd&#x3D;exec&#39;)</span><br><span class="line">161:X 17 Jul 09:57:05.552 # +config-update-from sentinel b9932b720e70c34059617068763213c5459516aa 172.17.0.4 26379 @ mymaster 172.17.0.2 6379</span><br><span class="line">161:X 17 Jul 09:57:05.552 # +switch-master mymaster 172.17.0.2 6379 172.17.0.4 6379</span><br><span class="line">161:X 17 Jul 09:57:05.552 * +slave slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">161:X 17 Jul 09:57:05.552 * +slave slave 172.17.0.2:6379 172.17.0.2 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">152:S 17 Jul 09:57:05.554 # CONFIG REWRITE executed with success.</span><br><span class="line">152:S 17 Jul 09:57:06.464 * Connecting to MASTER 172.17.0.4:6379</span><br><span class="line">152:S 17 Jul 09:57:06.465 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">152:S 17 Jul 09:57:06.465 * Non blocking connect for SYNC fired the event.</span><br><span class="line">152:S 17 Jul 09:57:06.465 * Master replied to PING, replication can continue...</span><br><span class="line">152:S 17 Jul 09:57:06.465 * Trying a partial resynchronization (request 19c4a1290245b212de71a98c36fab894d23be166:2412576).</span><br><span class="line">152:S 17 Jul 09:57:06.466 * Successful partial resynchronization with master.</span><br><span class="line">152:S 17 Jul 09:57:06.466 # Master replication ID changed to 350c1de8bccc6d29411e343d029b5254edc68330</span><br><span class="line">152:S 17 Jul 09:57:06.466 * MASTER &lt;-&gt; SLAVE sync: Master accepted a Partial Resynchronization.</span><br><span class="line"></span><br><span class="line"># 172.17.0.4 节点</span><br><span class="line"></span><br><span class="line">85:X 17 Jul 09:57:04.641 # +sdown master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.700 # +odown master mymaster 172.17.0.2 6379 #quorum 2&#x2F;2</span><br><span class="line">85:X 17 Jul 09:57:04.700 # +new-epoch 1</span><br><span class="line">85:X 17 Jul 09:57:04.700 # +try-failover master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.702 # +vote-for-leader b9932b720e70c34059617068763213c5459516aa 1</span><br><span class="line">85:X 17 Jul 09:57:04.719 # 2ded228e7ac4b38c47d8947cc243375a14c72061 voted for b9932b720e70c34059617068763213c5459516aa 1</span><br><span class="line">85:X 17 Jul 09:57:04.719 # 9ec47b87ed2e9ef90a82d8b2752429fa22708ae1 voted for b9932b720e70c34059617068763213c5459516aa 1</span><br><span class="line">85:X 17 Jul 09:57:04.754 # +elected-leader master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.754 # +failover-state-select-slave master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.838 # +selected-slave slave 172.17.0.4:6379 172.17.0.4 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.838 * +failover-state-send-slaveof-noone slave 172.17.0.4:6379 172.17.0.4 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:04.896 * +failover-state-wait-promotion slave 172.17.0.4:6379 172.17.0.4 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:05.503 # +promoted-slave slave 172.17.0.4:6379 172.17.0.4 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:05.503 # +failover-state-reconf-slaves master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:05.551 * +slave-reconf-sent slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:05.842 # -odown master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:06.543 * +slave-reconf-inprog slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:06.543 * +slave-reconf-done slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:06.596 # +failover-end master mymaster 172.17.0.2 6379</span><br><span class="line">85:X 17 Jul 09:57:06.596 # +switch-master mymaster 172.17.0.2 6379 172.17.0.4 6379</span><br><span class="line">85:X 17 Jul 09:57:06.596 * +slave slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">85:X 17 Jul 09:57:06.596 * +slave slave 172.17.0.2:6379 172.17.0.2 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">85:X 17 Jul 09:57:36.644 # +sdown slave 172.17.0.2:6379 172.17.0.2 6379 @ mymaster 172.17.0.4 6379</span><br></pre></td></tr></table></figure><p>根据日志可以看出：</p><ol><li>首先三个哨兵都认为 master 是 sdown了（<code>+sdown master mymaster 172.17.0.2 6379</code>）</li><li>超过 quorum 指定的哨兵都认为 sdown 后，就变为 odown (<code>+odown master mymaster 172.17.0.2 6379 #quorum 2/2</code>)</li><li>哨兵更新配置版本号（<code>+new-epoch 1</code>）</li><li>172.17.0.4 节点的哨兵尝试执行主备切换（<code>+try-failover master mymaster 172.17.0.2 6379</code>）</li><li>选举出一个要作为master的节点（<code>+vote-for-leader b9932b720e70c34059617068763213c5459516aa 1</code>）</li><li>对被选的slave执行 <code>slaveof-noone</code>，不再做为salve，旧的master不再做为master</li><li>哨兵开始修改各个Redis节点配置</li><li>旧的master(172.17.0.2)被改为salve，依然是宕机状态，被哨兵认为是 sdown</li></ol><h5 id="查看当前master节点信息-1"><a href="#查看当前master节点信息-1" class="headerlink" title="查看当前master节点信息"></a>查看当前master节点信息</h5><p>可以发现master已经从 <code>172.17.0.2</code> 切换到了 <code>172.17.0.4</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.17.0.2:26379&gt; sentinel get-master-addr-by-name mymaster </span><br><span class="line">1) &quot;172.17.0.4&quot;</span><br><span class="line">2) &quot;6379&quot;</span><br></pre></td></tr></table></figure><p>登录 <code>172.17.0.4</code> 查看replication信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">zj@zj-pc:~$ redis-cli -h 172.17.0.4</span><br><span class="line">172.17.0.4:6379&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">slave0:ip&#x3D;172.17.0.3,port&#x3D;6379,state&#x3D;online,offset&#x3D;2468205,lag&#x3D;0</span><br><span class="line">master_replid:350c1de8bccc6d29411e343d029b5254edc68330</span><br><span class="line">master_replid2:19c4a1290245b212de71a98c36fab894d23be166</span><br><span class="line">master_repl_offset:2468205</span><br><span class="line">second_repl_offset:2412576</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1419630</span><br><span class="line">repl_backlog_histlen:1048576</span><br></pre></td></tr></table></figure><p>可以看出当前集群中只有<code>172.17.0.3</code>一个slave， <code>172.17.0.2</code>依然是宕机状态</p><h5 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h5><p>启动 <code>172.17.0.2</code> 节点的Redis进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server &#x2F;etc&#x2F;redis&#x2F;6379.conf &amp;</span><br></pre></td></tr></table></figure><p>查看日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">113:X 17 Jul 10:02:24.631 # -sdown slave 172.17.0.2:6379 172.17.0.2 6379 @ mymaster 172.17.0.4 6379</span><br><span class="line">120:S 17 Jul 10:02:33.886 * Before turning into a slave, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.</span><br><span class="line">120:S 17 Jul 10:02:33.886 * SLAVE OF 172.17.0.4:6379 enabled (user request from &#39;id&#x3D;2 addr&#x3D;172.17.0.3:39643 fd&#x3D;7 name&#x3D;sentinel-9ec47b87-cmd age&#x3D;10 idle&#x3D;0 flags&#x3D;x db&#x3D;0 sub&#x3D;0 psub&#x3D;0 multi&#x3D;3 qbuf&#x3D;0 qbuf-free&#x3D;32768 obl&#x3D;36 oll&#x3D;0 omem&#x3D;0 events&#x3D;r cmd&#x3D;exec&#39;)</span><br><span class="line">120:S 17 Jul 10:02:33.889 # CONFIG REWRITE executed with success.</span><br><span class="line">120:S 17 Jul 10:02:34.755 * Connecting to MASTER 172.17.0.4:6379</span><br><span class="line">120:S 17 Jul 10:02:34.756 * MASTER &lt;-&gt; SLAVE sync started</span><br><span class="line">120:S 17 Jul 10:02:34.756 * Non blocking connect for SYNC fired the event.</span><br><span class="line">120:S 17 Jul 10:02:34.756 * Master replied to PING, replication can continue...</span><br><span class="line">120:S 17 Jul 10:02:34.756 * Trying a partial resynchronization (request c491477a647e8351f61a5edd36dc896049eaf86f:1).</span><br><span class="line">120:S 17 Jul 10:02:34.758 * Full resync from master: 350c1de8bccc6d29411e343d029b5254edc68330:2478400</span><br><span class="line">120:S 17 Jul 10:02:34.758 * Discarding previously cached master state.</span><br><span class="line">120:S 17 Jul 10:02:34.790 * MASTER &lt;-&gt; SLAVE sync: receiving 188 bytes from master</span><br><span class="line">120:S 17 Jul 10:02:34.790 * MASTER &lt;-&gt; SLAVE sync: Flushing old data</span><br><span class="line">120:S 17 Jul 10:02:34.791 * MASTER &lt;-&gt; SLAVE sync: Loading DB in memory</span><br><span class="line">120:S 17 Jul 10:02:34.791 * MASTER &lt;-&gt; SLAVE sync: Finished with success</span><br></pre></td></tr></table></figure><p>可以发现，重启的Redis 依次进行了重写配置，连接master，复制等一系列操作</p><h5 id="再次查看master的replication信息"><a href="#再次查看master的replication信息" class="headerlink" title="再次查看master的replication信息"></a>再次查看master的replication信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">172.17.0.4:6379&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">slave0:ip&#x3D;172.17.0.3,port&#x3D;6379,state&#x3D;online,offset&#x3D;2480192,lag&#x3D;0</span><br><span class="line">slave1:ip&#x3D;172.17.0.2,port&#x3D;6379,state&#x3D;online,offset&#x3D;2480057,lag&#x3D;1</span><br><span class="line">master_replid:350c1de8bccc6d29411e343d029b5254edc68330</span><br><span class="line">master_replid2:19c4a1290245b212de71a98c36fab894d23be166</span><br><span class="line">master_repl_offset:2480192</span><br><span class="line">second_repl_offset:2412576</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1431617</span><br><span class="line">repl_backlog_histlen:1048576</span><br></pre></td></tr></table></figure><p>发现宕机的旧master(172.17.0.2)节点已经成功的成为新master(172.17.0.4)的salve</p><h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><h4 id="哨兵节点的增加和删除"><a href="#哨兵节点的增加和删除" class="headerlink" title="哨兵节点的增加和删除"></a>哨兵节点的增加和删除</h4><p>哨兵节点的增加，可以被自动发现，删除哨兵需要以下步骤：</p><ul><li>停止 setinel 进程</li><li>在其他所有哨兵上执行 <code>SENTINEL RESET *</code></li><li>在其他所有哨兵上执行 <code>SENTINEL MASTER &lt;mastername&gt;</code></li></ul><h4 id="永久下线一个salve"><a href="#永久下线一个salve" class="headerlink" title="永久下线一个salve"></a>永久下线一个salve</h4><p>在所有哨兵上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SENTINEL RESET &lt;mastername&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LinearRegression建立广告投放与销售量的模型(sklearn)</title>
      <link href="/2018-03-10-ml-linearregression-advertising.html"/>
      <url>/2018-03-10-ml-linearregression-advertising.html</url>
      
        <content type="html"><![CDATA[<p>使用 sklearn 中的线性模型建立关于广告投放与销售量的模型,帮助广告主合理投放广告.  </p><p><a href="https://github.com/waterandair/daily-learning/blob/master/learn-ml/regression/logistic_creditcard.py" target="_blank" rel="noopener">源码</a></p><a id="more"></a><h4 id="数据-下载"><a href="#数据-下载" class="headerlink" title="数据(下载)"></a>数据(<a href="https://github.com/waterandair/daily-learning/blob/master/learn-ml/data/" target="_blank" rel="noopener">下载</a>)</h4><p>前三列表示在不同平台的投放量,最后一列表示产品的销售量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">,TV,Radio,Newspaper,Sales</span><br><span class="line">1,230.1,37.8,69.2,22.1</span><br><span class="line">2,44.5,39.3,45.1,10.4</span><br><span class="line">3,17.2,45.9,69.3,9.3</span><br><span class="line">4,151.5,41.3,58.5,18.5</span><br><span class="line"></span><br><span class="line">....</span><br></pre></td></tr></table></figure><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">path &#x3D; &quot;..&#x2F;data&#x2F;advertising.csv&quot;</span><br><span class="line">data &#x3D; pd.read_csv(path)</span><br><span class="line">x &#x3D; data[[&#39;TV&#39;, &#39;Radio&#39;, &#39;Newspaper&#39;]]</span><br><span class="line">y &#x3D; data[&#39;Sales&#39;]</span><br></pre></td></tr></table></figure><h4 id="绘图查看数据"><a href="#绘图查看数据" class="headerlink" title="绘图查看数据"></a>绘图查看数据</h4><p>绘制三张图,分别是每种媒体平台投放量x与销售额y的散点图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize&#x3D;(4, 6))</span><br><span class="line"># 子图, 子图规格, 三位[1, 3]的整数,</span><br><span class="line"># 第一个数表示共有几个子图,</span><br><span class="line"># 第二个数表示每行显示几个子图,</span><br><span class="line"># 第三个数表示现在绘制的是第几个子图</span><br><span class="line">plt.subplot(311)</span><br><span class="line"></span><br><span class="line">plt.plot(data[&#39;TV&#39;], y, &#39;ro&#39;)</span><br><span class="line">plt.title(&#39;TV&#39;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.subplot(312)</span><br><span class="line">plt.plot(data[&#39;Radio&#39;], y, &#39;g^&#39;)</span><br><span class="line">plt.title(&#39;Radio&#39;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.subplot(313)</span><br><span class="line">plt.plot(data[&#39;Newspaper&#39;], y, &#39;b*&#39;)</span><br><span class="line">plt.title(&#39;Newspaper&#39;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.tight_layout()  # 自动调整子图参数</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/ml/advertising-1.png" alt="image"></p><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>这个问题属于回归问题,所以选择线性回归,能用简单模型解决的问题,就不需要复杂的模型,复杂模型会增加不确定性,增加成本,且容易过拟合  </p><p>样本数据共有200条,设置<code>test_size=0.25</code>的意思是从样本中取50条数据为测试数据,150条数据为训练数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test &#x3D; train_test_split(x, y, test_size&#x3D;0.25, random_state&#x3D;1)</span><br><span class="line">linearReg &#x3D; LinearRegression()</span><br><span class="line">model &#x3D; linearReg.fit(x_train, y_train)</span><br><span class="line">print(model)</span><br><span class="line">print(linearReg.coef_)  # 估计系数</span><br><span class="line">print(linearReg.intercept_)  # 独立项&#x2F;截距</span><br></pre></td></tr></table></figure><h4 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_hat &#x3D; linearReg.predict(x_test)</span><br><span class="line">mse &#x3D; np.average((y_hat - np.array(y_test)) ** 2)  # 均方误差</span><br><span class="line">rmse &#x3D; np.sqrt(mse)  # 均方根误差</span><br><span class="line">print(mse, rmse)</span><br></pre></td></tr></table></figure><h5 id="绘图对比预测结果与真实值的差异"><a href="#绘图对比预测结果与真实值的差异" class="headerlink" title="绘图对比预测结果与真实值的差异"></a>绘图对比预测结果与真实值的差异</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t &#x3D; np.arange(len(x_test))</span><br><span class="line">plt.plot(t, y_test, &#39;r-&#39;, linewidth&#x3D;2, label&#x3D;&#39;Test&#39;)  # 红色折线图</span><br><span class="line">plt.plot(t, y_hat, &#39;g-&#39;, linewidth&#x3D;2, label&#x3D;&#39;Predict&#39;)  # 绿色折线图</span><br><span class="line">plt.legend(loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/ml/advertising-2.png" alt="image"></p><h4 id="使用交叉验证的方式建立Lasso和Ridge模型-源码"><a href="#使用交叉验证的方式建立Lasso和Ridge模型-源码" class="headerlink" title="使用交叉验证的方式建立Lasso和Ridge模型(源码)"></a>使用交叉验证的方式建立Lasso和Ridge模型(<a href="https://note.youdao.com/" target="_blank" rel="noopener">源码</a>)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test &#x3D; train_test_split(x, y, random_state&#x3D;1)</span><br><span class="line"># model &#x3D; Lasso()</span><br><span class="line">model &#x3D; Ridge()</span><br><span class="line"></span><br><span class="line">alpha &#x3D; np.logspace(-3, 2, 10)  # 取一个等差数列, 数列中的数作为超参数</span><br><span class="line">liner_model &#x3D; GridSearchCV(model, param_grid&#x3D;&#123;&#39;alpha&#39;: alpha&#125;, cv&#x3D;5)  # 训练数据分成5份做交叉验证</span><br><span class="line">liner_model.fit(x_train, y_train)</span><br><span class="line">print(&quot;超参数:&quot;, liner_model.best_params_)</span><br><span class="line"></span><br><span class="line">y_hat &#x3D; liner_model.predict(np.array(x_test))</span><br><span class="line">mse &#x3D; np.average((y_hat - np.array(y_test)) ** 2)</span><br><span class="line">rmse &#x3D; np.sqrt(mse)</span><br><span class="line">print(&quot;均方误差&quot;, mse, &quot;均方根误差&quot;, rmse)</span><br><span class="line">print(&quot;分数&quot;, liner_model.score(x_test, y_test))</span><br><span class="line">print(liner_model.best_score_)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/advertising-3.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降求解逻辑回归</title>
      <link href="/2018-03-03-ml-gradient-descent.html"/>
      <url>/2018-03-03-ml-gradient-descent.html</url>
      
        <content type="html"><![CDATA[<p>建立一个逻辑回归模型,根据已有的录取学生的考试成绩数据,预测一个学生是否能被录取.<br><a href="https://github.com/waterandair/daily-learning/tree/master/learn-ml/regression" target="_blank" rel="noopener"><strong>查看源码</strong></a></p><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><h4 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h4><p>每一列代表的意义分别是 <code>IQ</code>,<code>EQ</code>, <code>是否录取</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">34.62365962451697,78.0246928153624,0</span><br><span class="line">30.28671076822607,43.89499752400101,0</span><br><span class="line">35.84740876993872,72.90219802708364,0</span><br><span class="line">60.18259938620976,86.30855209546826,1</span><br><span class="line">79.0327360507101,75.3443764369103,1</span><br><span class="line">45.08327747668339,56.3163717815305,0</span><br><span class="line">61.10666453684766,96.51142588489624,1</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="python-库准备"><a href="#python-库准备" class="headerlink" title="python 库准备"></a>python 库准备</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import os</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># matplotlib 中文</span><br><span class="line">mpl.rcParams[u&#39;font.sans-serif&#39;] &#x3D; [&#39;SimHei&#39;]</span><br><span class="line">mpl.rcParams[&#39;axes.unicode_minus&#39;] &#x3D; False</span><br><span class="line"></span><br><span class="line"># pandas 加载数据</span><br><span class="line">data_path &#x3D; os.path.dirname(os.path.realpath(__file__)) + &#39;&#x2F;data&#x2F;LogiReg_data.txt&#39;</span><br></pre></td></tr></table></figure><h3 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3><p>sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，它可以把任何一个值转换为一个0到1之间的数,是一个非常良好的阈值函数。<br>在逻辑回归算法中,可以使用 Sigmoid 函数把目标值映射到一个 (0, 1) 的阈值区间,然后可以根据自己设定的阈值实现样本分类.</p><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p>$$<br>g(z) = \frac{1}{1+e^{-z}}<br>$$</p><h4 id="python-实现"><a href="#python-实现" class="headerlink" title="python 实现"></a>python 实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    映射到概率的函数, 可以把一个值映射到从0到1的一个值</span><br><span class="line">    :param z:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return 1 &#x2F; (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line">def show_sigmoid_demo():</span><br><span class="line">    nums &#x3D; np.arange(-10, 10)</span><br><span class="line">    fig, ax &#x3D; plt.subplots(figsize&#x3D;(12, 4))</span><br><span class="line">    ax.plot(nums, sigmoid(nums))</span><br><span class="line">    ax.set_title(&quot;sigmoid 函数&quot;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    show_sigmoid_demo()</span><br></pre></td></tr></table></figure><p><img src="/images/ml/sigmoid-demo.png" alt="image"></p><h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><p>回归模型是为一组样本找到一组最优参数,以此得出最拟合实际的模型.<br>对于非线性回归,不能直接求出最拟合的参数,所以采用梯度下降的算法为模型求出一组最合适的参数,以此确定模型.</p><p>$$<br>\begin{array}{ccc}<br>\begin{pmatrix}\theta_{0} &amp; \theta_{1} &amp; \theta_{2}\end{pmatrix} &amp; \times &amp;<br>\begin{matrix}\left\lgroup<br>\matrix{1 \cr x_{1} \cr x_{2}}\right\rgroup<br>\end{matrix}<br>\end{array}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}<br>$$</p><h4 id="python-实现-1"><a href="#python-实现-1" class="headerlink" title="python 实现"></a>python 实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def model(X, theta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    返回预测结果值</span><br><span class="line">    :param X: 样本</span><br><span class="line">    :param theta: 参数</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return sigmoid(np.dot(X, theta.T))</span><br></pre></td></tr></table></figure><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数是用来度量拟合的程度的, 损失函数越小，就代表模型拟合的越好。损失函数的期望成为平均损失(经验风险),<br>实际的模型并不是越拟合越好,过拟合的模型预测时效果会很不好.<br>将对数似然函数去负号:<br>$$<br>D(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))<br>$$<br>平均损失:<br>$$<br>J(\theta)=\frac{1}{n}\sum_{i=1}^{n} D(h_\theta(x_i), y_i)<br>$$</p><h4 id="python-实现-2"><a href="#python-实现-2" class="headerlink" title="python 实现"></a>python 实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def cost(X, y, theta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    损失函数(代价函数) 根据参数计算损失,损失越小,拟合越好</span><br><span class="line">    :param X: 样本</span><br><span class="line">    :param y: 目标值</span><br><span class="line">    :param theta: 参数</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    left &#x3D; np.multiply(-y, np.log(model(X, theta)))</span><br><span class="line">    right &#x3D; np.multiply(1 - y, np.log(1 - model(X, theta)))</span><br><span class="line">    return np.sum(left - right) &#x2F; (len(X))</span><br></pre></td></tr></table></figure><h3 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h3><p>$$<br>\frac{\partial J}{\partial \theta_j}=-\frac{1}{m}\sum_{i=1}^n (y_i - h_\theta (x_i))x_{ij}<br>$$</p><h4 id="python-实现-3"><a href="#python-实现-3" class="headerlink" title="python 实现"></a>python 实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def gradient(X, y, theta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算每个参数的梯度方向</span><br><span class="line">    :param X: 样本</span><br><span class="line">    :param y: 目标值</span><br><span class="line">    :param theta: 参数</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    grad &#x3D; np.zeros(theta.shape)</span><br><span class="line">    error &#x3D; (model(X, theta) - y).ravel()  # 计算误差</span><br><span class="line">    for j in range(len(theta.ravel())):</span><br><span class="line">        term &#x3D; np.multiply(error, X[:, j])</span><br><span class="line">        grad[0, j] &#x3D; np.sum(term) &#x2F; len(X)  # 计算每项的梯度</span><br><span class="line"></span><br><span class="line">    return grad</span><br></pre></td></tr></table></figure><h3 id="梯度下降求解"><a href="#梯度下降求解" class="headerlink" title="梯度下降求解"></a>梯度下降求解</h3><h4 id="三种不同的停止迭代策略"><a href="#三种不同的停止迭代策略" class="headerlink" title="三种不同的停止迭代策略"></a>三种不同的停止迭代策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">STOP_ITER &#x3D; 0  # 根据迭代次数停止迭代</span><br><span class="line">STOP_COST &#x3D; 1  # 根据损失值的变化停止迭代,如果两次迭代损失值变化很小很小,就停止迭代</span><br><span class="line">STOP_GRAD &#x3D; 2  # 根据梯度,如果梯度变化很小很小,就停止迭代</span><br><span class="line"></span><br><span class="line">def stopCriterion(type, value, threshold):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    设定三种不同的停止策略</span><br><span class="line">    :param type:</span><br><span class="line">    :param value:</span><br><span class="line">    :param threshold:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if type &#x3D;&#x3D; STOP_ITER:</span><br><span class="line">        return value &gt; threshold</span><br><span class="line">    elif type &#x3D;&#x3D; STOP_COST:</span><br><span class="line">        return abs(value[-1]-value[-2]) &lt; threshold</span><br><span class="line">    elif type &#x3D;&#x3D; STOP_GRAD:</span><br><span class="line">        return np.linalg.norm(value) &lt; threshold</span><br></pre></td></tr></table></figure><h4 id="打乱数据"><a href="#打乱数据" class="headerlink" title="打乱数据"></a>打乱数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def shuffleData(data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    重组数据</span><br><span class="line">    :param data:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    np.random.shuffle(data)</span><br><span class="line">    cols &#x3D; data.shape[1]</span><br><span class="line">    X &#x3D; data[:, 0:cols-1]</span><br><span class="line">    y &#x3D; data[:, cols-1:]</span><br><span class="line">    return X, y</span><br></pre></td></tr></table></figure><h4 id="求梯度下降"><a href="#求梯度下降" class="headerlink" title="求梯度下降"></a>求梯度下降</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def descent(data, theta, batchSize, stopType, thresh, alpha, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    梯度下降求解,计算参数更新</span><br><span class="line">    :param data: 样本数据</span><br><span class="line">    :param theta: 参数</span><br><span class="line">    :param batchSize: 每次迭代计算的样本数量</span><br><span class="line">    :param stopType: 停止策略</span><br><span class="line">    :param thresh: 停止策略对应的阈值</span><br><span class="line">    :param alpha: 学习率</span><br><span class="line">    :param n: 样本总数量</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    init_time &#x3D; time.time()</span><br><span class="line">    # 初始化</span><br><span class="line">    i &#x3D; 0  # 迭代次数</span><br><span class="line">    k &#x3D; 0  # 每次迭代计算的样本数量</span><br><span class="line">    X, y &#x3D; shuffleData(data)</span><br><span class="line">    grad &#x3D; np.zeros(theta.shape)  # 计算的梯度</span><br><span class="line">    costs &#x3D; [cost(X, y, theta)]  # 损失值</span><br><span class="line"></span><br><span class="line">    while True:</span><br><span class="line">        grad &#x3D; gradient(X[k:k + batchSize], y[k:k + batchSize], theta)</span><br><span class="line">        k +&#x3D; batchSize  # 取batch数量个数据</span><br><span class="line">        if k &gt;&#x3D; n:</span><br><span class="line">            k &#x3D; 0</span><br><span class="line">            X, y &#x3D; shuffleData(data)  # 重新洗牌</span><br><span class="line">        theta &#x3D; theta - alpha * grad  # 参数更新</span><br><span class="line">        costs.append(cost(X, y, theta))  # 计算新的损失</span><br><span class="line">        i +&#x3D; 1</span><br><span class="line"></span><br><span class="line">        if stopType &#x3D;&#x3D; STOP_ITER:</span><br><span class="line">            value &#x3D; i</span><br><span class="line">        elif stopType &#x3D;&#x3D; STOP_COST:</span><br><span class="line">            value &#x3D; costs</span><br><span class="line">        elif stopType &#x3D;&#x3D; STOP_GRAD:</span><br><span class="line">            value &#x3D; grad</span><br><span class="line">        if stopCriterion(stopType, value, thresh):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    return theta, i - 1, costs, grad, time.time() - init_time</span><br></pre></td></tr></table></figure><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>使用梯度下降计算出一组参数后,就可以使用这个模型对测试数据进行预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def predict(X,theta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    返回预测值</span><br><span class="line">    :param X: 测试样本</span><br><span class="line">    :param theta: 梯度下降计算得到的参数</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return [1 if x &gt; 0.5 else 0 for x in model(X, theta)]</span><br></pre></td></tr></table></figure><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><h4 id="封装运行函数"><a href="#封装运行函数" class="headerlink" title="封装运行函数"></a>封装运行函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def runExpe(data, theta, batchSize, stopType, thresh, alpha, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    :param data: 样本</span><br><span class="line">    :param theta: 参数</span><br><span class="line">    :param batchSize: 每次迭代要计算的样本数量,根据这个值,可以分为三种不同的梯度下降算法</span><br><span class="line">                      batchSize &#x3D;&#x3D; n(样本总量): 批量梯度下降,每次迭代计算所有样本,速度慢,精度高</span><br><span class="line">                      batchSize &#x3D;&#x3D; 1(样本总量): 随机梯度下降,每次迭代计算一个样本,速度快,精度低</span><br><span class="line">                      batchSize &#x3D;&#x3D; m(部分样本): 随机梯度下降,每次迭代计算一部分样本,兼顾速度和精度</span><br><span class="line">    :param stopType: 三种停止策略</span><br><span class="line">                      STOP_ITER &#x3D;&#x3D; 0:  根据迭代次数停止迭代</span><br><span class="line">                      STOP_COST &#x3D;&#x3D; 1:  根据损失值的变化停止迭代,如果两次迭代损失值变化很小很小,就停止迭代</span><br><span class="line">                      STOP_GRAD &#x3D;&#x3D; 2:  根据梯度,如果梯度变化很小很小,就停止迭代</span><br><span class="line">    :param thresh: 针对不同停止策略的阈值</span><br><span class="line">    :param alpha: 学习率</span><br><span class="line">    :param n: 样本重量</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    theta, iter, costs, grad, dur &#x3D; descent(data, theta, batchSize, stopType, thresh, alpha, n)</span><br><span class="line"></span><br><span class="line">    title &#x3D; &quot;原始数据&quot; if (data[:, 1] &gt; 2).sum() &gt; 1 else &quot;标准化数据&quot;</span><br><span class="line">    if batchSize &#x3D;&#x3D; n:</span><br><span class="line">        strDescType &#x3D; &quot;批量梯度下降&quot;</span><br><span class="line">    elif batchSize &#x3D;&#x3D; 1:</span><br><span class="line">        strDescType &#x3D; &quot;随机梯度下降&quot;</span><br><span class="line">    else:</span><br><span class="line">        strDescType &#x3D; &quot;小批量梯度下降 (&#123;&#125;)&quot;.format(batchSize)</span><br><span class="line"></span><br><span class="line">    title +&#x3D; strDescType + &quot; 学习率: &#123;&#125; &quot;.format(alpha)</span><br><span class="line">    title +&#x3D; &quot; 迭代停止策略: &quot;</span><br><span class="line">    if stopType &#x3D;&#x3D; STOP_ITER:</span><br><span class="line">        strStop &#x3D; &quot;迭代次数 &#x3D; &#123;&#125;&quot;.format(thresh)</span><br><span class="line">    elif stopType &#x3D;&#x3D; STOP_COST:</span><br><span class="line">        strStop &#x3D; &quot;损失变化 &lt; &#123;&#125;&quot;.format(thresh)</span><br><span class="line">    else:</span><br><span class="line">        strStop &#x3D; &quot;梯度变化 &lt; &#123;&#125;&quot;.format(thresh)</span><br><span class="line">    title +&#x3D; strStop</span><br><span class="line">    print(&quot;***&#123;&#125;\nTheta: &#123;&#125; - Iter: &#123;&#125; - Last cost: &#123;:03.2f&#125; - Duration: &#123;:03.2f&#125;s&quot;.format(</span><br><span class="line">        title, theta, iter, costs[-1], dur))</span><br><span class="line"></span><br><span class="line">    fig, ax &#x3D; plt.subplots(figsize&#x3D;(12, 4))</span><br><span class="line">    ax.plot(np.arange(len(costs)), costs, &#39;r&#39;)</span><br><span class="line">    ax.set_xlabel(&#39;迭代次数&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;损失&#39;)</span><br><span class="line">    ax.set_title(title)</span><br><span class="line">    plt.show()</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure><h4 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # 查看 sigmoid 函数曲线</span><br><span class="line">    show_sigmoid_demo()</span><br><span class="line">    student_data &#x3D; pd.read_csv(data_path, header&#x3D;None, names&#x3D;[&#39;iq&#39;, &#39;eq&#39;, &#39;admitted&#39;])</span><br><span class="line">    # show_plt(student_data)</span><br><span class="line">    # 添加一列偏置项系数, 设置为1</span><br><span class="line">    student_data.insert(0, &#39;ones&#39;, 1)</span><br><span class="line">    # 设置原始样本X和目标值y</span><br><span class="line">    orig_data &#x3D; student_data.as_matrix()  # 把DataFrame转为矩阵,方便计算</span><br><span class="line">    cols &#x3D; orig_data.shape[1]</span><br><span class="line">    X &#x3D; orig_data[:, 0:cols-1]</span><br><span class="line">    y &#x3D; orig_data[:, cols-1:]</span><br><span class="line">    # 给参数设置一个初始值</span><br><span class="line">    theta &#x3D; np.zeros([1, 3]</span><br></pre></td></tr></table></figure><h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h4><h5 id="根据迭代次数停止-STOP-ITER"><a href="#根据迭代次数停止-STOP-ITER" class="headerlink" title="根据迭代次数停止(STOP_ITER)"></a>根据迭代次数停止(STOP_ITER)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 根据迭代次数停止,设置阈值 5000, 迭代5000次</span><br><span class="line">res &#x3D; runExpe(orig_data, theta, 100, STOP_ITER, 5000, 0.000001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/batch_descent_stop_iter.png" alt="image"></p><h5 id="根据损失值变化停止-STOP-COST"><a href="#根据损失值变化停止-STOP-COST" class="headerlink" title="根据损失值变化停止(STOP_COST)"></a>根据损失值变化停止(STOP_COST)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(orig_data, theta, 100, STOP_COST, 0.000001, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/batch_descent_stop_cost.png" alt="image"></p><h5 id="根据梯度变化停止-STOP-GRAD"><a href="#根据梯度变化停止-STOP-GRAD" class="headerlink" title="根据梯度变化停止(STOP_GRAD)"></a>根据梯度变化停止(STOP_GRAD)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(orig_data, theta, 100, STOP_GRAD, 0.05, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/batch_descent_stop_grad.png" alt="image"></p><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(orig_data, theta, 1, STOP_ITER, 5000, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/random_descent_stop_iter.png" alt="image"></p><h4 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h4><h5 id="根据迭代次数停止-STOP-ITER-1"><a href="#根据迭代次数停止-STOP-ITER-1" class="headerlink" title="根据迭代次数停止(STOP_ITER)"></a>根据迭代次数停止(STOP_ITER)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 根据迭代次数停止,设置阈值 5000, 迭代5000次</span><br><span class="line">res &#x3D; runExpe(orig_data, theta, 15, STOP_ITER, 5000, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/part_batch_descent_stop_iter.png" alt="image"></p><h4 id="小批量梯度下降-标准化数据"><a href="#小批量梯度下降-标准化数据" class="headerlink" title="小批量梯度下降(标准化数据)"></a>小批量梯度下降(标准化数据)</h4><h5 id="数据标准化-数据预处理"><a href="#数据标准化-数据预处理" class="headerlink" title="数据标准化(数据预处理)"></a>数据标准化(数据预处理)</h5><p>将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing as pp</span><br><span class="line">scaled_data &#x3D; orig_data.copy()</span><br><span class="line">scaled_data[:, 1:3] &#x3D; pp.scale(orig_data[:, 1:3])</span><br></pre></td></tr></table></figure><h5 id="根据迭代次数停止-STOP-ITER-2"><a href="#根据迭代次数停止-STOP-ITER-2" class="headerlink" title="根据迭代次数停止(STOP_ITER)"></a>根据迭代次数停止(STOP_ITER)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(scaled_data, theta, 15, STOP_ITER, 5000, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/scaled_part_batch_descent_stop_iter.png" alt="image"></p><h5 id="根据损失值变化停止-STOP-COST-1"><a href="#根据损失值变化停止-STOP-COST-1" class="headerlink" title="根据损失值变化停止(STOP_COST)"></a>根据损失值变化停止(STOP_COST)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(scaled_data, theta, 15, STOP_COST, 0.000001, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/scaled_part_batch_descent_stop_cost.png" alt="image"></p><h5 id="根据梯度变化停止-STOP-GRAD-1"><a href="#根据梯度变化停止-STOP-GRAD-1" class="headerlink" title="根据梯度变化停止(STOP_GRAD)"></a>根据梯度变化停止(STOP_GRAD)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res &#x3D; runExpe(scaled_data, theta, 15, STOP_GRAD, 0.001, 0.001, 100)</span><br></pre></td></tr></table></figure><p><img src="/images/ml/scaled_part_batch_descent_stop_grad.png" alt="image"></p><h3 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 要被预测的数据</span><br><span class="line">scaled_X &#x3D; scaled_data[:, :3]</span><br><span class="line"># 目标值</span><br><span class="line">y &#x3D; scaled_data[:, 3]</span><br><span class="line"># 传入梯度下降算法得出的参数</span><br><span class="line">predictions &#x3D; predict(scaled_X, res)</span><br><span class="line">correct &#x3D; [1 if a &#x3D;&#x3D; b else 0 for (a, b) in zip(predictions, y)]</span><br><span class="line">accuracy &#x3D; (sum(map(int, correct)) % len(correct))</span><br><span class="line">print(&#39;准确率 &#x3D; &#123;0&#125;%&#39;.format(accuracy))  # 准确率 &#x3D; 89%</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>使用样本训练模型前,应该先对样本进行预处理</li><li>使用小批量梯度下降算法兼顾速度和准确度,推荐使用这种方式训练模型</li><li>训练模型的过程中,需要不断的调整迭代停止策略和对应的阈值以及学习率,直到找到满足需求的参数</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归算法推导</title>
      <link href="/2018-02-26-ml-regression-pic.html"/>
      <url>/2018-02-26-ml-regression-pic.html</url>
      
        <content type="html"><![CDATA[<p>关于线性回归问题的推导</p><h3 id="最小二乘法解回归"><a href="#最小二乘法解回归" class="headerlink" title="最小二乘法解回归"></a>最小二乘法解回归</h3><p><img src="/images/ml/regression-pic-1.png" alt="image"></p><p><img src="/images/ml/regression-pic-2.png" alt="image"></p><p><img src="/images/ml/regression-pic-3.png" alt="image"></p><h3 id="梯度下降解线性回归"><a href="#梯度下降解线性回归" class="headerlink" title="梯度下降解线性回归"></a>梯度下降解线性回归</h3><p><img src="/images/ml/regression-pic-4.png" alt="image"></p><p><img src="/images/ml/regression-pic-5.png" alt="image"></p><h3 id="逻辑回归处理分类任务"><a href="#逻辑回归处理分类任务" class="headerlink" title="逻辑回归处理分类任务"></a>逻辑回归处理分类任务</h3><p><img src="/images/ml/regression-pic-6.png" alt="image"></p><p><img src="/images/ml/regression-pic-7.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pandas 入门练习</title>
      <link href="/2018-02-18-pandas-intro.html"/>
      <url>/2018-02-18-pandas-intro.html</url>
      
        <content type="html"><![CDATA[<p>pandas 是基于 Numpy 构建的含有更高级数据结构和工具的数据分析包</p><a id="more"></a><h3 id="pandas引入规则"><a href="#pandas引入规则" class="headerlink" title="pandas引入规则"></a>pandas引入规则</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from pandas import Series, DataFrame</span><br><span class="line">import pandas as pd</span><br></pre></td></tr></table></figure><h3 id="pandas-数据结构"><a href="#pandas-数据结构" class="headerlink" title="pandas 数据结构"></a>pandas 数据结构</h3><h4 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h4><p>一种类似于一维数组的对象,它是由一组数据(各种Numpy数据类型)以及一组与之相关的数据标签(即索引)组成,仅由一组数据即可产生简单的 Series</p><h5 id="通过一维数组创建-Series"><a href="#通过一维数组创建-Series" class="headerlink" title="通过一维数组创建 Series"></a>通过一维数组创建 Series</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from pandas import Series, DataFrame</span><br><span class="line"></span><br><span class="line">arr &#x3D; np.array([1, 2, 3, 4])</span><br><span class="line">series &#x3D; Series(arr)</span><br><span class="line"># 0    1</span><br><span class="line"># 1    2</span><br><span class="line"># 2    3</span><br><span class="line"># 3    4</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">series2 &#x3D; Series([4, 3, 2, 1])</span><br><span class="line"># 0    4</span><br><span class="line"># 1    3</span><br><span class="line"># 2    2</span><br><span class="line"># 3    1</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line"># 如上所示,第一列为索引,最后一行为元素数据类型</span><br><span class="line"></span><br><span class="line">series.index</span><br><span class="line"># RangeIndex(start&#x3D;0, stop&#x3D;4, step&#x3D;1)</span><br><span class="line">series.values</span><br><span class="line"># array([1, 2, 3, 4])</span><br><span class="line">series.dtype</span><br><span class="line"># dtype(&#39;int64&#39;)</span><br><span class="line"></span><br><span class="line"># 通过数组创建时,如果没有为数据指定索引,则会自动创建一个从 0 到 N-1(N 为数据的长度)的整数索引,默认索引可以通过赋值方式进行修改</span><br><span class="line"></span><br><span class="line">series3 &#x3D; Series([45, 66, 88, 99], index&#x3D;[&#39;语文&#39;, &#39;数学&#39;, &#39;英语&#39;, &#39;体育&#39;])</span><br><span class="line"># 语文    45</span><br><span class="line"># 数学    66</span><br><span class="line"># 英语    88</span><br><span class="line"># 体育    99</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">series3.index</span><br><span class="line"># Index([&#39;语文&#39;, &#39;数学&#39;, &#39;英语&#39;, &#39;体育&#39;], dtype&#x3D;&#39;object&#39;)</span><br><span class="line"></span><br><span class="line">series3.values</span><br><span class="line">#  array([45, 66, 88, 99])</span><br></pre></td></tr></table></figure><h5 id="通过字典的方式创建-Series"><a href="#通过字典的方式创建-Series" class="headerlink" title="通过字典的方式创建 Series"></a>通过字典的方式创建 Series</h5><p>Series 可以被看成是一个定长的有序字典, 是索引值到数据值的一个映射,因此可以直接通过字典来创建 Series</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d &#x3D; &#123;&#39;A&#39;: 1, &#39;B&#39;: 2, &#39;C&#39;: 3, &#39;D&#39;: 4&#125;</span><br><span class="line">series4 &#x3D; Series(d)</span><br><span class="line">series4.index</span><br><span class="line"># Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype&#x3D;&#39;object&#39;)</span><br><span class="line">series4.values</span><br><span class="line"># array([1, 2, 3, 4])</span><br></pre></td></tr></table></figure><h5 id="Series-应用-Numpy-数组运算"><a href="#Series-应用-Numpy-数组运算" class="headerlink" title="Series 应用 Numpy 数组运算"></a>Series 应用 Numpy 数组运算</h5><p>numpy 中的数组运算,在 Series 中都保留使用,并且 Series 进行数组运算时,索引与值之间的映射关系不会改变</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">d &#x3D; &#123;&#39;A&#39;: 1, &#39;B&#39;: 2, &#39;C&#39;: 3, &#39;D&#39;: 4&#125;</span><br><span class="line">series &#x3D; Series(d)</span><br><span class="line"></span><br><span class="line">series[series &gt; 2]</span><br><span class="line"># C    3</span><br><span class="line"># D    4</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">series &#x2F; 2</span><br><span class="line"># A    0.5</span><br><span class="line"># B    1.0</span><br><span class="line"># C    1.5</span><br><span class="line"># D    2.0</span><br><span class="line"># dtype: float64</span><br><span class="line"></span><br><span class="line">np.power(series, 2)</span><br><span class="line"># A     1</span><br><span class="line"># B     4</span><br><span class="line"># C     9</span><br><span class="line"># D    16</span><br><span class="line"># dtype: int64</span><br></pre></td></tr></table></figure><h5 id="Series-缺失值检测"><a href="#Series-缺失值检测" class="headerlink" title="Series 缺失值检测"></a>Series 缺失值检测</h5><ul><li>NaN 在 pandas 中用于表示一个缺失的值</li><li>pandas 中的 isnull 和 notnull 函数可用于 Series 缺失值检测</li><li>isnull 和 not null 都返回一个布尔类型的 Series </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">scores &#x3D; Series(&#123;&quot;kobe&quot;: 92, &quot;wade&quot;: 90, &quot;ai&quot;: 90, &quot;jordan&quot;: 100&#125;)</span><br><span class="line">new_index &#x3D; [&#39;kobe&#39;, &#39;wade&#39;, &quot;james&quot;, &quot;ai&quot;, &#39;jordan&#39;]</span><br><span class="line">scores &#x3D; Series(scores, index&#x3D;new_index)</span><br><span class="line"># kobe       92.0</span><br><span class="line"># wade       90.0</span><br><span class="line"># james       NaN</span><br><span class="line"># ai         90.0</span><br><span class="line"># jordan    100.0</span><br><span class="line"># dtype: float64</span><br><span class="line"></span><br><span class="line">pd.isnull(scores)</span><br><span class="line"># kobe      False</span><br><span class="line"># wade      False</span><br><span class="line"># james      True</span><br><span class="line"># ai        False</span><br><span class="line"># jordan    False</span><br><span class="line"># dtype: bool</span><br><span class="line"></span><br><span class="line">pd.notnull(scores)</span><br><span class="line"># kobe       True</span><br><span class="line"># wade       True</span><br><span class="line"># james     False</span><br><span class="line"># ai         True</span><br><span class="line"># jordan     True</span><br><span class="line"># dtype: bool</span><br><span class="line"></span><br><span class="line"># 过滤出为缺失的项</span><br><span class="line">scores[pd.isnull(scores)]</span><br><span class="line"># james   NaN</span><br><span class="line"># dtype: float64</span><br></pre></td></tr></table></figure><h5 id="Series-自动对齐"><a href="#Series-自动对齐" class="headerlink" title="Series 自动对齐"></a>Series 自动对齐</h5><p>不同 Series 之间进行算术运算, 会自动对齐不同索引的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">product_num &#x3D; Series([23, 45, 67, 89], index&#x3D;[&#39;p1&#39;, &#39;p3&#39;, &#39;p4&#39;, &#39;p2&#39;])</span><br><span class="line">product_price_table &#x3D; Series([1, 2, 3, 4, 5], index&#x3D;[&#39;p2&#39;, &#39;p5&#39;, &#39;p3&#39;, &#39;p1&#39;, &#39;p4&#39;])</span><br><span class="line">product_sum &#x3D; product_num * product_price_table</span><br><span class="line"># p1     92.0</span><br><span class="line"># p2     89.0</span><br><span class="line"># p3    135.0</span><br><span class="line"># p4    335.0</span><br><span class="line"># p5      NaN</span><br><span class="line"># dtype: float64</span><br></pre></td></tr></table></figure><h5 id="Series-及其索引的-name-属性"><a href="#Series-及其索引的-name-属性" class="headerlink" title="Series 及其索引的 name 属性"></a>Series 及其索引的 name 属性</h5><p>Series 对象本身及其索引都有一个 <code>name</code> 属性, 可赋值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">product_num &#x3D; Series([23, 45, 67, 89], index&#x3D;[&#39;p1&#39;, &#39;p3&#39;, &#39;p4&#39;, &#39;p2&#39;])</span><br><span class="line">product_num.name &#x3D; &#39;ProductNums&#39;</span><br><span class="line">product_num.index.name &#x3D; &#39;ProductType&#39;</span><br><span class="line"># ProductType</span><br><span class="line"># p1    23</span><br><span class="line"># p3    45</span><br><span class="line"># p4    67</span><br><span class="line"># p2    89</span><br><span class="line"># Name: ProductNums, dtype: int64</span><br></pre></td></tr></table></figure><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>一个表格型的数据结构,含有一组<code>有序</code>的列, 每列可以是不同的值类型(数值,字符串,布尔值等), DataFrame 既有行索引,也有列索引,可以被看做是由 Series 组成的字典</p><h5 id="通过二维数组创建-DataFrame"><a href="#通过二维数组创建-DataFrame" class="headerlink" title="通过二维数组创建 DataFrame"></a>通过二维数组创建 DataFrame</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df1 &#x3D; DataFrame([[&#39;kobe&#39;, &#39;ai&#39;, &#39;wade&#39;], [81, 58, 55]])</span><br><span class="line">#     0   1     2     列索引</span><br><span class="line"># 0  kobe  ai  wade</span><br><span class="line"># 1    81  58    55</span><br><span class="line">  </span><br><span class="line">  行</span><br><span class="line">  索</span><br><span class="line">  引</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">df2 &#x3D; DataFrame([[&#39;kobe&#39;, 68], [&#39;ai&#39;, 58], [&#39;wade&#39;, &#39;55&#39;]])</span><br><span class="line">#       0   1  </span><br><span class="line"># 0  kobe  68</span><br><span class="line"># 1    ai  58</span><br><span class="line"># 2  wade  55</span><br><span class="line"></span><br><span class="line"># 自定义行索引(index), 列索引(columns)</span><br><span class="line">arr &#x3D; np.array([</span><br><span class="line">    [&#39;kobe&#39;, 68], </span><br><span class="line">    [&#39;ai&#39;, 58],</span><br><span class="line">    [&#39;wade&#39;, &#39;55&#39;]</span><br><span class="line">    ])</span><br><span class="line">df3 &#x3D; DataFrame(arr, index&#x3D;[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;], columns&#x3D;[&#39;name&#39;, &#39;point&#39;])</span><br><span class="line">#        name point</span><br><span class="line"># one    kobe    68</span><br><span class="line"># two      ai    58</span><br><span class="line"># three  wade    55</span><br></pre></td></tr></table></figure><h5 id="通过字典的方式创建-DataFrame"><a href="#通过字典的方式创建-DataFrame" class="headerlink" title="通过字典的方式创建 DataFrame"></a>通过字典的方式创建 DataFrame</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; &#123;</span><br><span class="line">    &quot;apart&quot;: [&#39;101&#39;, &#39;102&#39;, &#39;103&#39;, &#39;104&#39;],</span><br><span class="line">    &#39;profits&#39;: [567, 789, 456, 678],</span><br><span class="line">    &#39;year&#39;:[2016, 2017, 2018, 2017]</span><br><span class="line">&#125;</span><br><span class="line">df &#x3D; DataFrame(data)</span><br><span class="line">#   apart  profits  year</span><br><span class="line"># 0   101      567  2016</span><br><span class="line"># 1   102      789  2017</span><br><span class="line"># 2   103      456  2018</span><br><span class="line"># 3   104      678  2017</span><br><span class="line"></span><br><span class="line">df.index</span><br><span class="line"># RangeIndex(start&#x3D;0, stop&#x3D;4, step&#x3D;1)</span><br><span class="line"></span><br><span class="line">df.columns</span><br><span class="line"># Index([&#39;apart&#39;, &#39;profits&#39;, &#39;year&#39;], dtype&#x3D;&#39;object&#39;)</span><br><span class="line"></span><br><span class="line">df.values</span><br><span class="line"># array([[&#39;101&#39;, 567, 2016],</span><br><span class="line">#        [&#39;102&#39;, 789, 2017],</span><br><span class="line">#        [&#39;103&#39;, 456, 2018],</span><br><span class="line">#        [&#39;104&#39;, 678, 2017]], dtype&#x3D;object)</span><br></pre></td></tr></table></figure><h5 id="索引对象"><a href="#索引对象" class="headerlink" title="索引对象"></a>索引对象</h5><ul><li>不管是 Series 对象还是 DataFrame 对象,都有索引对象</li><li>索引对象负责管理轴标签和其他元数据(比如轴名称)</li><li>通过索引可以从 Series, DataFrame 中取值或对某个位置的值重新赋值</li><li>Series 或者 DataFrame 自动化对齐功能就是通过索引进行的</li></ul><h6 id="通过索引从-DataFrame-中取值"><a href="#通过索引从-DataFrame-中取值" class="headerlink" title="通过索引从 DataFrame 中取值"></a>通过索引从 DataFrame 中取值</h6><ul><li>可以直接通过类索引获取指定列的数据</li><li>要通过行索引获取指定行数据需要 loc 方法</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; &#123;</span><br><span class="line">    &quot;apart&quot;: [&#39;101&#39;, &#39;102&#39;],</span><br><span class="line">    &#39;profits&#39;: [567, 789],</span><br><span class="line">    &#39;year&#39;:[2016, 2017]</span><br><span class="line">&#125;</span><br><span class="line">df &#x3D; DataFrame(data)</span><br><span class="line">#   apart  profits  year</span><br><span class="line"># 0   101      567  2016</span><br><span class="line"># 1   102      789  2017</span><br><span class="line"></span><br><span class="line">df[&#39;year&#39;]</span><br><span class="line"># 0    2016</span><br><span class="line"># 1    2017</span><br><span class="line"># Name: year, dtype: int64</span><br><span class="line"></span><br><span class="line">df.loc[0]</span><br><span class="line"># apart       101</span><br><span class="line"># profits     567</span><br><span class="line"># year       2016</span><br><span class="line"># Name: 0, dtype: object</span><br></pre></td></tr></table></figure><h3 id="pandas-基本功能"><a href="#pandas-基本功能" class="headerlink" title="pandas 基本功能"></a>pandas 基本功能</h3><h4 id="常用的数学和统计方法"><a href="#常用的数学和统计方法" class="headerlink" title="常用的数学和统计方法"></a>常用的数学和统计方法</h4><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>count</td><td>非NA值的数量</td></tr><tr><td>describe</td><td>针对Series或各DataFrame列计算总统计</td></tr><tr><td>min/max</td><td>计算最小值、最大值</td></tr><tr><td>argmin、argmax</td><td>计算能够获取到最小值和最大值的索引位置(整数)</td></tr><tr><td>idxmin、idxmax</td><td>计算能够获取到最小值和最大值的索引值</td></tr><tr><td>quantile</td><td>计算样本的分位数(0到1)</td></tr><tr><td>sum</td><td>值的总和</td></tr><tr><td>mean</td><td>值的平均数</td></tr><tr><td>median</td><td>值的算术中位数 (50%分位数)</td></tr><tr><td>mad</td><td>根据平均值计算平均绝对离差</td></tr><tr><td>var</td><td>样本数值的方差</td></tr><tr><td>std</td><td>样本值的标准差</td></tr><tr><td>cumsum</td><td>样本值的累计和</td></tr><tr><td>cummin、cummax</td><td>样本值的累计最小值、最大值</td></tr><tr><td>cumprod</td><td>样本值的累计积</td></tr><tr><td>Pct_change</td><td>计算百分数变化</td></tr></tbody></table><blockquote><p>对于 DataFrame, 这些通过统计方法, 默认是计算各列上的数据, 如果要应用于各行数据,则增加参数 axis= 1</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">df &#x3D; DataFrame([</span><br><span class="line">    [0, 1, 2, 3],</span><br><span class="line">    [4, 5, 6, 7]</span><br><span class="line">])</span><br><span class="line">df.describe()</span><br><span class="line">#               0         1         2         3</span><br><span class="line"># count  2.000000  2.000000  2.000000  2.000000</span><br><span class="line"># mean   2.000000  3.000000  4.000000  5.000000</span><br><span class="line"># std    2.828427  2.828427  2.828427  2.828427</span><br><span class="line"># min    0.000000  1.000000  2.000000  3.000000</span><br><span class="line"># 25%    1.000000  2.000000  3.000000  4.000000</span><br><span class="line"># 50%    2.000000  3.000000  4.000000  5.000000</span><br><span class="line"># 75%    3.000000  4.000000  5.000000  6.000000</span><br><span class="line"># max    4.000000  5.000000  6.000000  7.000000</span><br><span class="line">#</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df.count()</span><br><span class="line"># 0    2</span><br><span class="line"># 1    2</span><br><span class="line"># 2    2</span><br><span class="line"># 3    2</span><br><span class="line"># dtype: int6</span><br><span class="line"></span><br><span class="line">df.count(axis&#x3D;1)</span><br><span class="line"># 0    4</span><br><span class="line"># 1    4</span><br><span class="line"># dtype: int64</span><br></pre></td></tr></table></figure><h4 id="相关系数与协方差"><a href="#相关系数与协方差" class="headerlink" title="相关系数与协方差"></a>相关系数与协方差</h4><h5 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a><a href="http://wiki.mbalib.com/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE/" target="_blank" rel="noopener">协方差</a></h5><h5 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a><a href="http://wiki.mbalib.com/wiki/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0" target="_blank" rel="noopener">相关系数</a></h5><h4 id="唯一值-值计算以及成员资格"><a href="#唯一值-值计算以及成员资格" class="headerlink" title="唯一值,值计算以及成员资格"></a>唯一值,值计算以及成员资格</h4><ul><li>unique方法用于获取Series唯一值数组</li><li>value_counts方法,用于计算一个Series中各值出现的频率</li><li>isin方法,用于判断矢量化集合的成员资格,可用于选取Series中或者<br>DataFrame中列中数据的子集</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">series &#x3D; Series([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;])</span><br><span class="line">series.unique()</span><br><span class="line"># array([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype&#x3D;object)</span><br><span class="line"></span><br><span class="line">df &#x3D; DataFrame(&#123;</span><br><span class="line">    &#39;id&#39;: [1, 2, 3, 4],</span><br><span class="line">    &#39;point&#39;: [55, 66, 77, 88],</span><br><span class="line">    &#39;team_id&#39;: [1, 2, 1, 1]</span><br><span class="line">&#125;)</span><br><span class="line">df[&#39;team_id&#39;].unique()</span><br><span class="line"># array([1, 2])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 返回结果默认会按值出现频率降序排序</span><br><span class="line">series.value_counts()</span><br><span class="line"># a    3</span><br><span class="line"># c    2</span><br><span class="line"># b    2</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">series.value_counts(ascending&#x3D;True)</span><br><span class="line"># b    2</span><br><span class="line"># c    2</span><br><span class="line"># a    3</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mask &#x3D; series.isin([&#39;b&#39;, &#39;c&#39;])</span><br><span class="line"># 0    False</span><br><span class="line"># 1     True</span><br><span class="line"># 2     True</span><br><span class="line"># 3    False</span><br><span class="line"># 4    False</span><br><span class="line"># 5     True</span><br><span class="line"># 6     True</span><br><span class="line"># dtype: bool</span><br><span class="line"></span><br><span class="line"># 选出值为 &#39;b&#39;, &#39;c&#39; 的项</span><br><span class="line">series[mask]</span><br><span class="line"># 1    b</span><br><span class="line"># 2    c</span><br><span class="line"># 5    b</span><br><span class="line"># 6    c</span><br><span class="line"># dtype: object</span><br></pre></td></tr></table></figure><h3 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h3><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>dropna</td><td>根据标签的值中是否存在缺失数据对轴标签进行过滤(删除),可通过阈值调节对缺失值的容忍度</td></tr><tr><td>findna</td><td>用指定值或插值方法(如ffill或bfill)填充缺失数据</td></tr><tr><td>isnull</td><td>返回一个含有布尔值的对象,这些布尔值表示哪些值是缺失值NA</td></tr><tr><td>notnull</td><td>Isnull的否定式</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">## 缺失值检测</span><br><span class="line">df &#x3D; DataFrame(&#123;</span><br><span class="line">    &#39;id&#39;: [1, 2, np.NaN],</span><br><span class="line">    &#39;point&#39;: [55, np.NaN, 88],</span><br><span class="line">    &#39;team_id&#39;: [1, 2, np.NaN]</span><br><span class="line">&#125;)</span><br><span class="line">df.isnull()</span><br><span class="line">#       id  point  team_id</span><br><span class="line"># 0  False  False    False</span><br><span class="line"># 1  False   True    False</span><br><span class="line"># 2   True  False     True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 过滤缺失数据</span><br><span class="line">series &#x3D; Series([1, 2, np.NaN, 4])</span><br><span class="line"># 0    1.0</span><br><span class="line"># 1    2.0</span><br><span class="line"># 3    4.0</span><br><span class="line"># dtype: float64</span><br><span class="line"></span><br><span class="line">df &#x3D; DataFrame([</span><br><span class="line">    [1, 2],</span><br><span class="line">    [np.NaN, np.NaN],</span><br><span class="line">    [4, np.NaN]</span><br><span class="line">])</span><br><span class="line"># 默认丢弃所有包含缺失值的行</span><br><span class="line">df.dropna()</span><br><span class="line">#      0    1</span><br><span class="line"># 0  1.0  2.0</span><br><span class="line"># 丢弃全部为缺失值的行</span><br><span class="line">df.dropna(how&#x3D;&#39;all&#39;)</span><br><span class="line">#     0    1</span><br><span class="line"># 0  1.0  2.0</span><br><span class="line"># 2  4.0  NaN</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 填充缺失数据</span><br><span class="line">df.fillna(0)</span><br><span class="line">#      0    1</span><br><span class="line"># 0  1.0  2.0</span><br><span class="line"># 1  0.0  0.0</span><br><span class="line"># 2  4.0  0.0</span><br></pre></td></tr></table></figure><h3 id="层次化索引"><a href="#层次化索引" class="headerlink" title="层次化索引"></a>层次化索引</h3><ul><li>在某个方向上拥有多个(两个及两个以上)索引级别</li><li>通过层次化索引,pandas能够以低维度形式处理高维度数据</li><li>通过层次化索引,可以按层级统计数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">series &#x3D; Series([40, 20 , 24, 32], index&#x3D;[[&#39;West&#39;, &#39;West&#39;, &#39;East&#39;, &#39;East&#39;],[&#39;Westbrook&#39;, &#39;kuzma&#39;, &#39;Brown&#39;, &#39;James&#39;]] )</span><br><span class="line"># West  Westbrook    40</span><br><span class="line">#       kuzma        20</span><br><span class="line"># East  Brown        24</span><br><span class="line">#       James        32</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">series.index.names &#x3D; [&#39;区域&#39;, &#39;姓名&#39;]</span><br><span class="line">series</span><br><span class="line"># 区域    姓名       </span><br><span class="line"># West  Westbrook    40</span><br><span class="line">#       kuzma        20</span><br><span class="line"># East  Brown        24</span><br><span class="line">#       James        32</span><br><span class="line"># dtype: int64</span><br><span class="line"></span><br><span class="line">df &#x3D; DataFrame(&#123;</span><br><span class="line">    &#39;Date&#39;: [&#39;03&#39;, &#39;03&#39;, &#39;04&#39;, &#39;04&#39;],</span><br><span class="line">    &#39;Team&#39;: [&#39;Lakers&#39;, &#39;Thunder&#39;, &#39;Lakers&#39; ,&#39;Thunder&#39;],</span><br><span class="line">    &#39;PTS&#39;: [115, 123, 92, 112],</span><br><span class="line">    &#39;REB&#39;: [42, 36, 52, 32]</span><br><span class="line">&#125;)</span><br><span class="line">#   Date  PTS  REB     Team</span><br><span class="line"># 0   03  115   42   Lakers</span><br><span class="line"># 1   03  123   36  Thunder</span><br><span class="line"># 2   04   92   52   Lakers</span><br><span class="line"># 3   04  112   32  Thunder</span><br><span class="line"></span><br><span class="line">df2 &#x3D; df.set_index([&#39;Date&#39;, &#39;Team&#39;])</span><br><span class="line">#               PTS  REB</span><br><span class="line"># Date Team             </span><br><span class="line"># 03   Lakers   115   42</span><br><span class="line">#      Thunder  123   36</span><br><span class="line"># 04   Lakers    92   52</span><br><span class="line">#      Thunder  112   32</span><br><span class="line"></span><br><span class="line">df2.index</span><br><span class="line"># MultiIndex(levels&#x3D;[[&#39;03&#39;, &#39;04&#39;], [&#39;Lakers&#39;, &#39;Thunder&#39;]],</span><br><span class="line">#            labels&#x3D;[[0, 0, 1, 1], [0, 1, 0, 1]],</span><br><span class="line">#            names&#x3D;[&#39;Date&#39;, &#39;Team&#39;])</span><br><span class="line"></span><br><span class="line"># 按层级统计</span><br><span class="line">df2.sum(level &#x3D; &#39;Date&#39;)</span><br><span class="line">#       PTS  REB</span><br><span class="line"># Date          </span><br><span class="line"># 03    238   78</span><br><span class="line"># 04    204   84</span><br><span class="line"></span><br><span class="line">df2.sum(level &#x3D; &#39;Team&#39;)</span><br><span class="line">#          PTS  REB</span><br><span class="line"># Team             </span><br><span class="line"># Lakers   207   94</span><br><span class="line"># Thunder  235   68</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS 系统基础介绍及使用(python)</title>
      <link href="/2018-02-16-hadoop-hdfs-base.html"/>
      <url>/2018-02-16-hadoop-hdfs-base.html</url>
      
        <content type="html"><![CDATA[<p>HDFS 是一个分布式的文件系统，它由三部分组成：NameNode(管理元数据)、DataNode(存储文件数据块)、Secondary NameNode(维护快照)。本文介绍HDFS的原理及基本使用。</p><a id="more"></a><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h4 id="HDFS-写数据流程"><a href="#HDFS-写数据流程" class="headerlink" title="HDFS 写数据流程"></a>HDFS 写数据流程</h4><ul><li>客户端向 namenode 发出上传文件的请求，namenode 检查目标文件是否已存在，父目录是否存在，向客户端返回是否可以上传。</li><li>如果可以上传，客户端向 namenode 询问第一个 block 上传到哪几个datanode服务器。</li><li>namenode 返回 n 个datanode节点，假设为 dn1、dn2、dn3。</li><li>客户端向 dn1 请求上传 block，dn1 收到请求会向 dn2 发起请求，然后 dn2 向dn3 发起请求，将这个通信管道建立完成</li><li>dn1、dn2、dn3 逐级应答客户端</li><li>客户端开始向 dn1 上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位，dn1收到一个packet就会传给dn2，dn2传给dn3，dn1每传一个 packet 会放入一个应答队列等待应答</li><li>当一个 block 传输完成之后，客户端再次向 namenode 请求上传第二个 block 的服务器， 然后重复之前的操作直到所有 block 上传完成。</li></ul><h4 id="HDFS-读数据流程"><a href="#HDFS-读数据流程" class="headerlink" title="HDFS 读数据流程"></a>HDFS 读数据流程</h4><ul><li>客户端向 namenode 请求下载文件，namenode 通过查询元数据，找到文件块所在的 datanode 地址。</li><li>挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。</li><li>datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。</li><li>客户端以 packet 为单位接收，先在本地缓存，然后写入目标文件。</li></ul><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><h5 id="namenode-工作流程"><a href="#namenode-工作流程" class="headerlink" title="namenode 工作流程"></a>namenode 工作流程</h5><ul><li>如果是第一次启动， namenode 进行格式化并创建 fsimage 和 edits 文件。如果不是第一次启动，直接加载镜像文件(fsimage)和编辑日志(edits)到内存。</li><li>客户端对元数据进行增删改的请求</li><li>namenode记录操作日志，更新滚动日志。</li><li>namenode在内存中对数据进行增删改查</li></ul><h5 id="镜像文件和编辑日志"><a href="#镜像文件和编辑日志" class="headerlink" title="镜像文件和编辑日志"></a>镜像文件和编辑日志</h5><p>namenode 被格式化之后，将在配置项<code>hadoop.tmp.dir</code>指定的目录中产生如下文件:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">name</span><br><span class="line">├── current</span><br><span class="line">│   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">│   ├── edits_0000000000000000003-0000000000000000004</span><br><span class="line">│   ├── edits_inprogress_0000000000000000005</span><br><span class="line">│   ├── fsimage_0000000000000000002</span><br><span class="line">│   ├── fsimage_0000000000000000002.md5</span><br><span class="line">│   ├── fsimage_0000000000000000004</span><br><span class="line">│   ├── fsimage_0000000000000000004.md5</span><br><span class="line">│   ├── seen_txid</span><br><span class="line">│   └── VERSION</span><br><span class="line">└── in_use.lock</span><br></pre></td></tr></table></figure><ul><li>fsimage 文件: 元数据镜像文件，包含所有目录和inode的序列化信息</li><li>edits 文件: 编辑日志，记录所有写操作</li><li>seen_txid: 保存一个数字，是最后一个 edits 文件的序号</li></ul><p>namenode 第一次启动会进行格式化，创建 fsimage 和 edits 等文件，之后每一次启动都会先 fsimage 文件读入内存，并依次执行 edits 文件中的更新操作，保证了内存中元数据信息是最新的同步的。  </p><p>查看镜像文件: <code>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</code><br>查看编辑日志: <code>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</code>  </p><p>fsimage 和 edits 文件合并的过程又称为滚动日志，可以在启动HDFS集群后使用命令 <code>hdfs dfsadmin -rollEdits</code> 进行手动滚动日志。</p><h5 id="namenode-版本号"><a href="#namenode-版本号" class="headerlink" title="namenode 版本号"></a>namenode 版本号</h5><p>在配置项 <code>hadoop.tmp.dir</code> 指定的目录中的 ./name/currrent 目录中，还有一个 <code>VERSION</code> 文件，记录了 namenode 的版本号，内容形如：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">namespaceID&#x3D;1443494756  # HDFS 集群中 namenode 的唯一标识</span><br><span class="line">clusterID&#x3D;CID-0e913bcb-85ea-41cf-a758-4151dc74b7db  # 集群 id</span><br><span class="line">cTime&#x3D;1533941647598  # namenode 的创建时间</span><br><span class="line">storageType&#x3D;NAME_NODE  # 说明该存储目录包含的是namenode的数据结构</span><br><span class="line">blockpoolID&#x3D;BP-1989204408-127.0.1.1-1533941647598  # 一个 block pool 的唯一标识</span><br><span class="line">layoutVersion&#x3D;-64</span><br></pre></td></tr></table></figure><h5 id="namenode-多目录配置"><a href="#namenode-多目录配置" class="headerlink" title="namenode 多目录配置"></a>namenode 多目录配置</h5><p>namenode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-site.xml]</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;$&#123;hadoop.tmp.dir&#125;&#x2F;dfs&#x2F;name1,file:&#x2F;&#x2F;&#x2F;$&#123;hadoop.tmp.dir&#125;&#x2F;dfs&#x2F;name2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h4 id="SecondaryNameNode"><a href="#SecondaryNameNode" class="headerlink" title="SecondaryNameNode"></a>SecondaryNameNode</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p>Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。<br>在配置项 <code>hadoop.tmp.dir</code> 指定的目录中有如下文件,类似与 namenode 的本地目录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">namesecondary&#x2F;</span><br><span class="line">├── current</span><br><span class="line">│   ├── edits_0000000000000000001-0000000000000000002</span><br><span class="line">│   ├── edits_0000000000000000003-0000000000000000004</span><br><span class="line">│   ├── fsimage_0000000000000000002</span><br><span class="line">│   ├── fsimage_0000000000000000002.md5</span><br><span class="line">│   ├── fsimage_0000000000000000004</span><br><span class="line">│   ├── fsimage_0000000000000000004.md5</span><br><span class="line">│   └── VERSION</span><br><span class="line">└── in_use.lock</span><br></pre></td></tr></table></figure><p>在主namenode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode恢复数据。</p><h5 id="Secondary-NameNode-工作流程"><a href="#Secondary-NameNode-工作流程" class="headerlink" title="Secondary NameNode 工作流程"></a>Secondary NameNode 工作流程</h5><ul><li>Secondary NameNode 询问 namenode 是否需要 checkpoint</li><li>Secondary NameNode 请求执行 checkpoint。</li><li>namenode 滚动正在写的 edits 日志</li><li>将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode</li><li>Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件fsimage.chkpoint</li><li>拷贝 fsimage.chkpoint 到 namenode</li><li>namenode 将 fsimage.chkpoint 重新命名成fsimage</li></ul><p>chkpoint检查时间参数设置:</p><ul><li>SecondaryNameNode 每隔一小时执行一次。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-default.xml]</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li><li>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode 执行一次。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-default.xml]</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;1000000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;description&gt;操作动作次数&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;60&lt;&#x2F;value&gt;</span><br><span class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><h5 id="DataNode-工作流程"><a href="#DataNode-工作流程" class="headerlink" title="DataNode 工作流程"></a>DataNode 工作流程</h5><ul><li>一个数据块在 datanode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li><li>DataNode 启动后向 namenode 注册，通过后周期性（1小时）的向 namenode 上报所有的块信息。</li><li>心跳是每3秒一次，心跳返回结果带有 namenode 给该 datanode 的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个 datanode 的心跳，则认为该节点不可用。</li><li>集群运行中可以安全加入和退出一些机器</li></ul><h5 id="DataNode-本地目录"><a href="#DataNode-本地目录" class="headerlink" title="DataNode 本地目录"></a>DataNode 本地目录</h5><p>在配置项 <code>hadoop.tmp.dir</code> 指定的目录中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── current</span><br><span class="line">│   ├── BP-1989204408-127.0.1.1-1533941647598</span><br><span class="line">│   │   ├── current</span><br><span class="line">│   │   │   ├── finalized</span><br><span class="line">│   │   │   ├── rbw</span><br><span class="line">│   │   │   └── VERSION</span><br><span class="line">│   │   ├── scanner.cursor</span><br><span class="line">│   │   └── tmp</span><br><span class="line">│   └── VERSION</span><br><span class="line">└── in_use.lock</span><br></pre></td></tr></table></figure><h6 id="data-current-下的版本号"><a href="#data-current-下的版本号" class="headerlink" title="./data/current 下的版本号"></a><code>./data/current</code> 下的版本号</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">storageID&#x3D;DS-1b998a1d-71a3-43d5-82dc-c0ff3294921b  # 存储id号</span><br><span class="line">clusterID&#x3D;CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175  # 集群id</span><br><span class="line">cTime&#x3D;0  # datanode存储系统的创建时间</span><br><span class="line">datanodeUuid&#x3D;970b2daf-63b8-4e17-a514-d81741392165  # datanode的唯一标识</span><br><span class="line">storageType&#x3D;DATA_NODE  # 存储类型</span><br><span class="line">layoutVersion&#x3D;-56</span><br></pre></td></tr></table></figure><h6 id="data-current-BP-1989204408-127-0-1-1-1533941647598-下的版本号"><a href="#data-current-BP-1989204408-127-0-1-1-1533941647598-下的版本号" class="headerlink" title="./data/current/BP-1989204408-127.0.1.1-1533941647598 下的版本号"></a><code>./data/current/BP-1989204408-127.0.1.1-1533941647598</code> 下的版本号</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">namespaceID&#x3D;1933630176  # datanode 从 namenode 处获取的 storageID 对每个 datanode 来说是唯一的，namenode 用这个属性来区分不同 datanode。</span><br><span class="line">cTime&#x3D;0  # datanode存储系统的创建时间</span><br><span class="line">blockpoolID&#x3D;BP-97847618-192.168.10.102-1493726072779  # block pool id标识一个block pool</span><br><span class="line">layoutVersion&#x3D;-56</span><br></pre></td></tr></table></figure><h5 id="数据完成性检验"><a href="#数据完成性检验" class="headerlink" title="数据完成性检验"></a>数据完成性检验</h5><ul><li>DataNode 读取 block 的时计算 checksum</li><li>如果计算后的 checksum，与 block 创建时的值不一样，说明 block 已经损坏。</li><li>client 读取其他 DataNode 上的 block.</li><li>datanode 在其文件创建后周期验证 checksum</li></ul><h5 id="宕机超时参数"><a href="#宕机超时参数" class="headerlink" title="宕机超时参数"></a>宕机超时参数</h5><p>datanode 进程和所在节点挂掉后，namenode 不会立即认为改节点失效，而是经过要给超时时间后才认为 datanode 宕机。  </p><p>超时时间计算方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">timeout  &#x3D; 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。</span><br><span class="line"></span><br><span class="line"># dfs.namenode.heartbeat.recheck-interval 默认为 5 分钟</span><br><span class="line"># dfs.heartbeat.interval 默认为 3 秒</span><br><span class="line"></span><br><span class="line">这两个参数在 hdfs-site.xml 中配置:</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;300000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt; dfs.heartbeat.interval &lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h5 id="增加新节点"><a href="#增加新节点" class="headerlink" title="增加新节点"></a>增加新节点</h5><p>假设已存在 <code>big-data-01</code>, <code>big-data-02</code>, <code>big-data-03</code> 三个 datanode 节点，现在加入一个新节点: <code>big-data-04</code> </p><h6 id="创建-dfs-hosts-文件"><a href="#创建-dfs-hosts-文件" class="headerlink" title="创建 dfs.hosts 文件"></a>创建 dfs.hosts 文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">big-data-01</span><br><span class="line">big-data-02</span><br><span class="line">big-data-03</span><br><span class="line">big-data-04</span><br></pre></td></tr></table></figure><h6 id="在-namenode-的-hdfs-site-xml-配置文件中增加-dfs-hosts-属性"><a href="#在-namenode-的-hdfs-site-xml-配置文件中增加-dfs-hosts-属性" class="headerlink" title="在 namenode 的 hdfs-site.xml 配置文件中增加 dfs.hosts 属性"></a>在 namenode 的 hdfs-site.xml 配置文件中增加 dfs.hosts 属性</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.hosts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;dfs.hosts&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h6 id="刷新-namenode和resourcemanager"><a href="#刷新-namenode和resourcemanager" class="headerlink" title="刷新 namenode和resourcemanager"></a>刷新 namenode和resourcemanager</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hdfs dfsadmin -refreshNodes</span><br><span class="line">shell&gt; yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><h6 id="在-namenode-的-slaves-文件中增加新主机名称"><a href="#在-namenode-的-slaves-文件中增加新主机名称" class="headerlink" title="在 namenode 的 slaves 文件中增加新主机名称"></a>在 namenode 的 slaves 文件中增加新主机名称</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">big-data-01</span><br><span class="line">big-data-02</span><br><span class="line">big-data-03</span><br><span class="line">big-data-04</span><br></pre></td></tr></table></figure><h6 id="单独命令启动新的节点"><a href="#单独命令启动新的节点" class="headerlink" title="单独命令启动新的节点"></a>单独命令启动新的节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; sbin&#x2F;hadoop-daemon.sh start datanode</span><br><span class="line">shell&gt; sbin&#x2F;yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><h6 id="解决数据不均衡"><a href="#解决数据不均衡" class="headerlink" title="解决数据不均衡"></a>解决数据不均衡</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; .&#x2F;sbin&#x2F;start-balancer.sh</span><br></pre></td></tr></table></figure><h5 id="删除一个节点"><a href="#删除一个节点" class="headerlink" title="删除一个节点"></a>删除一个节点</h5><p>已有节点  <code>big-data-01</code>, <code>big-data-02</code>, <code>big-data-03</code> ,<code>big-data-04</code> 4 个节点，删除节点<code>big-data-04</code></p><h6 id="创建-dfs-hosts-exclude-文件"><a href="#创建-dfs-hosts-exclude-文件" class="headerlink" title="创建 dfs.hosts.exclude 文件"></a>创建 dfs.hosts.exclude 文件</h6><p>在 namenode 节点 hadoop 的配置文件目录创建 <code>dfs.hosts.exclude</code> 文件，添加要删除的节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">big-data-04</span><br></pre></td></tr></table></figure><h6 id="在-namenode-的h-dfs-site-xml-配置文件中增加-dfs-hosts-exclude-属性"><a href="#在-namenode-的h-dfs-site-xml-配置文件中增加-dfs-hosts-exclude-属性" class="headerlink" title="在 namenode 的h dfs-site.xml 配置文件中增加 dfs.hosts.exclude 属性"></a>在 namenode 的h dfs-site.xml 配置文件中增加 <code>dfs.hosts.exclude</code> 属性</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.hosts.exclude&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;dfs.hosts.exclude&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h6 id="刷新-namenode和resourcemanager-1"><a href="#刷新-namenode和resourcemanager-1" class="headerlink" title="刷新 namenode和resourcemanager"></a>刷新 namenode和resourcemanager</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hdfs dfsadmin -refreshNodes</span><br><span class="line">shell&gt; yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><h6 id="复制数据到其他节点"><a href="#复制数据到其他节点" class="headerlink" title="复制数据到其他节点"></a>复制数据到其他节点</h6><p>web 端查看，如果操作正确，被删除的节点状态为 <code>decommission in progress</code>, 此时该节点正在复制块到其他节点。  </p><p>复制完成后，该节点的状态更新为 <code>decommissioned</code>，停止datanode和nodemanager:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin&#x2F;hadoop-daemon.sh stop datanode</span><br><span class="line">sbin&#x2F;yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure><p>注意: 如果当前运行的节点不能小于副本数，否则不能删除节点，需要修改副本数后才能退役 </p><h6 id="从-include-文件中删除退役节点，再运行刷新节点的命令"><a href="#从-include-文件中删除退役节点，再运行刷新节点的命令" class="headerlink" title="从 include 文件中删除退役节点，再运行刷新节点的命令"></a>从 include 文件中删除退役节点，再运行刷新节点的命令</h6><p>更新 <code>dfs.hosts</code> 文件，删除 <code>big-data-04</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">big-data-01</span><br><span class="line">big-data-02</span><br><span class="line">big-data-03</span><br></pre></td></tr></table></figure><p>刷新 namenode 和 resourcemanager:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><h6 id="从-namenode-的-slave-文件中删除退役节点"><a href="#从-namenode-的-slave-文件中删除退役节点" class="headerlink" title="从 namenode 的 slave 文件中删除退役节点"></a>从 namenode 的 slave 文件中删除退役节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">big-data-01</span><br><span class="line">big-data-02</span><br><span class="line">big-data-03</span><br></pre></td></tr></table></figure><h6 id="解决数据不均衡-1"><a href="#解决数据不均衡-1" class="headerlink" title="解决数据不均衡"></a>解决数据不均衡</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; .&#x2F;sbin&#x2F;start-balancer.sh</span><br></pre></td></tr></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="常用命令行操作-bin-hadoop-fs-具体命令-or-bin-hdfs-dfs-具体命令"><a href="#常用命令行操作-bin-hadoop-fs-具体命令-or-bin-hdfs-dfs-具体命令" class="headerlink" title="常用命令行操作 bin/hadoop fs 具体命令 or bin/hdfs dfs 具体命令"></a>常用命令行操作 <code>bin/hadoop fs 具体命令</code> or <code>bin/hdfs dfs 具体命令</code></h4><h5 id="help-输出这个命令参数"><a href="#help-输出这个命令参数" class="headerlink" title="-help 输出这个命令参数"></a><code>-help</code> 输出这个命令参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -help rm</span><br></pre></td></tr></table></figure><h5 id="ls-显示目录信息"><a href="#ls-显示目录信息" class="headerlink" title="-ls 显示目录信息"></a><code>-ls</code> 显示目录信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -ls &#x2F;</span><br></pre></td></tr></table></figure><h5 id="mkdir-在hdfs上创建目录"><a href="#mkdir-在hdfs上创建目录" class="headerlink" title="-mkdir 在hdfs上创建目录"></a><code>-mkdir</code> 在hdfs上创建目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -mkdir -p &#x2F;nba&#x2F;lakers</span><br></pre></td></tr></table></figure><h5 id="moveFromLocal-从本地剪切粘贴到hdfs"><a href="#moveFromLocal-从本地剪切粘贴到hdfs" class="headerlink" title="-moveFromLocal 从本地剪切粘贴到hdfs"></a><code>-moveFromLocal</code> 从本地剪切粘贴到hdfs</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 会删除本地磁盘文件</span><br><span class="line">zj@pc:~$ hadoop fs -moveFromLocal &#x2F;home&#x2F;zj&#x2F;kobe.txt &#x2F;nba&#x2F;lakers</span><br><span class="line"></span><br><span class="line"># &#96;-moveToLocal&#96; 从hdfs剪切粘贴到本地（尚未实现）</span><br></pre></td></tr></table></figure><h5 id="cat-显示文件内容"><a href="#cat-显示文件内容" class="headerlink" title="-cat 显示文件内容"></a><code>-cat</code> 显示文件内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -cat &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br><span class="line">24</span><br></pre></td></tr></table></figure><h5 id="appendToFile-追加一个文件到已经存在的文件末尾"><a href="#appendToFile-追加一个文件到已经存在的文件末尾" class="headerlink" title="--appendToFile 追加一个文件到已经存在的文件末尾"></a><code>--appendToFile</code> 追加一个文件到已经存在的文件末尾</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ cat kobe2.txt </span><br><span class="line">8</span><br><span class="line">zj@pc:~$ hadoop fs -appendToFile &#x2F;home&#x2F;zj&#x2F;kobe2.txt &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br><span class="line">zj@pc:~$ hadoop fs -cat &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br><span class="line">24</span><br><span class="line">8</span><br></pre></td></tr></table></figure><h5 id="tail-显示一个文件的末尾"><a href="#tail-显示一个文件的末尾" class="headerlink" title="-tail 显示一个文件的末尾"></a><code>-tail</code> 显示一个文件的末尾</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -tail &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br><span class="line">24</span><br><span class="line">8</span><br></pre></td></tr></table></figure><h5 id="chgrp-、-chmod、-chown-修改文件所属权限-linux文件系统中的用法一样"><a href="#chgrp-、-chmod、-chown-修改文件所属权限-linux文件系统中的用法一样" class="headerlink" title="-chgrp 、-chmod、-chown 修改文件所属权限(linux文件系统中的用法一样)"></a><code>-chgrp 、-chmod、-chown</code> 修改文件所属权限(linux文件系统中的用法一样)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -chown zj:zj &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br><span class="line">zj@pc:~$ hadoop fs -chmod 666 &#x2F;nba&#x2F;lakers&#x2F;kobe.txt</span><br></pre></td></tr></table></figure><h5 id="copyFromLocal-从本地文件系统中拷贝文件到hdfs路径去-等同于-put"><a href="#copyFromLocal-从本地文件系统中拷贝文件到hdfs路径去-等同于-put" class="headerlink" title="-copyFromLocal 从本地文件系统中拷贝文件到hdfs路径去(等同于-put)"></a><code>-copyFromLocal</code> 从本地文件系统中拷贝文件到hdfs路径去(等同于<code>-put</code>)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -copyFromLocal .&#x2F;james.txt &#x2F;nba&#x2F;lakers</span><br></pre></td></tr></table></figure><h5 id="copyToLocal-从hdfs拷贝到本地-等同于-get"><a href="#copyToLocal-从hdfs拷贝到本地-等同于-get" class="headerlink" title="-copyToLocal 从hdfs拷贝到本地(等同于 -get)"></a><code>-copyToLocal</code> 从hdfs拷贝到本地(等同于 <code>-get</code>)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -copyToLocal &#x2F;nba&#x2F;lakers&#x2F;james.txt .&#x2F;james2.txt</span><br></pre></td></tr></table></figure><h5 id="cp-从hdfs的一个路径拷贝到hdfs的另一个路径"><a href="#cp-从hdfs的一个路径拷贝到hdfs的另一个路径" class="headerlink" title="-cp 从hdfs的一个路径拷贝到hdfs的另一个路径"></a><code>-cp</code> 从hdfs的一个路径拷贝到hdfs的另一个路径</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -mkdir &#x2F;nba&#x2F;heat</span><br><span class="line">zj@pc:~$ hadoop fs -cp &#x2F;nba&#x2F;lakers&#x2F;james.txt &#x2F;nba&#x2F;heat</span><br></pre></td></tr></table></figure><h5 id="mv-在hdfs目录中移动文件-修改名称"><a href="#mv-在hdfs目录中移动文件-修改名称" class="headerlink" title="-mv 在hdfs目录中移动文件(修改名称)"></a><code>-mv</code> 在hdfs目录中移动文件(修改名称)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop  fs  -mv  &#x2F;nba&#x2F;heat  &#x2F;nba&#x2F;MIA</span><br><span class="line">zj@pc:~$ hadoop  fs  -mv  &#x2F;nba&#x2F;lakers  &#x2F;nba&#x2F;LAL</span><br></pre></td></tr></table></figure><h5 id="getmerge-合并下载多个文件，所有文件内容都集中到一个文件中"><a href="#getmerge-合并下载多个文件，所有文件内容都集中到一个文件中" class="headerlink" title="-getmerge 合并下载多个文件，所有文件内容都集中到一个文件中"></a><code>-getmerge</code> 合并下载多个文件，所有文件内容都集中到一个文件中</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -getmerge &#x2F;nba&#x2F;LAL&#x2F;* .&#x2F;lakers.txt</span><br><span class="line">zj@pc:~$ cat lakers.txt </span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">8</span><br></pre></td></tr></table></figure><h5 id="rm-删除文件或文件夹"><a href="#rm-删除文件或文件夹" class="headerlink" title="-rm 删除文件或文件夹"></a><code>-rm</code> 删除文件或文件夹</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -rm -r &#x2F;nba&#x2F;MIA&#x2F;james.txt</span><br><span class="line">Deleted &#x2F;nba&#x2F;MIA&#x2F;james.txt</span><br></pre></td></tr></table></figure><h5 id="rmdir-删除空目录"><a href="#rmdir-删除空目录" class="headerlink" title="-rmdir 删除空目录"></a><code>-rmdir</code> 删除空目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -rmdir &#x2F;nba&#x2F;MIA&#x2F;</span><br></pre></td></tr></table></figure><h5 id="df-统计文件系统的可用空间信息"><a href="#df-统计文件系统的可用空间信息" class="headerlink" title="-df 统计文件系统的可用空间信息"></a><code>-df</code> 统计文件系统的可用空间信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -df -h &#x2F;</span><br></pre></td></tr></table></figure><h5 id="du-统计文件夹的大小信息"><a href="#du-统计文件夹的大小信息" class="headerlink" title="-du 统计文件夹的大小信息"></a><code>-du</code> 统计文件夹的大小信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -du -s -h &#x2F;</span><br><span class="line">127.5 K  127.5 K  &#x2F;</span><br><span class="line">zj@pc:~$ hadoop fs -du  -h &#x2F;</span><br><span class="line">127.4 K  127.4 K  &#x2F;home</span><br><span class="line">57       57       &#x2F;nba</span><br><span class="line">0        0        &#x2F;tmp</span><br><span class="line">0        0        &#x2F;user</span><br></pre></td></tr></table></figure><h5 id="count-统计一个指定目录下的文件节点数量"><a href="#count-统计一个指定目录下的文件节点数量" class="headerlink" title="-count 统计一个指定目录下的文件节点数量"></a><code>-count</code> 统计一个指定目录下的文件节点数量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -count &#x2F;</span><br><span class="line">          18           14             130512 &#x2F;</span><br><span class="line">    嵌套文件层级  包含文件的总数</span><br></pre></td></tr></table></figure><h5 id="setrep-设置hdfs中文件的副本数量"><a href="#setrep-设置hdfs中文件的副本数量" class="headerlink" title="-setrep 设置hdfs中文件的副本数量"></a><code>-setrep</code> 设置hdfs中文件的副本数量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@pc:~$ hadoop fs -setrep 3 &#x2F;nba&#x2F;LAL&#x2F;kobe.txt</span><br><span class="line">Replication 3 set: &#x2F;nba&#x2F;LAL&#x2F;kobe.txt</span><br></pre></td></tr></table></figure><h4 id="python-客户端操作"><a href="#python-客户端操作" class="headerlink" title="python 客户端操作"></a>python 客户端操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">from hdfs import InsecureClient</span><br><span class="line">from hdfs.client import Client</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 读取hdfs文件内容,将每行存入数组返回</span><br><span class="line">def read_hdfs_file(client, filename):</span><br><span class="line">    lines &#x3D; []</span><br><span class="line">    with client.read(filename, encoding&#x3D;&#39;utf-8&#39;, delimiter&#x3D;&#39;\n&#39;) as reader:</span><br><span class="line">        for line in reader:</span><br><span class="line">            lines.append(line.strip())</span><br><span class="line">    return lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def write_hdfs_file(client, filename, content):</span><br><span class="line">    with client.write(filename) as writer:</span><br><span class="line">        writer.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    client &#x3D; InsecureClient(&#39;http:&#x2F;&#x2F;localhost:9870&#39;, user&#x3D;&#39;zj&#39;)</span><br><span class="line">    # 读取文件内容</span><br><span class="line">    # print(read_hdfs_file(client, &quot;&#x2F;nba&#x2F;LAL&#x2F;kobe.txt&quot;))</span><br><span class="line"></span><br><span class="line">    # 写文件，不可以有重复文件</span><br><span class="line">    # write_hdfs_file(client, &quot;&#x2F;nba&#x2F;LAL&#x2F;kobe2.txt&quot;, b&quot;content&quot;)</span><br><span class="line"></span><br><span class="line">    # 追加数据到hdfs文件</span><br><span class="line">    # client.write(&quot;&#x2F;nba&#x2F;LAL&#x2F;kobe.txt&quot;, &quot;append&quot;, overwrite&#x3D;False, append&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # 覆盖数据写到hdfs文件</span><br><span class="line">    # client.write(&quot;&#x2F;nba&#x2F;LAL&#x2F;kobe.txt&quot;, &quot;overwrite&quot;, overwrite&#x3D;True, append&#x3D;False)</span><br><span class="line"></span><br><span class="line">    # 获取目录下所有文件和文件夹</span><br><span class="line">    # print(client.list(&quot;&#x2F;nba&#x2F;LAL&quot;))</span><br><span class="line"></span><br><span class="line">    # 获取文件或文件夹的元信息</span><br><span class="line">    # print(client.content(&quot;&#x2F;nba&#x2F;LAL&quot;))</span><br><span class="line"></span><br><span class="line">    # 获取文件或文件夹的元信息</span><br><span class="line">    # print(client.status(&quot;&#x2F;nba&#x2F;LAL&quot;))</span><br><span class="line"></span><br><span class="line">    # 移动或修改文件或文件夹</span><br><span class="line">    # client.rename(&quot;&#x2F;nba&quot;, &quot;&#x2F;NBA&quot;)</span><br><span class="line"></span><br><span class="line">    # 创建目录</span><br><span class="line">    # client.makedirs(&quot;&#x2F;new&#x2F;dir&quot;)</span><br><span class="line"></span><br><span class="line">    # 删除文件或目录</span><br><span class="line">    # client.delete(&#39;&#x2F;NBA&#39;, recursive&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # 上传文件</span><br><span class="line">    # client.upload(&quot;&#x2F;new&quot;, &quot;&#x2F;etc&#x2F;hosts&quot;)</span><br><span class="line"></span><br><span class="line">    # 下载文件或文件夹到本地</span><br><span class="line">    # client.download(&#39;&#x2F;NBA&#39;, &#39;&#x2F;home&#x2F;zj&#x2F;&#39;, n_threads&#x3D;5)</span><br><span class="line"></span><br><span class="line">    # 获取文件夹下所有的目录以及目录下的文件， 可以指定深度</span><br><span class="line">    # files &#x3D; client.walk(&quot;&#x2F;&quot;, 3)</span><br><span class="line">    # for dpath, _, fnames in files:</span><br><span class="line">    #     print(dpath, fnames)</span><br><span class="line"></span><br><span class="line">    # 使用 content() 或 status() 判断文件是否存在, 加入参数 strict&#x3D;False，如果问价不存在返回 None</span><br><span class="line">    print(client.content(&quot;&#x2F;not&#x2F;exist&#x2F;file&quot;, strict&#x3D;False))  # None</span><br><span class="line">    print(client.status(&quot;&#x2F;not&#x2F;exist&#x2F;file&quot;, strict&#x3D;False))  # None</span><br></pre></td></tr></table></figure><h3 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能"></a>其他功能</h3><h4 id="集群间数据拷贝"><a href="#集群间数据拷贝" class="headerlink" title="集群间数据拷贝"></a>集群间数据拷贝</h4><p>linux 中的 <code>scp</code> 命令可以在主机之间复制数据:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 向主机发送文件</span><br><span class="line">scp -r test.txt root@host:&#x2F;path&#x2F;test.txt  </span><br><span class="line"># 向主机拉取文件</span><br><span class="line">scp -r root@host:&#x2F;path&#x2F;test.txt test.txt</span><br><span class="line"># 两个远程主机之间复制文件</span><br><span class="line">scp -r root@host1:&#x2F;path&#x2F;test.txt root@host2:&#x2F;path&#x2F;test.txt</span><br></pre></td></tr></table></figure><p>hadoop 也提供了相似的功能用于实现 hadoop 集群之间的递归数据复制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hadoop distcp hdfs:&#x2F;&#x2F;host1:9000&#x2F;path&#x2F;test.txt hdfs:&#x2F;&#x2F;host2:9000&#x2F;path&#x2F;test.txt</span><br></pre></td></tr></table></figure><h4 id="hadoop-存档"><a href="#hadoop-存档" class="headerlink" title="hadoop 存档"></a>hadoop 存档</h4><p>Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时允许对文件进行透明的访问。Hadoop存档文件可以用作MapReduce的输入。</p><p>将目录 <code>/path/data</code> 下的文件归档成一个 <code>data.har</code> 文件存入<code>/path/har</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 启动 yarn 进程</span><br><span class="line">shell&gt; start-yarn.sh</span><br><span class="line"># 归档文件:</span><br><span class="line">shell&gt; bin&#x2F;hadoop archive -archiveName data.har -p &#x2F;path&#x2F;data &#x2F;path&#x2F;har</span><br><span class="line"># 查看归档</span><br><span class="line">shell&gt; hadoop fs -ls -R &#x2F;path&#x2F;har&#x2F;data.har</span><br><span class="line">shell&gt; hadoop fs -ls -R har:&#x2F;&#x2F;&#x2F;path&#x2F;har&#x2F;data.har</span><br><span class="line"># 接归档文件</span><br><span class="line">shell&gt; hadoop fs -cp har:&#x2F;&#x2F;&#x2F;path&#x2F;har&#x2F;data.har&#x2F;* &#x2F;path&#x2F;data</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy 入门练习</title>
      <link href="/2018-02-10-numpy-intro.html"/>
      <url>/2018-02-10-numpy-intro.html</url>
      
        <content type="html"><![CDATA[<p>NumPy 是一个 Python 包。 它代表 “Numeric Python”。 它是一个由多维数组对象和用于处理数组的例程集合组成的库。</p><a id="more"></a><h3 id="数据类型-ndarray"><a href="#数据类型-ndarray" class="headerlink" title="数据类型 ndarray"></a>数据类型 ndarray</h3><ul><li>全称 N-dimensiona array, N维数组, ndarray 是一个具有矢量算术运算和复杂广播能力的快速且节省空间的<code>多维数据</code>.  </li><li>ndarray 是一种由相同元素组成的多维数组,<code>元素数量是事先指定好的</code></li><li>元素的类型由 dtype 对象来指定,每个 ndarray 只有一种 dtype 类型</li><li>大小固定,创建好数组时一旦指定好大小,就不会再发生改变</li></ul><p>``</p><h3 id="ndarray-属性"><a href="#ndarray-属性" class="headerlink" title="ndarray 属性"></a>ndarray 属性</h3><ul><li>ndim: 维度数量  </li><li>shape: 数组的形状, 是一个表示各维度大小的元组 </li><li>dtype: 表示数组元素类型的对象</li><li>size: 数组中元素的总个数,是 shape 中各维度数量相乘得到的值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 约定俗成的引用方法是 import numpy as np</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.array([</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>],</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">4</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>],</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">3</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">8</span>],</span><br><span class="line">    ]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">arr.ndim  <span class="comment"># 3</span></span><br><span class="line">arr.dtype  <span class="comment"># dtype('int64')</span></span><br><span class="line">arr.shape  <span class="comment"># (3, 2, 4), 表示第一层中括号下有 3 个元素.第二层中括号下有 2 个元素,第三层中括号下有 4 个元素</span></span><br><span class="line">arr.size  <span class="comment"># 24</span></span><br></pre></td></tr></table></figure><h3 id="ndarray-常见的创建方式"><a href="#ndarray-常见的创建方式" class="headerlink" title="ndarray 常见的创建方式"></a>ndarray 常见的创建方式</h3><h4 id="array-函数"><a href="#array-函数" class="headerlink" title="array 函数"></a>array 函数</h4><p>接收一个普通的 python 序列, 转成 ndarray</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.array([1, 2, 3, 4])</span><br><span class="line">arr &#x3D; np.array((1, 2, 3, 4))</span><br></pre></td></tr></table></figure><h4 id="zeros-函数"><a href="#zeros-函数" class="headerlink" title="zeros 函数"></a>zeros 函数</h4><p>创建指定长度或形状的全零数组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.zeros((3, 4))</span><br><span class="line"># 创建的数组如下,zeros 的参数其实就是要创建的数组的 shape 值,</span><br><span class="line"># array([[0., 0., 0., 0.],</span><br><span class="line">#        [0., 0., 0., 0.],</span><br><span class="line">#        [0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure><h4 id="ones-函数"><a href="#ones-函数" class="headerlink" title="ones 函数"></a>ones 函数</h4><p>创建指定长度或形状的全 1 数组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.ones((3, 4))</span><br><span class="line"></span><br><span class="line"># array([[1., 1., 1., 1.],</span><br><span class="line">#       [1., 1., 1., 1.],</span><br><span class="line">#       [1., 1., 1., 1.]])</span><br></pre></td></tr></table></figure><h4 id="empty-函数"><a href="#empty-函数" class="headerlink" title="empty 函数"></a>empty 函数</h4><p>创建一个没有任何具体值的数组, 所以创建速度会比 zeros 和 ones 快, 但这种方式需要手动指定所有值,慎用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 在测试中发现一个有趣的现象,如果在 empty() 中指定的 shape 在之前已经被 zeros 或 ones 指定过,然么 empty 会直接使用 zeros 或 ones 的值</span><br><span class="line"></span><br><span class="line">arr &#x3D; np.empty((2, 2))</span><br><span class="line"></span><br><span class="line"># array([[4.9e-324, 9.9e-324],</span><br><span class="line">#        [1.5e-323, 2.0e-323]])</span><br><span class="line"></span><br><span class="line">arr &#x3D; np.ones((2, 2))</span><br><span class="line"># array([[1., 1.],</span><br><span class="line">#        [1., 1.]])</span><br><span class="line"></span><br><span class="line">arr &#x3D; np.empty((2, 2))</span><br><span class="line"># array([[1., 1.],</span><br><span class="line">#        [1., 1.]])</span><br></pre></td></tr></table></figure><h4 id="arange-函数"><a href="#arange-函数" class="headerlink" title="arange 函数"></a>arange 函数</h4><p>类似于 python 中的 range 函数,通过指定开始值,终值和步长来创建一维数组,注意不包括终值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(6)</span><br><span class="line"># array([0, 1, 2, 3, 4, 5])</span><br><span class="line"></span><br><span class="line"># 同 python 的 range 类似, 第三个参数是步长</span><br><span class="line">arr &#x3D; np.arange(0, 7, 2)</span><br><span class="line"># array([0, 2, 4, 6])</span><br><span class="line"></span><br><span class="line"># 可以使用 reshape 把一维数组转为多维</span><br><span class="line">arr &#x3D; np.arange(12).reshape(3, 4)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#        [ 4,  5,  6,  7],</span><br><span class="line">#        [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><h4 id="linespace-函数"><a href="#linespace-函数" class="headerlink" title="linespace 函数"></a>linespace 函数</h4><p>通过指定开始值,终值和元素个数来创建一维数组(等差数列),可以通过 endpoint 关键字指定是否包括终值, 默认包括</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 第三个参数表示元素个数</span><br><span class="line">arr &#x3D; np.linspace(0, 10, 5)</span><br><span class="line"># array([ 0. ,  2.5,  5. ,  7.5, 10. ])</span><br></pre></td></tr></table></figure><h4 id="logspace-函数"><a href="#logspace-函数" class="headerlink" title="logspace 函数"></a>logspace 函数</h4><p>和 linespace 类似,不过它创建等比数列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.logspace(0, 2, 3)</span><br><span class="line"># array([  1.,  10., 100.])</span><br></pre></td></tr></table></figure><h4 id="random-函数生成数组"><a href="#random-函数生成数组" class="headerlink" title="random() 函数生成数组"></a>random() 函数生成数组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.random.random((2, 2))</span><br><span class="line"># array([[0.34659492, 0.83549888],</span><br><span class="line">#       [0.83618173, 0.0771643 ]])</span><br></pre></td></tr></table></figure><h4 id="改变-ndarray-的形状的"><a href="#改变-ndarray-的形状的" class="headerlink" title="改变 ndarray 的形状的"></a>改变 ndarray 的形状的</h4><h5 id="1-直接修改-ndarray-的-shape-值"><a href="#1-直接修改-ndarray-的-shape-值" class="headerlink" title="1. 直接修改 ndarray 的 shape 值"></a>1. 直接修改 ndarray 的 shape 值</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(10)  </span><br><span class="line"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class="line">arr.shape  # (10,)</span><br><span class="line">arr.shape &#x3D; 2, 5</span><br><span class="line">arr</span><br><span class="line"># array([[0, 1, 2, 3, 4],</span><br><span class="line">#        [5, 6, 7, 8, 9]])</span><br><span class="line"></span><br><span class="line">arr.shape &#x3D; -1, 2</span><br><span class="line">arr </span><br><span class="line"># array([[0, 1],</span><br><span class="line">#        [2, 3],</span><br><span class="line">#        [4, 5],</span><br><span class="line">#        [6, 7],</span><br><span class="line">#        [8, 9]])</span><br></pre></td></tr></table></figure><h5 id="2-使用-reshape-函数"><a href="#2-使用-reshape-函数" class="headerlink" title="2. 使用 reshape 函数"></a>2. 使用 reshape 函数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(9)</span><br><span class="line"># array([0, 1, 2, 3, 4, 5, 6, 7, 8])</span><br><span class="line">arr.shape  # (9,)</span><br><span class="line">arr.reshape(3, 3)</span><br><span class="line">arr</span><br><span class="line"># array([[0, 1, 2],</span><br><span class="line">#       [3, 4, 5],</span><br><span class="line">#       [6, 7, 8]])</span><br></pre></td></tr></table></figure><blockquote><p>注意: 当指定新数组某个轴的元素为 -1 时, 将根据数组元素的个数自动计算此轴的长度</p></blockquote><h3 id="numpy-中的数据类型"><a href="#numpy-中的数据类型" class="headerlink" title="numpy 中的数据类型"></a>numpy 中的数据类型</h3><p>创建 ndarray 数据时,可以通过 dtype 属性显示指定数据类型, 如果不指定, numpy 会自动推断出合适的数据类型,所以一般无需显示指定</p><p>astype 方法,可以转换数组的元素数据类型, 得到一个新数组</p><table><thead><tr><th>数据类型</th><th>类型代码</th><th>说明</th></tr></thead><tbody><tr><td>int_</td><td></td><td>默认整型 (与 C 中的 long 相同, 通常为 int64 或 int32)</td></tr><tr><td>intc</td><td></td><td>完全等同于 C 中的 long (通常为 int64 或者 int32)</td></tr><tr><td>intp</td><td></td><td>表示索引的整型, 与 C 中的 size_t 相同, 通常为 int64 或者 int32</td></tr><tr><td>int8</td><td>i1</td><td>字节(-128 ~ 127), 1个字节</td></tr><tr><td>int16</td><td>i2</td><td>整型(-32768 ~　32767). 2个字节</td></tr><tr><td>int32</td><td>i4</td><td>整型 (-2147483648 ~ 2147483647), 4个字节</td></tr><tr><td>int64</td><td>i8</td><td>整型 (-9223372036854775808 ~ 9223372036854775807), 8个字节</td></tr><tr><td>uint8</td><td>u1</td><td>无符号整型 (0 ~ 255)</td></tr><tr><td>uint16</td><td>u2</td><td>无符号整型 (0 ~ 65535)</td></tr><tr><td>uint32</td><td>u4</td><td>无符号整型 (0 ~ 4294967295)</td></tr><tr><td>unint64</td><td>u8</td><td>无符号整型 (0 ~ 18446744073709551615)</td></tr><tr><td>float_</td><td></td><td>float64 的简写形式</td></tr><tr><td>float16</td><td>f2</td><td>半精度浮点型: 符号位, 5 位指数, 10 位小数部分</td></tr><tr><td>float32</td><td>f4 或者 f</td><td>单精度浮点数：32位，符号位, 8 位指数, 23 位小数部分</td></tr><tr><td>float64</td><td>f8 或者 d</td><td>双精度浮点数：64位，符号位, 11 位指数, 52 位小数部分</td></tr><tr><td>float128</td><td>f16 或者 g</td><td>扩展精度浮点数</td></tr><tr><td>complex_</td><td>c16</td><td>complex128 的简写形式</td></tr><tr><td>complex64</td><td>c8</td><td>复数,由两个 32 位的浮点数来表示(实数部分和虚数部分)</td></tr><tr><td>complex128</td><td>c16</td><td>复数,由两个 64 位的浮点数来表示(实数部分和虚数部分)</td></tr><tr><td>bool_</td><td></td><td>以一个字节形式存储的布尔值(True / False)</td></tr><tr><td>bool</td><td>?</td><td>存储 True 和 False 值的布尔类型</td></tr><tr><td>object</td><td>O</td><td>python 对象类型</td></tr><tr><td>String_</td><td>S</td><td>固定长度的字符串类型(每个字符1个字节) eg.创建一个长度为 8 的字符串使用 S8</td></tr><tr><td>Unicode_</td><td>U</td><td>固定长度的 unicode 类型(字节数由平台决定), 跟字符串的定义方式一样, eg U8</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.array([&quot;python&quot;, &quot;scala&quot;, &quot;javascript&quot;, &quot;lua&quot;])</span><br><span class="line">arr.dtype  # dtype(&#39;&lt;U10&#39;)</span><br><span class="line">arr &#x3D; np.array([&quot;python&quot;, &quot;scala&quot;, &quot;javascript&quot;, &quot;lua&quot;], dtype&#x3D;&#39;S6&#39;)</span><br><span class="line"># array([b&#39;python&#39;, b&#39;scala&#39;, b&#39;javasc&#39;, b&#39;lua&#39;], dtype&#x3D;&#39;|S6&#39;) 注意有元素被截断</span><br><span class="line">arr.dtype  # dtype(&#39;S6&#39;)</span><br><span class="line"></span><br><span class="line">## 在需要指定类型的时候,推荐使用 np 对象内提供的类型</span><br><span class="line"></span><br><span class="line">arr &#x3D; np.array([&quot;python&quot;, &quot;scala&quot;, &quot;javascript&quot;, &quot;lua&quot;], dtype&#x3D;np.string_)</span><br><span class="line"># array([b&#39;python&#39;, b&#39;scala&#39;, b&#39;javascript&#39;, b&#39;lua&#39;], dtype&#x3D;&#39;|S10&#39;)</span><br></pre></td></tr></table></figure><h3 id="numpy-基本操作"><a href="#numpy-基本操作" class="headerlink" title="numpy 基本操作"></a>numpy 基本操作</h3><h4 id="数组与标量-数组之间的运算"><a href="#数组与标量-数组之间的运算" class="headerlink" title="数组与标量, 数组之间的运算"></a>数组与标量, 数组之间的运算</h4><ul><li>数组不用循环即可对每个元素执行批量运算,这通常就叫做矢量化,即用数组表达式代替循环的做法</li><li>矢量化数组运算性能要比纯 python 方式快一两个数量级</li><li>大小相等的数组之间的任何算术运算都会将运算应用到元素级</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">## 数组与标量之间的运算</span><br><span class="line">arr &#x3D; np.arange(1, 6)</span><br><span class="line"># array([1, 2, 3, 4, 5])</span><br><span class="line"></span><br><span class="line">arr + 2</span><br><span class="line"># array([3, 4, 5, 6, 7])</span><br><span class="line"></span><br><span class="line">arr - 2</span><br><span class="line"># array([-1,  0,  1,  2,  3])</span><br><span class="line"></span><br><span class="line">arr * 2</span><br><span class="line"># array([ 2,  4,  6,  8, 10])</span><br><span class="line"></span><br><span class="line">arr &#x2F; 2</span><br><span class="line"># array([0.5, 1. , 1.5, 2. , 2.5])</span><br><span class="line"></span><br><span class="line">arr ** 2 </span><br><span class="line"># array([ 1,  4,  9, 16, 25])</span><br><span class="line"></span><br><span class="line">## shape 相同的数组之间的运算</span><br><span class="line">arr1 &#x3D; np.arange(4).reshape(2, 2)</span><br><span class="line"># array([[0, 1],</span><br><span class="line">#        [2, 3]])</span><br><span class="line"></span><br><span class="line">arr2 &#x3D; np.arange(1, 5).reshape(2, 2)</span><br><span class="line"># array([[1, 2],</span><br><span class="line">#        [3, 4]])</span><br><span class="line"></span><br><span class="line">arr1 + arr2</span><br><span class="line"># array([[1, 3],</span><br><span class="line">#       [5, 7]])</span><br><span class="line"></span><br><span class="line">arr1 * arr2</span><br><span class="line"># array([[ 0,  2],</span><br><span class="line">#       [ 6, 12]])</span><br><span class="line"></span><br><span class="line">arr1 &#x2F; arr2</span><br><span class="line"># array([[0.        , 0.5       ],</span><br><span class="line">#       [0.66666667, 0.75      ]])</span><br></pre></td></tr></table></figure><h4 id="数组的矩阵积-matrix-product"><a href="#数组的矩阵积-matrix-product" class="headerlink" title="数组的矩阵积(matrix product)"></a>数组的矩阵积(matrix product)</h4><ul><li>两个多为矩阵(通常是二维)满足第一个矩阵的列数与第二个矩阵的行数相同,那么可以进行矩阵乘法</li><li>两个矩阵相乘结果所得到的数据中每个元素为,第一个矩阵中与该元素行号相同的元素与第二个矩阵中与该元素列号相同的元素,两两相乘后的求和</li></ul><p>eg. 假设有一下两张表格,要求计算每个部门购买产品的总价格和总占用空间</p><table><thead><tr><th>\</th><th>产品A</th><th>产品B</th><th>产品C</th></tr></thead><tbody><tr><td>部门1</td><td>2</td><td>4</td><td>6</td></tr><tr><td>部门2</td><td>1</td><td>3</td><td>5</td></tr><tr><td>部门3</td><td>2</td><td>3</td><td>4</td></tr></tbody></table><table><thead><tr><th>\</th><th>单价</th><th>单件体积</th></tr></thead><tbody><tr><td>产品A</td><td>6</td><td>5</td></tr><tr><td>产品B</td><td>4</td><td>3</td></tr><tr><td>产品C</td><td>2</td><td>1</td></tr></tbody></table><p>分别计算每个部分的用花费和总体积:</p><p>部门1  总花费: 2 x 6 + 4 x 4 + 6 x 2 = <strong>40</strong>  总体积: 2 x 5 + 4 x 3 + 6 x 1 = <strong>28</strong><br>部门2  总花费: 1 x 6 + 3 x 4 + 5 x 2 = <strong>28</strong>  总体积: 1 x 5 + 3 x 3 + 5 x 1 = <strong>19</strong><br>部门3  总花费: 2 x 6 + 3 x 4 + 4 x 2 = <strong>32</strong>  总体积: 2 x 5 + 3 x 3 + 4 x 1 = <strong>23</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 用 numpy 的矩阵积计算</span><br><span class="line"></span><br><span class="line">arr1 &#x3D; np.array([</span><br><span class="line">    [2, 4, 6],</span><br><span class="line">    [1, 3, 5],</span><br><span class="line">    [2, 3, 4]</span><br><span class="line">])</span><br><span class="line">arr2 &#x3D; np.array([</span><br><span class="line">    [6, 5],</span><br><span class="line">    [4, 3],</span><br><span class="line">    [2, 1]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 直接就可以计算出满足需求的结果</span><br><span class="line">res &#x3D; np.dot(arr1, arr2)</span><br><span class="line"># array([[40, 28],</span><br><span class="line">#        [28, 19],</span><br><span class="line">#       [32, 23]])</span><br></pre></td></tr></table></figure><h4 id="数组的索引与切片"><a href="#数组的索引与切片" class="headerlink" title="数组的索引与切片"></a>数组的索引与切片</h4><h5 id="多维数组的索引"><a href="#多维数组的索引" class="headerlink" title="多维数组的索引"></a>多维数组的索引</h5><p>对于二维数组的索引如表中所示,其他维度的数组索引与此类似<br>横轴\ 纵轴| 0 | 1 | 2 | 3<br>—|—|—|—|—<br> <strong>0</strong> | [0, 0] | [0, 1] | [0, 2] | [0, 3]<br> <strong>1</strong> | [1, 0] | [1, 1] | [1, 2] | [1, 3]<br> <strong>2</strong> | [2, 0] | [2, 1] | [2, 2] | [2, 3]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(2, 2, 3)</span><br><span class="line"># array([[[ 0,  1,  2],</span><br><span class="line">#        [ 3,  4,  5]],</span><br><span class="line">#</span><br><span class="line">#       [[ 6,  7,  8],</span><br><span class="line">#        [ 9, 10, 11]]])</span><br><span class="line"></span><br><span class="line">arr[1]</span><br><span class="line"># array([[ 6,  7,  8],</span><br><span class="line">#       [ 9, 10, 11]])</span><br><span class="line"></span><br><span class="line">arr[1][1]</span><br><span class="line"># array([ 9, 10, 11])</span><br><span class="line"></span><br><span class="line">arr[1, 1]</span><br><span class="line"># array([ 9, 10, 11])</span><br><span class="line"></span><br><span class="line">arr[1, 1, 2]</span><br><span class="line"># 11</span><br></pre></td></tr></table></figure><h5 id="布尔型索引"><a href="#布尔型索引" class="headerlink" title="布尔型索引"></a>布尔型索引</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(2, 2, 3)</span><br><span class="line"># array([[[ 0,  1,  2],</span><br><span class="line">#        [ 3,  4,  5]],</span><br><span class="line">#</span><br><span class="line">#       [[ 6,  7,  8],</span><br><span class="line">#        [ 9, 10, 11]]])</span><br><span class="line"></span><br><span class="line">arr &lt; 5</span><br><span class="line"># array([[[ True,  True,  True],</span><br><span class="line">#         [ True,  True, False]],</span><br><span class="line"># </span><br><span class="line">#        [[False, False, False],</span><br><span class="line">#         [False, False, False]]])</span><br><span class="line"></span><br><span class="line">arr[arr &lt; 5]</span><br><span class="line"># array([0, 1, 2, 3, 4])  中 True 位置上取出的元素组成新的数组</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">names &#x3D; np.array([&#39;zj&#39;, &#39;zk&#39;, &#39;zl&#39;])</span><br><span class="line">scores &#x3D; np.array([</span><br><span class="line">    [98, 97, 96],</span><br><span class="line">    [89, 79, 69],</span><br><span class="line">    [77, 66, 55]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"># 取出 &#39;zj&#39; 对应的分数</span><br><span class="line">scores[names &#x3D;&#x3D; &#39;zj&#39;]</span><br><span class="line"># array([[98, 97, 96]])</span><br><span class="line"></span><br><span class="line"># 取出 &#39;zj&#39; 对应的分数的第二项分数</span><br><span class="line">scores[names&#x3D;&#x3D;&#39;zj&#39;, 1]</span><br><span class="line"># array([97])</span><br><span class="line"></span><br><span class="line"># numpy 中可以使用 逻辑与 &amp;, 逻辑或 |, 逻辑非 ~</span><br><span class="line"># 取出不是 zj 的分数</span><br><span class="line">scores[~(names &#x3D;&#x3D; &quot;zj&quot;)]</span><br><span class="line"># array([[89, 79, 69],</span><br><span class="line">#       [77, 66, 55]])array([[89, 79, 69],</span><br><span class="line">#       [77, 66, 55]])</span><br></pre></td></tr></table></figure><h5 id="花式索引"><a href="#花式索引" class="headerlink" title="花式索引"></a>花式索引</h5><p>指的是利用整数数组进行索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(6, 2)</span><br><span class="line"># array([[ 0,  1],</span><br><span class="line">#       [ 2,  3],</span><br><span class="line">#       [ 4,  5],</span><br><span class="line">#       [ 6,  7],</span><br><span class="line">#       [ 8,  9],</span><br><span class="line">#       [10, 11]])</span><br><span class="line"></span><br><span class="line"># 取第一维度的第 0, 3, 5 这几个元素</span><br><span class="line">arr[[0, 3, 5]]</span><br><span class="line"># array([[ 0,  1],</span><br><span class="line">#       [ 6,  7],</span><br><span class="line">#       [10, 11]])</span><br><span class="line"></span><br><span class="line"># 取第一维度的第 0 个元素中的 第 1 个元素,第 3 个元素中的第 0 个元素,第 4 个元素中的第 1 个元素</span><br><span class="line">arr[[0, 3, 5], [1, 0 ,1]]</span><br><span class="line"># array([ 1,  6, 11])</span><br><span class="line"></span><br><span class="line"># ix_函数产生一个索引器</span><br><span class="line">ix &#x3D; np.ix_([0, 3, 5], [1, 0, 1])</span><br><span class="line"># (array([[0],[3],[5]]), array([[1, 0, 1]]))</span><br><span class="line"> </span><br><span class="line">arr[ix]</span><br><span class="line"># array([[ 1,  0,  1],</span><br><span class="line">#        [ 7,  6,  7],</span><br><span class="line">#        [11, 10, 11]])</span><br></pre></td></tr></table></figure><h5 id="numpy-数组的切片"><a href="#numpy-数组的切片" class="headerlink" title="numpy 数组的切片"></a>numpy 数组的切片</h5><ul><li>在各维度上单独切片,如果某维度都要保留,则直接使用 <code>:</code> 冒号, 不指定起始值和终止值</li><li>numpy 中通过切片得到的新数组,只是原来数组的一个视图, 因此对新数组进行操作也会影响原数组</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(2, 2, 3)</span><br><span class="line"># array([[[ 0,  1,  2],</span><br><span class="line">#        [ 3,  4,  5]],</span><br><span class="line">#</span><br><span class="line">#       [[ 6,  7,  8],</span><br><span class="line">#        [ 9, 10, 11]]])</span><br><span class="line"></span><br><span class="line">arr[1][1][1:]</span><br><span class="line"># array([10, 11])</span><br><span class="line"></span><br><span class="line">arr[0, : , 1:]</span><br><span class="line"># array([[1, 2],</span><br><span class="line">#       [4, 5]])</span><br></pre></td></tr></table></figure><h4 id="数组转置"><a href="#数组转置" class="headerlink" title="数组转置"></a>数组转置</h4><p>transpose 函数和数组的 T 属性用于数组转置,对于二维数组就是行列互换</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(3, 4)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br><span class="line"></span><br><span class="line">arr.transpose()</span><br><span class="line"># array([[ 0,  4,  8],</span><br><span class="line">#       [ 1,  5,  9],</span><br><span class="line">#       [ 2,  6, 10],</span><br><span class="line">#       [ 3,  7, 11]])</span><br><span class="line"></span><br><span class="line">arr.T</span><br><span class="line"># array([[ 0,  4,  8],</span><br><span class="line">#       [ 1,  5,  9],</span><br><span class="line">#       [ 2,  6, 10],</span><br><span class="line">#       [ 3,  7, 11]])</span><br></pre></td></tr></table></figure><h4 id="通用函数-快速的元素级数组函数"><a href="#通用函数-快速的元素级数组函数" class="headerlink" title="通用函数: 快速的元素级数组函数"></a>通用函数: 快速的元素级数组函数</h4><p>ufunc: 一种对 dnarray 中的数据执行元素级运算的函数,也可以看做是简单函数(接受一个或多个标量值,并产生一个或多个标量值)的矢量化包装器</p><h5 id="常见的一元通用函数"><a href="#常见的一元通用函数" class="headerlink" title="常见的一元通用函数"></a>常见的一元通用函数</h5><table><thead><tr><th>一元 ufunc</th><th>说明</th></tr></thead><tbody><tr><td>abs, fabs</td><td>计算整数,浮点数或复数的绝对值,对于非复数值</td></tr><tr><td>sqrt</td><td>计算各元素的平方根, 相当于 arr**0.5</td></tr><tr><td>square</td><td>计算各元素的平方, 相当于 arr**2</td></tr><tr><td>exp</td><td>计算各元素的指数 e 的 x 次方</td></tr><tr><td>log. log10, log2, log1p</td><td>分别为自然对数,底数是 10 的 log, 底数为 2 的 log, log(1+x)</td></tr><tr><td>sign</td><td>计算各元素的正负号, 1: 正数, 0: 零, -1 负数</td></tr><tr><td>cell</td><td>计算大于等于该值的最小正数</td></tr><tr><td>floor</td><td>计算小于等于该值的最大整数</td></tr><tr><td>rint</td><td>将各元素值四舍五入到最接近的整数, 保留 dtype</td></tr><tr><td>modf</td><td>将数组的小数位和整数部分以两个独立数组的形式返回</td></tr><tr><td>isnan</td><td>返回一个表示 “哪些值是 NaN” 的布尔类型数组</td></tr><tr><td>isfinite, isinf</td><td>分别返回一个表示 “哪些元素是有穷的(非 inf, 非 nan)” 或 “哪些元素是无穷的” 的布尔型整数</td></tr><tr><td>cos, cosh, sin, sinh, tan, tanh</td><td>普通型和双曲型三角函数</td></tr><tr><td>arccos, arccosh, arc, sin, arctan, arctanh</td><td>反三角函数</td></tr><tr><td>logical_not</td><td>计算各元素 not x 的真值, 相当于 ~ 和 -arr</td></tr></tbody></table><h5 id="常见的二元通用函数"><a href="#常见的二元通用函数" class="headerlink" title="常见的二元通用函数"></a>常见的二元通用函数</h5><table><thead><tr><th>二元 ufunc</th><th>说明</th></tr></thead><tbody><tr><td>add</td><td>将数据中相同位置对应的元素相加</td></tr><tr><td>substract</td><td>从第一个数组中减去第二个数组中的元素</td></tr><tr><td>multiply</td><td>数据元素相乘</td></tr><tr><td>divide, floor_divive</td><td>除法或者向下整除法(丢弃余数)</td></tr><tr><td>pow</td><td>对第一个数组中的元素 A, 根据第二个数组中相应元素 B, 计算 A 的 B 次方</td></tr><tr><td>maximum, fmax</td><td>元素级别的最大值, fmax 会忽略 NaN</td></tr><tr><td>minimum, fmin</td><td>元素级别的最小值, fmin 会忽略 NaN</td></tr><tr><td>mod</td><td>元素级求模(除法的余数)</td></tr><tr><td>copysign</td><td>将第二个数组中的值的符号复制给第一个数组中对应位置的值</td></tr><tr><td>greater,greater_equal,less,less_equal,equal,not_equal</td><td>执行元素级别的比较运算,最终产生布尔型数组</td></tr><tr><td>logical_and, logical_or, logical_xor</td><td>执行元素级别的布尔逻辑运算, 相当于 &amp;,</td></tr></tbody></table><h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><p>聚合函数是对一组值(比如一个数组)进行操作,返回一个单一值作为结果的函数.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.array([1, 2, 3, 4])</span><br><span class="line">arr.max()  #最大值  4</span><br><span class="line">arr.min()  # 最小值  1</span><br><span class="line">arr.mean()  # 均值  2.5</span><br><span class="line">arr.std()  # 标准差  1.118033988749895</span><br><span class="line">np.sqrt(np.power(arr - arr.mean(), 2).sum()&#x2F;arr.size)  # 标准差  1.118033988749895</span><br></pre></td></tr></table></figure><p>聚合函数可以指定对数值的某个轴元素进行操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.array([</span><br><span class="line">    [1, 2, 3, 4],</span><br><span class="line">    [5, 6, 7, 8]</span><br><span class="line">    ])</span><br><span class="line"># 对同一列上的元素计算均值, axis&#x3D;0 表示横轴, axis&#x3D;1 表示纵轴</span><br><span class="line">arr.mean(axis&#x3D;0)  # array([3., 4., 5., 6.])</span><br><span class="line"># 对同一行上的元素计算均值</span><br><span class="line">arr.mean(axis&#x3D;1)  # array([2.5, 6.5])</span><br><span class="line"># 同一列求和</span><br><span class="line">arr.sum(axis&#x3D;0)  # array([ 6,  8, 10, 12])</span><br><span class="line"># 同一行求最大值</span><br><span class="line">arr.max(axis&#x3D;1)  # array([4, 8])</span><br><span class="line"># 同一列求标准差</span><br><span class="line">arr.std(axis&#x3D;0)  # array([2., 2., 2., 2.])</span><br></pre></td></tr></table></figure><h4 id="np-where-函数"><a href="#np-where-函数" class="headerlink" title="np.where 函数"></a>np.where 函数</h4><p>三元表达式 <code>x if condition else y</code> 的矢量化版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">xarr &#x3D; np.array([1, 2, 3, 4, 5])</span><br><span class="line">yarr &#x3D; np.array([2, 3, 4, 5, 6])</span><br><span class="line">condition &#x3D; np.array([True, False, True, False, True])</span><br><span class="line">result &#x3D; [(x if c else y) for x, y, c in zip(xarr, yarr, condition)]</span><br><span class="line"># [1, 3, 3, 5, 5]</span><br><span class="line"></span><br><span class="line">result &#x3D; np.where(condition, xarr, yarr)</span><br><span class="line"># array([1, 3, 3, 5, 5])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 将数组中所有的 NaN 缺失值替换为 0 </span><br><span class="line">arr &#x3D; np.array([</span><br><span class="line">    [1, 2, np.NaN, 4],</span><br><span class="line">    [3, 4, 5, np.NaN]</span><br><span class="line">])</span><br><span class="line">condition &#x3D; np.isnan(arr)</span><br><span class="line">result &#x3D; np.where(condition, 0, arr)</span><br><span class="line"># array([[1., 2., 0., 4.],</span><br><span class="line">#       [3., 4., 5., 0.]])</span><br></pre></td></tr></table></figure><h4 id="np-unique-函数"><a href="#np-unique-函数" class="headerlink" title="np.unique 函数"></a>np.unique 函数</h4><p>求数组中去掉重复元素后的一维数组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.array([</span><br><span class="line">    [&#39;a&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;h&#39;, &#39;c&#39;, &#39;f&#39;],</span><br><span class="line">    [&#39;a&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;h&#39;, &#39;c&#39;, &#39;f&#39;]</span><br><span class="line">])</span><br><span class="line">result &#x3D; np.unique(arr)</span><br><span class="line"># array([&#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;f&#39;, &#39;h&#39;], dtype&#x3D;&#39;&lt;U1&#39;)</span><br></pre></td></tr></table></figure><h3 id="数组数据文件读写"><a href="#数组数据文件读写" class="headerlink" title="数组数据文件读写"></a>数组数据文件读写</h3><h4 id="将数组以二进制格式保存到磁盘"><a href="#将数组以二进制格式保存到磁盘" class="headerlink" title="将数组以二进制格式保存到磁盘"></a>将数组以二进制格式保存到磁盘</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12).reshape(3, 4)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br><span class="line"></span><br><span class="line"># 可以存储到本地文件名为 &#39;arr&#39;, 后缀名为 &#39;.npy&#39; 的文件中, </span><br><span class="line">np.save(&#39;arr&#39;, arr)</span><br><span class="line"></span><br><span class="line"># 读取二进制文件</span><br><span class="line">arr &#x3D; np.load(&#39;arr.npy&#39;)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><h4 id="存取文本文件"><a href="#存取文本文件" class="headerlink" title="存取文本文件"></a>存取文本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.arange(12, dtype&#x3D;np.int).reshape(3, 4)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br><span class="line"></span><br><span class="line"># 转存为 &#39;csv&#39; 格式文件</span><br><span class="line">np.savetxt(&#39;arr.csv&#39;, arr, delimiter&#x3D;&#39;,&#39;, fmt&#x3D;&quot;%s&quot;)</span><br><span class="line"></span><br><span class="line"># 从 csv 文件中读取</span><br><span class="line">arr &#x3D; np.loadtxt(&#39;arr.csv&#39;, delimiter&#x3D;&#39;,&#39;, dtype&#x3D;np.int)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br><span class="line"></span><br><span class="line"># 从 csv 文件中读取</span><br><span class="line">arr &#x3D; np.genfromtxt(&#39;arr.csv&#39;, delimiter&#x3D;&#39;,&#39;, dtype&#x3D;np.int)</span><br><span class="line"># array([[ 0,  1,  2,  3],</span><br><span class="line">#       [ 4,  5,  6,  7],</span><br><span class="line">#       [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><p>使用 genformtxt 还可以指定 header, 假设有文件内容如下的 test.csv 文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A,B,C,D</span><br><span class="line">0,1,2,3</span><br><span class="line">4,5,6,7</span><br><span class="line">8,9,10,11</span><br></pre></td></tr></table></figure><p>可以这样灵活操作:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr &#x3D; np.genfromtxt(&quot;test.csv&quot;, delimiter&#x3D;&quot;,&quot;, names&#x3D;True, dtype&#x3D;np.int)</span><br><span class="line"># array([(0, 1,  2,  3), (4, 5,  6,  7), (8, 9, 10, 11)],</span><br><span class="line"># dtype&#x3D;[(&#39;A&#39;, &#39;&lt;i8&#39;), (&#39;B&#39;, &#39;&lt;i8&#39;), (&#39;C&#39;, &#39;&lt;i8&#39;), (&#39;D&#39;, &#39;&lt;i8&#39;)])</span><br><span class="line"></span><br><span class="line">arr[&#39;A&#39;]  # array([0, 4, 8])</span><br><span class="line">arr[0]  #  (0, 1, 2, 3)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark 性能优化方法</title>
      <link href="/2018-02-02-spark-base-optimization.html"/>
      <url>/2018-02-02-spark-base-optimization.html</url>
      
        <content type="html"><![CDATA[<p>开发 spark 程序的关键,是时时刻刻保持对性能消耗的敏感,尽量优化.</p><a id="more"></a><h3 id="尽可能分配更多的资源"><a href="#尽可能分配更多的资源" class="headerlink" title="尽可能分配更多的资源"></a>尽可能分配更多的资源</h3><h4 id="需要增加的资源"><a href="#需要增加的资源" class="headerlink" title="需要增加的资源"></a>需要增加的资源</h4><ul><li>executor: executor 的数量与可以并行执行的 task 数量正相关.比如有 3个 executor,每个 executor 有 2 个 CPUcore, 那么同时能够并行的执行的 task 就是 6 个,这 6 个 执行完后再执行下一批 6 个 task</li><li>CPUcore: CPUcore 的数量与可以并行执行的 task 数量正相关.</li><li>executor 的内存量: rdd的持久化，shuffle等操作需要足够的内存</li></ul><h4 id="启动时用于分配资源的参数"><a href="#启动时用于分配资源的参数" class="headerlink" title="启动时用于分配资源的参数"></a>启动时用于分配资源的参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;spark&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">--num-executors 3 \  # 配置executor的数量</span><br><span class="line">--driver-memory 100m \   # 配置driver的内存（影响不大）</span><br><span class="line">--executor-memory 100m \  # 配置每个executor的内存大小</span><br><span class="line">--executor-cores 3 \  # 配置每个executor的cpu core数量</span><br><span class="line">&#x2F;test.py</span><br></pre></td></tr></table></figure><h3 id="调节并行度"><a href="#调节并行度" class="headerlink" title="调节并行度"></a>调节并行度</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>Spark 作业中,各个 stage 的 task 数量代表了 Spark 作业的各个阶段 (stage)的并行度.  </p><p>合理的调节并行度,可以充分利用集群的计算资源,并且减少每个 task 要处理的数据量,很好的提升 spark 作业的性能和运行速度</p><p>task 数量至少设置成与 spark application 总 CPUcore 数量相同,官方推荐设置成 CPUcore 的 2~3 倍</p><h4 id="调节"><a href="#调节" class="headerlink" title="调节"></a>调节</h4><h5 id="设置默认并行度"><a href="#设置默认并行度" class="headerlink" title="设置默认并行度"></a>设置默认并行度</h5><p><code>spark.defalut.parallelism</code>   默认是没有值的， 设置了之后在shuffle操作中才会自动进行分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conf &#x3D; SparkConf()</span><br><span class="line">conf.set(&quot;spark.default.parallelism&quot;, &quot;500&quot;)  # 调节并行度</span><br><span class="line">sc &#x3D; SparkContext(&quot;local[*]&quot;, conf&#x3D;conf)</span><br></pre></td></tr></table></figure><h5 id="HDFS-源"><a href="#HDFS-源" class="headerlink" title="HDFS 源"></a>HDFS 源</h5><p>如果读取的数据在HDFS上，block数与partition对应，所以增加了block数，也就提高了并行度。</p><h5 id="在一些算子中设置"><a href="#在一些算子中设置" class="headerlink" title="在一些算子中设置"></a>在一些算子中设置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd &#x3D; ...</span><br><span class="line">rdd.repartition(numPartitions)  #给RDD重新设置 partition 的数量</span><br><span class="line">rdd.groupByKey([numTasks])</span><br><span class="line">rdd.reduceByKey(func, [numTasks])</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="重构rdd架构以及rdd持久化"><a href="#重构rdd架构以及rdd持久化" class="headerlink" title="重构rdd架构以及rdd持久化"></a>重构rdd架构以及rdd持久化</h3><p>默认情况下,多次对一个 RDD 执行算子都会对这个 RDD 以及之前的父 RDD全部重新计算一次.<br>这种情况会导致性能急剧降低,要尽量避免。</p><ul><li>尽量去复用 RDD, 差不多的 RDD 可以抽取称为一个共同的 RDD,供后面的 RDD 计算时反复使用.</li><li>公共 RDD 一定要实现持久化，RDD 可以使用 persist() 方法或 cache() 方法进行持久化。  </li><li>将数据持久化到内存中,可能会导致内存溢出，当内存无法支撑公共RDD数据完全存放的时候,就应该考虑 <code>使用序列化的方式再纯内存中存储</code>.</li></ul><h3 id="广播大变量"><a href="#广播大变量" class="headerlink" title="广播大变量"></a>广播大变量</h3><p>task 算子中如果使用了外部变量,每个 task 都会获取一份变量的副本，这样就会造成大量的网络传输,降低性能。</p><p>使用广播变量,就不是每个task一份变量副本,而是每个节点的 executor 一份副本。</p><h3 id="使用效率更高的序列化机制"><a href="#使用效率更高的序列化机制" class="headerlink" title="使用效率更高的序列化机制"></a>使用效率更高的序列化机制</h3><p>Spark 内部是使用 java 的序列化机制来进行序列化。这种默认序列化机制处理起来比较方便,只需要在算子里使用的变量实现 Serializable 接口即可。缺点是默认的序列化机制的效率不高,序列化的速度比较慢。  </p><p>Spark支持使用 Kryo(java/scala) 序列化机制,比默认的java机制速度要快,序列化以后的数据要更小,大概是java序列化机制的 1/10。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># pyspark</span><br><span class="line">conf &#x3D; SparkConf().set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br></pre></td></tr></table></figure><h3 id="调节的本地化的时长"><a href="#调节的本地化的时长" class="headerlink" title="调节的本地化的时长"></a>调节的本地化的时长</h3><h4 id="本地化级别介绍"><a href="#本地化级别介绍" class="headerlink" title="本地化级别介绍"></a>本地化级别介绍</h4><p>本地化级别指 task 和 数据的距离，由近到远为：</p><ul><li>PROCESS_LOCAL 进程本地化,代码和数据在如同一个进程中,也就是在同一个executor中,计算数据的task由executor执行,数据在executor的BlockManager中,性能最好</li><li>NODE_LOCAL 节点本地化,代码和数据在同一个节点上;比如说,数据作为一个 HDFS block 块,就在节点上,而 task 在节点上某个 executor中运行;或者是数据和task在一个节点上的不同 executor中,数据需要在进程间进行传输</li><li>NO_PREF 对于 task 来说,数据从哪里获取都一样,没有好坏之分</li><li>RACK_LOCAL 机架本地化,数据和task在一个机架的两个节点上,数据需要通过网络在节点之间进行传输</li><li>ANY 数据和task可能在集群中的任何地方,而且不在一个机架中,性能最差</li></ul><h4 id="Spark-切换本地化级别的机制"><a href="#Spark-切换本地化级别的机制" class="headerlink" title="Spark 切换本地化级别的机制"></a>Spark 切换本地化级别的机制</h4><p>Spark 在 driver 上对 task 进行分配之前,会计算出每个 task 要对应的分片数据,优先将 task 分配到数据所在的节点。但是一些 task 可能因为CPU 繁忙而不会分配到数据所在的节点。这时 spark 会等待一段时间，超时后就会切换到较低的本地化级别。降低本地化级别会发生数据传输,task 会通过其所在节点的 BlockManager 来获取数据.BlockManager 发现自己本地没有数据,会使用一个getRemote() 方法通过 TransferService(网络数据传输组件)从数据所在节点的BlockManager中获取数据。为了尽量避免本地化级别切换,可以适当的调节本地化级别切换的等待时间。</p><h4 id="调节等待时长"><a href="#调节等待时长" class="headerlink" title="调节等待时长"></a>调节等待时长</h4><p>用 client 模式(方便查看日志) 执行 application，日志里面会找到 <code>starting task..., PROCESS LOCAL, NODE LOCAL</code>  </p><p>如果大部分 task 的数据本地化级别是 PROCESS LOCAL, 就不需要调节参数了，如果发现有很多的级别都是 NODE LOCAL,ANY,那么就增加等待时长再执行，观察大部分的 task 本地化级别有没有提升,spark作业的运行时间有没有缩短</p><p>注意不要本末倒置,本地化级别提高了,但是因为增加了本地化等待的时长,spark 作业的运行时间反而着增加了.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># pyspark</span><br><span class="line">conf &#x3D; SparkConf().set(&quot;spark.locality.wait&quot;, &quot;20&quot;)</span><br><span class="line">sc &#x3D; SparkContext(conf&#x3D;conf)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 性能调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 的 Yarn-cluster 和 Yarn-client 提交模式</title>
      <link href="/2018-01-27-spark-yarn-client-cluster.html"/>
      <url>/2018-01-27-spark-yarn-client-cluster.html</url>
      
        <content type="html"><![CDATA[<p>详细介绍 Spark on yarn 的两种不同运行模式的区别</p><a id="more"></a><h3 id="Yarn-client-模式"><a href="#Yarn-client-模式" class="headerlink" title="Yarn-client 模式"></a>Yarn-client 模式</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>Yarn-client 模式用于测试, 因为 Driver 运行在本地客户端,负责调度 application, 会与 Yarn 集群产生大量的网络通信,从而导致网卡流量激增.<br>Yarn-client 模式的优点是可以在本地看到所有日志,方便调试</p><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p><img src="/images/spark/yarn-client.png" alt="image"></p><ol><li>spark-submit 本地提交 application, <strong>在本地启动 Driver 进程</strong>, 发送请求到 ResourceManager, 请求启动 ApplicationMaster</li><li>分配一个 container, 在某个 Nodemanager 上启动一个 ExecutorLanucher(相当于  ApplicationMaster)</li><li>ExecutorLanucher 向 ResourceManager 请求 Container, 启动 Executor</li><li>ResourceManager 分配一批 container 用于启动 Executor</li><li>ExecutorLanucher 连接到分配过来的 container(Nodemanager, 相当于Worker而), 启动 Executor</li><li><strong>Executor 启动后向本地 Driver 反向注册</strong></li></ol><h3 id="Yarn-cluster-模式"><a href="#Yarn-cluster-模式" class="headerlink" title="Yarn-cluster 模式"></a>Yarn-cluster 模式</h3><h4 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h4><p>Yarn-cluster 模式用于生产环境,因为 Driver 运行在 nodemanager 上, 不会有网卡流量激增的问题.<br>Yarn-cluster 模式缺点是调试不方便,不能直接查看日志,需要使用 <code>yarn logs -applicationId xxx</code> 命令查看</p><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h4><p><img src="/images/spark/yarn-cluster.png" alt="image"></p><ol><li>spark-submit 本地提交 application, 发送请求到 ResourceManager, 请求启动 ApplicationMaster</li><li>分配一个 container, 在某个 Nodemanager 上启动 ApplicationMaster(相当于 Driver)</li><li>ApplicationMaster 向 ResourceManager 请求 Container, 启动 Executor</li><li>ResourceManager 分配一批 container 用于启动 Executor</li><li>ApplicationMaster 连接到分配过来的 container(Nodemanager, 相当于Worker而), 启动 Executor</li><li>Executor 启动后向 ApplicationMaster 反向注册</li></ol><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>Yarn-client 模式的 Driver 进程运行在提交 application 的本地, 而 Yarn-cluster 模式的 Driver 运行在 ApplicationMaster</p><h3 id="配置注意"><a href="#配置注意" class="headerlink" title="配置注意"></a>配置注意</h3><p>配置 Spark Yarn 模式 必须在 spark-env 文件中, 配置 HADOOP_CONF_DIR 或 或YARN_CONF_DIR属性, spark 会根据这个路径找打 hadoop 的配置,从而管理 Hdfs 和 Yarn</p><table><thead><tr><th>名称</th><th>默认值</th><th>含义</th></tr></thead><tbody><tr><td>spark.yarn.am.memory</td><td>512m</td><td>client模式下，YARN Application Master使用的内存总量</td></tr><tr><td>spark.yarn.am.cores</td><td>1</td><td>client模式下，Application Master使用的cpu数量</td></tr><tr><td>spark.driver.cores</td><td>1</td><td>cluster模式下，driver使用的cpu core数量，driver与Application Master运行在一个进程中，所以也控制了Application Master的cpu数量</td></tr><tr><td>spark.yarn.am.waitTime</td><td>100s</td><td>cluster模式下，Application Master要等待SparkContext初始化的时长; client模式下，application master等待driver来连接它的时长</td></tr><tr><td>spark.yarn.submit.file.replication</td><td>hdfs副本数</td><td>作业写到hdfs上的文件的副本数量，比如工程jar，依赖jar，配置文件等，最小一定是1</td></tr><tr><td>spark.yarn.preserve.staging.files</td><td>false</td><td>如果设置为true，那么在作业运行完之后，会避免工程jar等文件被删除掉</td></tr><tr><td>spark.yarn.scheduler.heartbeat.interval-ms</td><td>3000</td><td>application master向resourcemanager发送心跳的间隔，单位ms</td></tr><tr><td>spark.yarn.scheduler.initial-allocation.interval</td><td>200ms</td><td>application master在有pending住的container分配需求时，立即向resourcemanager发送心跳的间隔</td></tr><tr><td>spark.yarn.max.executor.failures</td><td>executor数量*2，最小3</td><td>整个作业判定为失败之前，executor最大的失败次数</td></tr><tr><td>spark.yarn.historyServer.address</td><td>无</td><td>spark history server的地址</td></tr><tr><td>spark.yarn.dist.archives</td><td>无</td><td>每个executor都要获取并放入工作目录的archive</td></tr><tr><td>spark.yarn.dist.files</td><td>无</td><td>每个executor都要放入的工作目录的文件</td></tr><tr><td>spark.executor.instances</td><td>2</td><td>默认的executor数量</td></tr><tr><td>spark.yarn.executor.memoryOverhead</td><td>executor内存10%</td><td>每个executor的堆外内存大小，用来存放诸如常量字符串等东西</td></tr><tr><td>spark.yarn.driver.memoryOverhead</td><td>driver内存7%</td><td>同上</td></tr><tr><td>spark.yarn.am.memoryOverhead</td><td>AM内存7%</td><td>同上</td></tr><tr><td>spark.yarn.am.port</td><td>随机</td><td>application master端口</td></tr><tr><td>spark.yarn.jar</td><td>无</td><td>spark jar文件的位置</td></tr><tr><td>spark.yarn.access.namenodes</td><td>无</td><td>spark作业能访问的hdfs namenode地址</td></tr><tr><td>spark.yarn.containerLauncherMaxThreads</td><td>25</td><td>application master能用来启动executor container的最大线程数量</td></tr><tr><td>spark.yarn.am.extraJavaOptions</td><td>无</td><td>application master的jvm参数</td></tr><tr><td>spark.yarn.am.extraLibraryPath</td><td>无</td><td>application master的额外库路径</td></tr><tr><td>spark.yarn.maxAppAttempts</td><td></td><td>提交spark作业最大的尝试次数</td></tr><tr><td>spark.yarn.submit.waitAppCompletion</td><td>true</td><td>cluster模式下，client是否等到作业运行完再退出</td></tr></tbody></table><h3 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h3><p>在yarn模式下，spark作业运行相关的executor和ApplicationMaster都是运行在yarn的container中的, 所以日志也是分散的, 有下面几种方式查看日志</p><h4 id="聚合日志-推荐-常用"><a href="#聚合日志-推荐-常用" class="headerlink" title="聚合日志(推荐,常用)"></a>聚合日志(推荐,常用)</h4><p>这种方式将散落在集群中各个机器上的日志，最后都聚合起来，container的日志会拷贝到hdfs上去，并从机器中删除,方便统一查看.<br><strong>开启:</strong><br>开启日志聚合的选项，即 <code>yarn.log-aggregation-enable</code>，<br><strong>查看:</strong><br>使用 <code>yarn logs -applicationId &lt;app ID&gt;</code>命令查看日志</p><p>yarn logs命令，会打印出 application 对应的所有 container 的日志出来，因为日志是在hdfs上的，也可以通过hdfs的命令行来直接从hdfs中查看日志,日志在hdfs中的目录，可以通过查看yarn.nodemanager.remote-app-log-diryarn.nodemanager.remote-app-log-dir-suffix属性来获知</p><h4 id="web-ui-方式"><a href="#web-ui-方式" class="headerlink" title="web ui 方式"></a>web ui 方式</h4><p>需要启动History Server，让spark history server 和 mapreduce history server运行着,并且在yarn-site.xml文件中，配置yarn.log.server.url属性.</p><p>spark history server web ui中的log url，会重定向到mapreduce history server上去查看日志</p><h4 id="分散查看（通常不推荐）"><a href="#分散查看（通常不推荐）" class="headerlink" title="分散查看（通常不推荐）"></a>分散查看（通常不推荐）</h4><p>如果没有打开聚合日志选项，那么日志默认就是散落在各个机器上的本次磁盘目录中的，在YARN_APP_LOGS_DIR目录下,根据hadoop版本的不同，通常在/tmp/logs目录下，或者$HADOOP_HOME/logs/userlogs目录下<br>如果要查看某个container的日志，那么就得登录到那台机器上去，然后到指定的目录下去，找到那个日志文件，就才能查看.</p><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  --master yarn-cluster \ </span><br><span class="line">    --num-executors 1 \</span><br><span class="line">    --driver-memory 100m \</span><br><span class="line">    --executor-memory 100m \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue hadoop队列 \</span><br><span class="line">    &#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;wordcount.py file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;test.txt</span><br><span class="line"></span><br><span class="line">--queue : 不同的大数据项目，共用同一个yarn集群，运行spark作业推荐一定要用--queue，指定不同的hadoop队列，做项目之间的队列隔离</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 算子调优方法</title>
      <link href="/2018-01-22-spark-functions-optimization.html"/>
      <url>/2018-01-22-spark-functions-optimization.html</url>
      
        <content type="html"><![CDATA[<p>记录一些关于 spark 算子使用的优化方法</p><a id="more"></a><h3 id="MapPartitions提升Map类操作性能"><a href="#MapPartitions提升Map类操作性能" class="headerlink" title="MapPartitions提升Map类操作性能"></a>MapPartitions提升Map类操作性能</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>mapPartition 类似 map, 不同之处在于 map 算子, 一次就处理一个 partition 中的一条数据,<br>而 mapPartitions 算子, 一次处理一个 partition 中所有的数据</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>如果是普通的map,比如一个 partition 中有 1万 条数据;那么算子函数要执行1万次<br>但是,如果使用 MapPartitions 操作之后,一个 task 仅仅会执行一次函数,函数一次接收所有的 partition 数据,只要执行一次就可以了,性能比较高.</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>如果是普通的 map 操作,一次函数执行只处理一条数据,假设处理 100 万 条数据,处理到10万的时候,内存不够了,可以将已经处理完的 N<br>条数据从内存里面垃圾回收掉,或者用其他方法腾出空间.<br>所以普通的 map 操作,通常不会导致内存的 OOM 异常</p><p>但是 MapPartitions 操作,对于大量数据来说,比如一个 partition 有 100 万条数据,一次传入一个函数后,内存可能一下子就不够了,但是又没有办法<br>腾出内存空间来,那么就会 OOM 内存溢出</p><h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><p>数据量不是特别大的时候,推荐使用 MapPartitions 替换 map 操作,但是如果出现了 OOM , 就不能用了 </p><h3 id="foreachPartition-优化写数据库性能"><a href="#foreachPartition-优化写数据库性能" class="headerlink" title="foreachPartition 优化写数据库性能"></a>foreachPartition 优化写数据库性能</h3><h4 id="foreach写库的性能缺陷"><a href="#foreach写库的性能缺陷" class="headerlink" title="foreach写库的性能缺陷"></a>foreach写库的性能缺陷</h4><p>task 为每个数据都去执行一次函数(写数据库),如果有一个 partition 有100万条数据,每个数据,都去创建一个数据库连接的话,那么就得创建 100万 次数据库连接</p><p>数据库连接的创建和销毁都是非常消耗性能的,即使用了数据库连接池,只是创建了固定数量的数据库连接</p><p>还需要多次通过数据库连接发送sql语句,如果有100万条数据,就需要发送100万次sql语句,同样很消耗性能</p><h4 id="使用-foreachPartition-算子优化"><a href="#使用-foreachPartition-算子优化" class="headerlink" title="使用 foreachPartition 算子优化"></a>使用 foreachPartition 算子优化</h4><ol><li>对于每个算子函数,就调用一次,一次传入一个 partition 所有的数据</li><li>主要创建或者获取一个数据库连接就可以</li><li>只要向数据库发送一次 sql 语句和多组参数即可</li></ol><p>在实际生产环境中,都是使用 foreachPartition 操作,但是有一个问题与 mapPartitions 操作一样,如果一个 partition 特别大,可能会出现 OOM 问题</p><h3 id="filter之后使用-coalesce-减少分区数量"><a href="#filter之后使用-coalesce-减少分区数量" class="headerlink" title="filter之后使用 coalesce 减少分区数量"></a>filter之后使用 coalesce 减少分区数量</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>默认情况下,经过了 filter 操作后, RDD 每个 partition 的数据量,可能都不太一样了,可能出现问题:  </p><ol><li>每个 partition 数据量变少了,但是在后面进行处理的时候,还是要跟 partition 数量一样数量的task,来进行处理,有点浪费 task 计算资源</li><li>每个 partition 数据量不一样,会导致后面的每个 task 处理每个 partition 的时候,每个 task 要处理的数据量不同,这个时候很容易发生数据倾斜</li></ol><p>比如说,第二个 partition 的数据量才 100, 但是第三个 partition 的数据量是 900;那么在后面的 task 处理逻辑一样的情况下,不同的 task 要处理的<br>数据量可能差别达到了 9 倍,同样也就导致了速度差别 9 倍,这样,就会导致有些 task 运行的速度很快,有些 task 运行的速度很慢,在进行某些操作的时候,还会造成数据倾斜.</p><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><ol><li><p>针对第一个问题,可以进行 partition 压缩,因为数量变少了,那么 partition 其实完全可以对应的变少.比如原来是 4个 partition ,现在完全可以<br>变成 2 个 partition,那么就只要用到后面 2 个 task 来处理即可,就不会造成 task 计算资源浪费(不必要针对只有一点点数据的 partition,还去启动一<br>个 task 来计算)</p></li><li><p>第二个问题,解决方案和第一个问题是一样的也是去压缩 partition, 尽量让每个 partition 的数据量差不多.</p></li></ol><h4 id="coalesce-算子压缩-partition"><a href="#coalesce-算子压缩-partition" class="headerlink" title="coalesce() 算子压缩 partition"></a>coalesce() 算子压缩 partition</h4><p>在 filter 操作之后,针对每个 partition 的数据量各不相同的情况,来压缩 partition 的数量,减少 partition 的数量,而且让每个 partition 的数据量都尽量均匀紧凑.</p><h3 id="使用-repartition-解决sparksql低并行度的问题"><a href="#使用-repartition-解决sparksql低并行度的问题" class="headerlink" title="使用 repartition 解决sparksql低并行度的问题"></a>使用 repartition 解决sparksql低并行度的问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>如果没有使用 Spark SQL(DataFrame),那么整个 spark application 默认所有 stage 的并行度都是设置的那个参数(除非使用 coalesce 算子缩减过 partition 数量)</p><p>Spark SQL 的 stage 的并行度不能自己设定, Spark SQL 会默认根据 hive 表对应的 hdfs 文件的 block,自动设置 Spark SQL 查询所在的那个<br>stage 的并行度.  </p><p>我们自己通过 spark.default.parallelism 参数指定的并行度,只会在没有 spark sql 的 stage 生效</p><p>比如第一个stage, 用了 spark sql 从 hive 表中查询出了一些数据,然后做了一些 transformation 操作,接着做了一个 shuffle 操作(groupByKey),<br>下一个 stage,在 shuffle 操作之后,做了一些 transformation 操作.  </p><p>hive 表,对应了一个 hdfs 文件, 有 20 个 block ,自己设定的spark.default.parallelism 参数为 100. </p><p>事实上,第一个 stage 的并行度,是不受设置参数控制的,和 block 的数量相同,只有 20 个 task , 第二个 stage,才会根据设置的并行度 100 去执行</p><p>这种情况导致第一个 stage 的速度特别慢,第二个 stage 特别快</p><h4 id="优化-1"><a href="#优化-1" class="headerlink" title="优化"></a>优化</h4><p>为了解决 Spark SQL 无法设置并行度和task数量,可以使用 repratition 算子.</p><p>可以将用 Spark SQL 查询出来的 RDD ,使用 reparitition 算子,去重新分区,此时可以分区成多个 partition.  </p><p>比如从 20 个 partition 分区成 100 个<br>然后,从 repartititon 以后的 RDD ,并行度和 task 数量, 就会按照预期的进行,就可以避免跟 spark sql 绑定在一个 stage 中的算子,只能使用少<br>量的 task 去处理大数据以及复杂的算法逻辑</p><p>并行度的设置一般用两种方法:</p><ol><li>设置参数: spark.default.parallelism</li><li>读取数据时,比如 textfile(‘/path/xx.txt’, 100)传入第二个参数,指定partition数量(比较少用)</li></ol><p>官方推荐,根据总 cpu core,手动设置 spark.default.parallelism 参赛,指定为 cpucore 总数的 2~3 倍</p><h3 id="reduceByKey本地聚合"><a href="#reduceByKey本地聚合" class="headerlink" title="reduceByKey本地聚合"></a>reduceByKey本地聚合</h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>reduceByKey,相较于普通的 shuffle 操作,它的一个特点就是会进行 map 端的本地聚合<br>对 map 端给下个 stage 每个 task 创建的输出文件中,写数据之前,就会进行本地的 combiner 操作,也就是说对每一个 key,对应的values,都会执行算子函数</p><h4 id="对性能的提升"><a href="#对性能的提升" class="headerlink" title="对性能的提升"></a>对性能的提升</h4><ol><li>在进行本地聚合以后,在 map 端的数据量就变少了,减少磁盘 IO. 而且可以减少磁盘空间的占用</li><li>下一个 stage ,拉取数据的量,也就变少了,减少网络的数据传输的性能消耗</li><li>在 reduce 端进行数据缓存的内存占用就变少了</li><li>reduce 端,要进行聚合的数据量也变少了</li></ol><h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><ol><li>对于非常普通的,比如说,就是实现类似于 wordcount 程序一样,对每个 key 对应的值,进行某种数据公式或者算法的计算(累加,累乘)</li><li>对于一些类似于要对每个 key 进行一些字符串拼接的这种较为复杂的操作,可以自己衡量一下,最好用 reduceByKey 实现.<br>(shuffle 基本上占了整个 spark 作业 90% 的性能消耗,只要能对 shuffle 进行一定的调优,都是有价值的)</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 性能调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Standalone 运行模式</title>
      <link href="/2018-01-17-spark-standalone.html"/>
      <url>/2018-01-17-spark-standalone.html</url>
      
        <content type="html"><![CDATA[<p>Standalone 模式是使用 spark 自带的集群管理器管理节点的方式,相比 Yarn 模式, standalone 模式不依赖 Hadoop 环境.</p><a id="more"></a><h3 id="standalone模式启动集群命令详解"><a href="#standalone模式启动集群命令详解" class="headerlink" title="standalone模式启动集群命令详解"></a>standalone模式启动集群命令详解</h3><h4 id="直接启动-master、worker-集群"><a href="#直接启动-master、worker-集群" class="headerlink" title="直接启动 master、worker 集群"></a>直接启动 master、worker 集群</h4><p>使用 <code>sbin/start-all.sh</code></p><h4 id="单独启动-master、worker-集群"><a href="#单独启动-master、worker-集群" class="headerlink" title="单独启动  master、worker 集群"></a>单独启动  master、worker 集群</h4><p>大多数时候,集群中各个节点的机器的核数,内存等资源是不同的,不方便使用 <code>sbin/start-all.sh</code> 直接启动, 这时候,就可以使用命令行单独启动 master 和 worker 节点.</p><p>单独分别启动master和worker进程必须先启动master进程，再启动worker进程，因为worker进程启动后，需要向 master 进程去注册,反过来先启动 worker 进程，再启动 master 进程，可能会有问题.</p><h5 id="启动-master-进程"><a href="#启动-master-进程" class="headerlink" title="启动 master 进程"></a>启动 master 进程</h5><p>使用<code>sbin/start-master.sh</code>启动.</p><p>master 启动后日志会打印出master 的URL地址和集群监控webUI地址, 形如<code>spark://HOST:PORT URL</code>和 <code>http://MASTER_HOST:8081</code></p><h5 id="启动worker进程"><a href="#启动worker进程" class="headerlink" title="启动worker进程"></a>启动worker进程</h5><p>使用 <code>sbin/start-slave.sh &lt;master-spark-URL&gt;</code> 启动</p><p>启动后，访问 <code>http://MASTER_HOST:8082</code>,就可以看见新启动的worker节点，包括该节点的cpu和内存资源等信息</p><h5 id="命令行可用的参数"><a href="#命令行可用的参数" class="headerlink" title="命令行可用的参数"></a>命令行可用的参数</h5><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>-h HOST, –host HOST</td><td>在哪台机器上启动，默认就是本机</td></tr><tr><td>-p PORT, –port PORT</td><td>对外提供服务的端口, master默认是7077，worker默认是随机的</td></tr><tr><td>–webui-port PORT</td><td>web ui的端口，master默认是8080，worker默认是8081</td></tr><tr><td>-c CORES, –cores CORES</td><td>仅限于 worker，总共能让spark作业使用多少个cpu core，默认是当前机器上所有的 cpu core</td></tr><tr><td>-m MEM, –memory MEM</td><td>仅限于worker，总共能让spark作业使用多少内存，是100M或者1G这样的格式，默认是1g</td></tr><tr><td>-d DIR, –work-dir DIR</td><td>仅限于worker，工作目录，默认是SPARK_HOME/work目录</td></tr><tr><td>–properties-file FILE</td><td>master和worker加载默认配置文件的地址，默认是conf/spark-defaults.conf</td></tr></tbody></table><h3 id="配置spark-env-sh说明"><a href="#配置spark-env-sh说明" class="headerlink" title="配置spark-env.sh说明"></a>配置spark-env.sh说明</h3><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>SPARK_MASTER_IP</td><td>指定master进程所在的机器的ip地址</td></tr><tr><td>SPARK_MASTER_PORT</td><td>指定master监听的端口号（默认是7077）</td></tr><tr><td>SPARK_MASTER_WEBUI_PORT</td><td>指定master web ui的端口号（默认是8080</td></tr><tr><td>SPARK_LOCAL_DIRS</td><td>spark的工作目录，包括了shufflemap输出文件，以及持久化到磁盘的RDD等</td></tr><tr><td>SPARK_WORKER_PORT</td><td>worker节点的端口号，默认是随机的</td></tr><tr><td>SPARK_WORKER_WEBUI_PORT</td><td>worker节点的web ui端口号，默认是8081</td></tr><tr><td>SPARK_WORKER_CORES</td><td>worker节点上，允许spark作业使用的最大cpu数量，默认是机器上所有的cpu core</td></tr><tr><td>SPARK_WORKER_MEMORY</td><td>worker节点上，允许spark作业使用的最大内存量，格式为1000m，2g等，默认最小是1g内存</td></tr><tr><td>SPARK_WORKER_INSTANCES</td><td>当前机器上的worker进程数量，默认是1，可以设置成多个，但是这时一定要设置SPARK_WORKER_CORES，限制每个worker的cpu数量</td></tr><tr><td>SPARK_WORKER_DIR</td><td>spark作业的工作目录，包括了作业的日志等，默认是spark_home/work</td></tr><tr><td>SPARK_WORKER_OPTS</td><td>worker的额外参数，使用”-Dx=y”设置各个参数</td></tr><tr><td>SPARK_DAEMON_MEMORY</td><td>分配给master和worker进程自己本身的内存，默认是1g</td></tr><tr><td>SPARK_PUBLISC_DNS</td><td>master和worker的公共dns域名，默认是没有的</td></tr></tbody></table><p><strong>有三个配置项是用于配置额外参数的:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_OPTS:</span><br><span class="line">    设置master的额外参数，使用&quot;-Dx&#x3D;y&quot;设置各个参数</span><br><span class="line">    比如 export SPARK_MASTER_OPTS&#x3D;&quot;-Dspark.deploy.defaultCores&#x3D;1&quot;</span><br><span class="line">    可用的配置项:</span><br><span class="line">    spark.deploy.retainedApplications200  在sparkwebui上最多显示多少个application的信息</span><br><span class="line">    spark.deploy.retainedDrivers200  在spark web ui上最多显示多少个driver的信息</span><br><span class="line">    spark.deploy.spreadOut           true   资源调度策略，spreadOut会尽量将application的executor进程分布在更多worker上，适合基于hdfs文件计算的情况，提升数据本地化概率；非spreadOut会尽量将executor分配到一个worker上，适合计算密集型的作业</span><br><span class="line">    spark.deploy.defaultCores   无限大  每个spark作业最多在standalone集群中使用多少个cpu core，默认是无限大，有多少用多少</span><br><span class="line">    spark.deploy.timeout   60     单位秒，一个worker多少时间没有响应之后，master认为worker挂掉了</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">SPARK_WORKER_OPTS:</span><br><span class="line">    worker的额外参数，使用&quot;-Dx&#x3D;y&quot;设置各个参数</span><br><span class="line">    spark.worker.cleanup.enabled   false           是否启动自动清理worker工作目录，默认是false</span><br><span class="line">    spark.worker.cleanup.interval   1800           单位秒，自动清理的时间间隔，默认是30分钟</span><br><span class="line">    spark.worker.cleanup.appDataTtl   7 * 24 * 3600   默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SPARK_DAEMON_JAVA_OPTS:</span><br><span class="line">    设置master和worker自己的jvm参数，使用&quot;-Dx&#x3D;y&quot;设置各个参数</span><br></pre></td></tr></table></figure><h3 id="三种模式提交-spark-作业"><a href="#三种模式提交-spark-作业" class="headerlink" title="三种模式提交 spark 作业"></a>三种模式提交 spark 作业</h3><p>spark standalone 有三种提交的 spark 作业的模式,分别是 local,client 和 cluster 模式.</p><p>local 模式用于本地开发,为了方便测试和调试<br>client 模式用于测试环境, 为了方便看日志<br>cluster 模式用于生产环境, 为了解决 driver 单点故障问题</p><h4 id="local-模式"><a href="#local-模式" class="headerlink" title="local 模式"></a>local 模式</h4><p>local 模式下, 没有 master+worke ,相当于启动一个本地进程，然后在一个进程内模拟spark集群中作业的运行.<br>一个spark作业，就对应了进程中的一个或多个executor线程,开始执行作业的调度，task分配等等操作. </p><p>在实际工作当中，local模式，主要用于本地开发测试,通常是手动生成一份数据去使用</p><p>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--driver-memory 100m \</span><br><span class="line">--executor-memory 100m \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;wordcount.py file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;test.txt</span><br></pre></td></tr></table></figure><h4 id="client-模式"><a href="#client-模式" class="headerlink" title="client 模式"></a>client 模式</h4><p>client 模式下，提交作业后，driver在本机启动，可以实时看到详细的日志信息，方便追踪和排查错误.</p><p>standalone模式提交，唯一与local区别就是要设置master的地址,有三种方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、硬编码: SparkConf.setMaster(&quot;spark:&#x2F;&#x2F;IP:PORT&quot;)</span><br><span class="line">2、spark-submit: --master spark:&#x2F;&#x2F;IP:PORT  (最合适)</span><br><span class="line">3、spark-shell: --master spark:&#x2F;&#x2F;IP:PORT</span><br></pre></td></tr></table></figure><p>提交模式默认是 client,也可以用 <code>--deploy-mode client</code> 配置项声明.</p><h4 id="cluster-模式"><a href="#cluster-模式" class="headerlink" title="cluster 模式"></a>cluster 模式</h4><p>cluster 模式下, 向 master 提交申请后, 会在 worker 节点中选择一个节点作为 driver 节点,可以在 web ui 中观察到启动的 driver.</p><p>standalone cluster模式，还支持监控driver进程，并且在driver挂掉的时候，自动重启该进程<br>要使用这个功能，在spark-submit脚本中，使用–supervise标识即可</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速区分 spark 四种关联方式(结论适用于 sql)</title>
      <link href="/2018-01-09-spark-and-sql-join-pattern.html"/>
      <url>/2018-01-09-spark-and-sql-join-pattern.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>关联操作是 spark 或 sql 中常用的操作, 这里以 spark 为例,区别四种不同的 join 方式</p></blockquote><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local&quot;)</span><br><span class="line"></span><br><span class="line">x &#x3D; sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</span><br><span class="line">y &#x3D; sc.parallelize([(&quot;a&quot;, 2), (&quot;a&quot;, 3), (&quot;c&quot;, 5)])</span><br><span class="line"></span><br><span class="line">print(&quot;join:&quot;, sorted(x.join(y).collect()))</span><br><span class="line">print(&quot;leftOuterJoin:&quot;, sorted(x.leftOuterJoin(y).collect()))</span><br><span class="line">print(&quot;rightOuterJoin:&quot;, sorted(x.rightOuterJoin(y).collect()))</span><br><span class="line">print(&quot;fullOuterJoin:&quot;, sorted(x.fullOuterJoin(y).collect()))</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">          join: [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3))]              </span><br><span class="line"> leftOuterJoin: [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3)), (&#39;b&#39;, (4, None))]</span><br><span class="line">rightOuterJoin: [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3)), (&#39;c&#39;, (None, 5))]</span><br><span class="line"> fullOuterJoin: [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3)), (&#39;b&#39;, (4, None)), (&#39;c&#39;, (None, 5))]</span><br></pre></td></tr></table></figure><p>结论:</p><ul><li>join (INNER JOIN)：如果两个rdd(sql表)中的数据有至少一个匹配，则返回行</li><li>leftOuterJoin (LEFT JOIN)：以左边的 rdd (左sql表) 为基准, 返回所有匹配的值,不能匹配的用 None 填充</li><li>rightOuterJoin (RIGHT JOIN)：以右边的 rdd (右sql表) 为基准, 返回所有匹配的值,不能匹配的用 None 填充</li><li>fullOuterJoin (FULL JOIN)：返回左右两个 rdd (两个 sql表) 所有的行, 不匹配的用 None 填充</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD 的 窄依赖和宽依赖(以 wordcount 程序为例)</title>
      <link href="/2018-01-01-spark-rdd-dependence.html"/>
      <url>/2018-01-01-spark-rdd-dependence.html</url>
      
        <content type="html"><![CDATA[<p>以 wordcount 为例解释窄依赖和宽依赖</p><a id="more"></a><h3 id="窄依赖-Narrow-Dependence"><a href="#窄依赖-Narrow-Dependence" class="headerlink" title="窄依赖(Narrow Dependence)"></a>窄依赖(Narrow Dependence)</h3><p>如果一个 RDD 与父 RDD 只有简单的一对一的依赖关系,具体的说,RDD 的每个 partition 仅仅依赖于父 RDD 中的一个 partition, 这种情况下,RDD 之间的关系称为窄依赖</p><h3 id="宽依赖-Shuffle-Dependence"><a href="#宽依赖-Shuffle-Dependence" class="headerlink" title="宽依赖(Shuffle Dependence)"></a>宽依赖(Shuffle Dependence)</h3><p>如果 RDD 中的每个 partition 有可能来自于父 RDD 的一个或多个 partition,这种操作称为 Shuffle 操作,这种情况下,RDD 之间的关系称为宽依赖</p><h3 id="wordcount-解析"><a href="#wordcount-解析" class="headerlink" title="wordcount 解析"></a>wordcount 解析</h3><p><img src="/images/spark/rdd-dependence.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Spark 原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 读取和转存数据(python)</title>
      <link href="/2018-01-01-pyspark-read-write.html"/>
      <url>/2018-01-01-pyspark-read-write.html</url>
      
        <content type="html"><![CDATA[<p>介绍 spark core， spark sql 和 spark streaming 读取多种数据源及数据转存</p><a id="more"></a><h3 id="spark-rdd-读取和转存数据"><a href="#spark-rdd-读取和转存数据" class="headerlink" title="spark rdd 读取和转存数据"></a>spark rdd 读取和转存数据</h3><h4 id="通过集合创建rdd-测试用"><a href="#通过集合创建rdd-测试用" class="headerlink" title="通过集合创建rdd(测试用)"></a>通过集合创建rdd(测试用)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">list &#x3D; [&quot;hadoop&quot;, &quot;spark&quot;, &quot;hive&quot;]</span><br><span class="line">rdd &#x3D;  sc.parallelize(list)</span><br></pre></td></tr></table></figure><h4 id="本地文件系统读写"><a href="#本地文件系统读写" class="headerlink" title="本地文件系统读写"></a>本地文件系统读写</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 读文件&#x2F;目录</span><br><span class="line">rdd &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;word.txt&quot;)</span><br><span class="line"># 存文件</span><br><span class="line">rdd.saveAsTextFile(&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;word_bak.txt&quot;)</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>注意存文件的时候, <code>wrod_bak.txt</code> 并不是文件而是一个文件夹  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; ls &#x2F;home&#x2F;zj&#x2F;word_bak.txt</span><br><span class="line">part-00000</span><br><span class="line">_SUCCESS</span><br></pre></td></tr></table></figure><p>part-00000 中就是存数据的文件</p><h4 id="分布式文件系统HDFS的数据读写"><a href="#分布式文件系统HDFS的数据读写" class="headerlink" title="分布式文件系统HDFS的数据读写"></a>分布式文件系统HDFS的数据读写</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 上传文件到 hdfs</span><br><span class="line">shell&gt; hdfs dfs -put &#x2F;home&#x2F;zj&#x2F;word.txt &#x2F;user&#x2F;zj&#x2F;word.txt</span><br><span class="line"># 读文件&#x2F;目录</span><br><span class="line">rdd &#x3D; sc.textFile(&quot;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;user&#x2F;zj&#x2F;word.txt&quot;)</span><br><span class="line">rdd &#x3D; sc.textFile(&quot;&#x2F;user&#x2F;zj&#x2F;word.txt&quot;)</span><br><span class="line"># 存文件</span><br><span class="line">rdd.saveAsTextFile(&quot;word_bak.txt&quot;)</span><br></pre></td></tr></table></figure><p>同样的,这里存的 <code>word_bak.txt</code> 也是一个目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; hsfs dfs -ls &#x2F;user&#x2F;zj&#x2F;word_bak.txt</span><br><span class="line">part-00000</span><br><span class="line">_SUCCESS</span><br></pre></td></tr></table></figure><h4 id="读取-json-格式文件"><a href="#读取-json-格式文件" class="headerlink" title="读取 json 格式文件"></a>读取 json 格式文件</h4><p>需要使用map操作对读取到的json文件内容进行转换  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;home&#x2F;zjprople.json</span><br><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">import json</span><br><span class="line">sc &#x3D; SparkContext(&#39;local&#39;,&#39;json&#39;)</span><br><span class="line">inputFile &#x3D;  &quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;people.json&quot;</span><br><span class="line">jsonStrs &#x3D; sc.textFile(inputFile)</span><br><span class="line">result &#x3D; jsonStrs.map(lambda s : json.loads(s))</span><br><span class="line">result.foreach(print)</span><br></pre></td></tr></table></figure><h3 id="Spark-Sql-读取和转存数据"><a href="#Spark-Sql-读取和转存数据" class="headerlink" title="Spark Sql 读取和转存数据"></a>Spark Sql 读取和转存数据</h3><h4 id="读取本地-json-文件"><a href="#读取本地-json-文件" class="headerlink" title="读取本地 json 文件"></a>读取本地 json 文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark&#x3D;SparkSession.builder.getOrCreate()</span><br><span class="line">df &#x3D; spark.read.json(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.json&quot;)</span><br></pre></td></tr></table></figure><h4 id="从-RDD-转换到-DataFrame"><a href="#从-RDD-转换到-DataFrame" class="headerlink" title="从 RDD 转换到 DataFrame"></a>从 RDD 转换到 DataFrame</h4><h5 id="利用反射机制推断-RDD-模式"><a href="#利用反射机制推断-RDD-模式" class="headerlink" title="利用反射机制推断 RDD 模式"></a>利用反射机制推断 RDD 模式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import Row</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;reflect rdd to dataFrame&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc &#x3D; spark.sparkContext</span><br><span class="line"># Michael, 29</span><br><span class="line"># Andy, 30</span><br><span class="line"># Justin, 19</span><br><span class="line">lines &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.txt&quot;)</span><br><span class="line">parts &#x3D; lines.map(lambda l: l.split(&quot;,&quot;))</span><br><span class="line">people &#x3D; parts.map(lambda p: Row(name&#x3D;p[0], age&#x3D;int(p[1])))</span><br><span class="line"></span><br><span class="line">schemaPeople &#x3D; spark.createDataFrame(people)</span><br><span class="line">schemaPeople.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">teenagers &#x3D; spark.sql(&quot;SELECT name FROM people WHERE age &gt;&#x3D; 13 AND age &lt;&#x3D;19&quot;)</span><br><span class="line"></span><br><span class="line">teenNames &#x3D; teenagers.rdd.map(lambda p: &quot;Name: &quot; + p.name).collect()</span><br><span class="line">for name in teenNames:</span><br><span class="line">    print(name)</span><br></pre></td></tr></table></figure><h5 id="以编程的方式指定Schema"><a href="#以编程的方式指定Schema" class="headerlink" title="以编程的方式指定Schema"></a>以编程的方式指定Schema</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import StructField, StructType, StringType</span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder\</span><br><span class="line">    .appName(&quot;coding_rdd&quot;)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">sc &#x3D; spark.sparkContext</span><br><span class="line"></span><br><span class="line">lines &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.txt&quot;)</span><br><span class="line">parts &#x3D; lines.map(lambda l: l.split(&quot;,&quot;))</span><br><span class="line">people &#x3D; parts.map(lambda p: (p[0], p[1].strip()))</span><br><span class="line"></span><br><span class="line"># 定义 schema</span><br><span class="line">schemaString &#x3D; &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">fields &#x3D; [StructField(field_name, StringType(), True) for field_name in schemaString.split()]</span><br><span class="line">schema &#x3D; StructType(fields)</span><br><span class="line"></span><br><span class="line">schemaPeople &#x3D; spark.createDataFrame(people, schema)</span><br><span class="line"># 必须注册为临时表才能供下面查询使用</span><br><span class="line">schemaPeople.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line">results &#x3D; spark.sql(&quot;SELECT name FROM people&quot;)</span><br><span class="line">results.show()</span><br></pre></td></tr></table></figure><h4 id="转存-dataFrame-为格式化文件"><a href="#转存-dataFrame-为格式化文件" class="headerlink" title="转存 dataFrame 为格式化文件"></a>转存 dataFrame 为格式化文件</h4><h5 id="直接转为格式化文件"><a href="#直接转为格式化文件" class="headerlink" title="直接转为格式化文件"></a>直接转为格式化文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">peopleDF &#x3D; spark.read.format(&quot;json&quot;).load(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;people.json&quot;)</span><br><span class="line"># 转为为 csv 文件</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;csv&quot;).save(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;newpeople.csv&quot;)</span><br></pre></td></tr></table></figure><h5 id="先转为-rdd-再转为文件"><a href="#先转为-rdd-再转为文件" class="headerlink" title="先转为 rdd, 再转为文件"></a>先转为 rdd, 再转为文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.rdd.saveAsTextFile(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;newpeople.txt&quot;)</span><br></pre></td></tr></table></figure><h4 id="读写-parquet-文件"><a href="#读写-parquet-文件" class="headerlink" title="读写 parquet 文件"></a>读写 parquet 文件</h4><h5 id="读取-parquet-文件"><a href="#读取-parquet-文件" class="headerlink" title="读取 parquet 文件"></a>读取 parquet 文件</h5><p>Parquet 是许多其他数据处理系统支持的 columnar format （列式存储）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parquetFileDF &#x3D; spark.read.parquet(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;users.parquet&quot;</span><br><span class="line">parquetFileDF.createOrReplaceTempView(&quot;parquetFile&quot;)</span><br><span class="line">namesDF &#x3D; spark.sql(&quot;SELECT * FROM parquetFile&quot;)</span><br><span class="line">namesDF.rdd.foreach(lambda person: print(person.name))</span><br></pre></td></tr></table></figure><h5 id="转存-parquet-文件"><a href="#转存-parquet-文件" class="headerlink" title="转存 parquet 文件"></a>转存 parquet 文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.write.parquet(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;newpeople.parquet&quot;)</span><br></pre></td></tr></table></figure><h5 id="重新加载转存的-parquet-文件"><a href="#重新加载转存的-parquet-文件" class="headerlink" title="重新加载转存的 parquet 文件"></a>重新加载转存的 parquet 文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF &#x3D; spark.read.parquet(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;myCode&#x2F;people.parquet&quot;)</span><br></pre></td></tr></table></figure><h4 id="通过-jdbc-读写-mysql"><a href="#通过-jdbc-读写-mysql" class="headerlink" title="通过 jdbc 读写 mysql"></a>通过 jdbc 读写 mysql</h4><h5 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h5><p>使用 jdbc 连接 mysql 必须要导入需要的ja包, 比如 mysql-connector-java-5.1.46.jar</p><ol><li>下载地址: <a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a></li><li>下载后解压到指定目录 /usr/local/spark-2.2.1/jars/</li><li>编辑 spark 根目录下的 /conf/spark-env.sh, 把目标jar包写入环境变量 SPARK_DIST_CLASSPATH<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath):&#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;jars&#x2F;mysql-connector-java-5.1.46&#x2F;mysql-connector-java-5.1.46-bin.jar</span><br></pre></td></tr></table></figure><h5 id="MYSQL-环境示例准备"><a href="#MYSQL-环境示例准备" class="headerlink" title="MYSQL 环境示例准备"></a>MYSQL 环境示例准备</h5></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">MySQL [none]&gt; create database spark;</span><br><span class="line">MySQL [none]&gt;  use spark;</span><br><span class="line">MySQL [spark]&gt; create table people(id int unsigned auto_increment primary key, name char(20), age int(4);</span><br><span class="line">MySQL [spark]&gt; insert into people (name, age) values  (&quot;Mchael&quot;, 29),(&quot;Andy&quot;, 30),(&quot;Justn&quot;, 19);</span><br><span class="line">MySQL [spark]&gt; select * from people;</span><br><span class="line">+----+--------+------+</span><br><span class="line">| id | name   | age  |</span><br><span class="line">+----+--------+------+</span><br><span class="line">|  1 | Mchael |   29 |</span><br><span class="line">|  2 | Andy   |   30 |</span><br><span class="line">|  3 | Justn  |   19 |</span><br><span class="line">+----+--------+------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="spark-sql-从-mysql-读取数据"><a href="#spark-sql-从-mysql-读取数据" class="headerlink" title="spark sql 从 mysql 读取数据"></a>spark sql 从 mysql 读取数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder\</span><br><span class="line">    .appName(&quot;coding_rdd&quot;)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">jdbcDF &#x3D; spark.read \</span><br><span class="line">    .format(&quot;jdbc&quot;) \</span><br><span class="line">    .option(&quot;url&quot;, &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spark&quot;) \</span><br><span class="line">    .option(&quot;dbtable&quot;, &quot;people&quot;) \</span><br><span class="line">    .option(&quot;user&quot;, &quot;root&quot;) \</span><br><span class="line">    .option(&quot;password&quot;, &quot;000000&quot;) \</span><br><span class="line">    .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) \</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">jdbcDF.show()</span><br></pre></td></tr></table></figure><p>执行: python3 datasource_jdbc.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+---+------+---+</span><br><span class="line">| id|  name|age|</span><br><span class="line">+---+------+---+</span><br><span class="line">|  1|Mchael| 29|</span><br><span class="line">|  2|  Andy| 30|</span><br><span class="line">|  3| Justn| 19|</span><br><span class="line">+---+------+---+</span><br></pre></td></tr></table></figure><p>如果不想在 /conf/spark-env.sh 中写入环境变量,也可以使用 spark-submit 指令指定 jar 包的地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --driver-class-path &#x2F;usr&#x2F;local&#x2F;spark-2.2.1&#x2F;jars&#x2F;mysql-connector-java-5.1.46&#x2F;mysql-connector-java-5.1.46-bin.jar datasource_jdbc.py</span><br></pre></td></tr></table></figure><h5 id="spark-sql-写数据到-mysql"><a href="#spark-sql-写数据到-mysql" class="headerlink" title="spark sql 写数据到 mysql"></a>spark sql 写数据到 mysql</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.types import Row</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession\</span><br><span class="line">    .builder\</span><br><span class="line">    .appName(&quot;write_DF_to_mysql&quot;)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc &#x3D; spark.sparkContext</span><br><span class="line"></span><br><span class="line"># mmysql 表id字段设置为 AUTO_INCREMENT, 这里就可以不填充 id 字段</span><br><span class="line">peoplelist &#x3D; [</span><br><span class="line">    &quot;zj 25&quot;,</span><br><span class="line">    &quot;kobe 41&quot;</span><br><span class="line">]</span><br><span class="line">peopleRDD &#x3D; sc.parallelize(peoplelist)</span><br><span class="line">people &#x3D; peopleRDD\</span><br><span class="line">    .map(lambda l: l.split())\</span><br><span class="line">    .map(lambda p: Row(name&#x3D;p[0], age&#x3D;p[1]))</span><br><span class="line"></span><br><span class="line">peopleDF &#x3D; spark.createDataFrame(people)</span><br><span class="line"></span><br><span class="line">peopleDF.write\</span><br><span class="line">    .format(&quot;jdbc&quot;)\</span><br><span class="line">    .option(&quot;url&quot;, &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spark&quot;)\</span><br><span class="line">    .option(&quot;dbtable&quot;, &quot;people&quot;) \</span><br><span class="line">    .option(&quot;user&quot;, &quot;root&quot;) \</span><br><span class="line">    .option(&quot;password&quot;, &quot;000000&quot;) \</span><br><span class="line">    .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) \</span><br><span class="line">    .mode(&quot;append&quot;)\</span><br><span class="line">    .save()</span><br><span class="line"></span><br><span class="line"># 第二种写法</span><br><span class="line"># prop &#x3D; &#123;</span><br><span class="line">#     &#39;user&#39;: &#39;root&#39;,</span><br><span class="line">#     &#39;password&#39;: &#39;000000&#39;,</span><br><span class="line">#     &#39;driver&#39;: &#39;com.mysql.jdbc.Driver&#39;</span><br><span class="line"># &#125;</span><br><span class="line"># peopleDF.write\</span><br><span class="line">#     .jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spark&quot;,&#39;people&#39;,&#39;append&#39;, prop)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MySQL [spark]&gt; select * from people;</span><br><span class="line">+----+--------+------+</span><br><span class="line">| id | name   | age  |</span><br><span class="line">+----+--------+------+</span><br><span class="line">|  1 | Mchael |   29 |</span><br><span class="line">|  2 | Andy   |   30 |</span><br><span class="line">|  3 | Justn  |   19 |</span><br><span class="line">|  4 | zj     |   25 |</span><br><span class="line">|  5 | kobe   |   41 |</span><br></pre></td></tr></table></figure><h3 id="spark-streaming-读取数据"><a href="#spark-streaming-读取数据" class="headerlink" title="spark streaming 读取数据"></a>spark streaming 读取数据</h3><h4 id="spark-streaming-基本步骤"><a href="#spark-streaming-基本步骤" class="headerlink" title="spark streaming 基本步骤"></a>spark streaming 基本步骤</h4><ol><li>定义输入源</li><li>通过对DStream应用转换操作和输出操作来定义流计算。</li><li>用streamingContext.start()来开始接收数据和处理流程。</li><li>通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。</li><li>可以通过streamingContext.stop()来手动结束流计算进程。</li></ol><h4 id="从文件流读取数据"><a href="#从文件流读取数据" class="headerlink" title="从文件流读取数据"></a>从文件流读取数据</h4><p>spark 可以读取一个文件或文件夹下的增量数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line"></span><br><span class="line">#local[*] 中必须设置大于1的并行数量</span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;streaming&quot;)</span><br><span class="line"># 设置每次计算的时间间隔</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 5)</span><br><span class="line">lines &#x3D; ssc.textFileStream(&#39;file:&#x2F;&#x2F;&#x2F;home&#x2F;zj&#x2F;logs&#39;)</span><br><span class="line">words &#x3D; lines.flatMap(lambda l: l.split())</span><br><span class="line">wordsPair &#x3D; words.map(lambda x: (x, 1))</span><br><span class="line">wordscount &#x3D; wordsPair.reduceByKey(lambda a, b: a + b)</span><br><span class="line">wordscount.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>执行后,向 <code>/home/zj/logs</code> 中添加一个新的包含内容的文件, spark 就可以实时读取到增量数据,进行计算</p><h4 id="从-socket流-中读取数据"><a href="#从-socket流-中读取数据" class="headerlink" title="从 socket流 中读取数据"></a>从 socket流 中读取数据</h4><p>Spark Streaming可以通过Socket端口监听并接收数据，然后进行相应处理。</p><p>先执行 <code>nc -lk 9999</code>, 然后再执行下面的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[2]&quot;, &quot;streaming_socket&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 10)</span><br><span class="line"></span><br><span class="line">lines &#x3D; ssc.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class="line">wordcount &#x3D; lines\</span><br><span class="line">    .flatMap(lambda l: l.split())\</span><br><span class="line">    .map(lambda w: (w, 1))\</span><br><span class="line">    .reduceByKey(lambda a, b: a + b)</span><br><span class="line"></span><br><span class="line">wordcount.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h4 id="使用-list-或-rdds-创建输入流-用于测试"><a href="#使用-list-或-rdds-创建输入流-用于测试" class="headerlink" title="使用 list 或 rdds 创建输入流(用于测试)"></a>使用 list 或 rdds 创建输入流(用于测试)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[4]&quot;, &quot;streaming_rdds&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, 1)</span><br><span class="line"></span><br><span class="line">queue &#x3D; []</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    queue +&#x3D; [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]</span><br><span class="line"></span><br><span class="line">wordcount &#x3D; ssc.queueStream(queue).map(lambda x: (x % 10, 1)).reduceByKey(lambda a, b: a + b)</span><br><span class="line">wordcount.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">time.sleep(10)</span><br><span class="line"></span><br><span class="line"># stopGracefully, 等待所有任务完成</span><br><span class="line">ssc.stop(stopSparkContext&#x3D;True, stopGraceFully&#x3D;True)</span><br></pre></td></tr></table></figure><h4 id="kafka-作为数据源"><a href="#kafka-作为数据源" class="headerlink" title="kafka 作为数据源"></a>kafka 作为数据源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">from pyspark.streaming import StreamingContext</span><br><span class="line">from pyspark.streaming.kafka import KafkaUtils</span><br><span class="line"></span><br><span class="line">sc &#x3D; SparkContext(&quot;local[4]&quot;, &quot;streaming-kafka&quot;)</span><br><span class="line">ssc &#x3D; StreamingContext(sc, batchDuration&#x3D;3)</span><br><span class="line"></span><br><span class="line">zkQuorum &#x3D; &quot;172.17.0.2:2181&quot;</span><br><span class="line">group_id &#x3D; &quot;group-5&quot;</span><br><span class="line">topics &#x3D; &#123;</span><br><span class="line">    &quot;test&quot;: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kafkaStream &#x3D; KafkaUtils.createStream(ssc, zkQuorum, group_id, topics)</span><br><span class="line"></span><br><span class="line">word_counts &#x3D; kafkaStream\</span><br><span class="line">    .map(lambda x: x[1])\</span><br><span class="line">    .flatMap(lambda line: line.split(&quot; &quot;))\</span><br><span class="line">    .map(lambda word: (word, 1))\</span><br><span class="line">    .reduceByKey(lambda a, b: a + b)</span><br><span class="line"></span><br><span class="line">word_counts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 运行机制</title>
      <link href="/2017-12-25-spark-operating-mechanism.html"/>
      <url>/2017-12-25-spark-operating-mechanism.html</url>
      
        <content type="html"><![CDATA[<p>Spark 程序运行顺序及说明</p><a id="more"></a><h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><ul><li>Application: spark应用程序,就是根据 spark api 写的代码</li><li>Driver: 就是集群中提交 Spark 程序的节点上的一个进程</li><li>DAGScheduler: 根据编写的程序代码建立DAG(又向无环图), 且将DAG划分为多个执行阶段(stage)</li><li>TaskScheduler: 主要负责与 Master 和 Executor 进行通讯,向 Executor 发送 task</li><li>Master: 负责资源调度和分配,还有集群监控的进程</li><li>Worker: 工作节点,负责启动 Executor 进程,并对 RDD 上的 Partition进行处理和计算</li><li>Executor: 集群管理器为application在worker上分配的进程, 负责并行执行对 RDD 定义的算子操作, 并将数据保存在内存或磁盘中,每个 application 都有自己的 executor</li><li>Task: Executor 进程下的线程</li><li>task: Driver 发送给 Executor 要执行的任务</li><li>Job: 每个spark application，根据执行了多少次action操作，就会有多少个job</li><li>Stage: 每个job都会划分为多个stage（阶段），每个stage都会有对应的一批task，分配到executor上去执行</li></ul><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ol><li>执行 spark-submit 命令, 启动 driver 进程</li><li>Driver 进程启动后,会做一些初始化的操作,主要是构造出 DAGScheduler 和 TaskScheduler </li><li>初始化过程中,TaskScheduler 会发送请求到 Master 节点上,进行 Spark 程序的注册,通俗的说,就是通知 Master 有一个新的程序要运行</li><li>Master 在接收到注册申请之后,会发送请求给 Worker, 进行资源(Executor)的调度和分配</li><li>Worker 接收到 Master 的请求之后,会为 Spark 程序启动 Executor </li><li>Executor 启动之后, 会向 Driver 上的 TaskScheduler 进行反注册,通俗的说,就是告诉 Driver 有哪些 Executor 会为它服务</li><li>Driver 注册了一些 Executor 之后,就会结束初始化,并开始执行编写的 Spark 程序</li><li>程序运行第一步,就是读取数据源(HDFS/Hive/kafka),创建初始RDD.</li><li>数据源被读取到多个 Worker 节点上,形成内存中的分布式数据集,也就是初始 RDD</li><li>继续执行代码,每执行到一个 action 操作, 就会创建一个 job, 并将 job 提交给 DAGScheduler</li><li>DAGScheduler 根据 stage 划分算法, 将 job 划分为一个或多个 stage, 为每个 stage 创建一个 TaskSet 并提交给 TaskScheduler</li><li>TaskScheduler 根据 task 分配算法,把 task 提交给 Executor</li><li>Executor 接收到 task 之后,就会启动多个 Task 进程执行 task</li><li>Task 会对 RDD 的 partition 数据进行算子操作,形成新的RDD</li></ol><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ol><li>每个spark application，都有属于自己的 executor进程, 不可能出现多个spark aplication共享一个executor进程,executor进程，在整个spark application运行的生命周期内，都会一直存在，不会自己消失.executor进程，最主要的，就是使用多线程的方式，运行SparkContext分配过来的task，来一批task就执行一批，一批执行完了，再换下一批task执行</li><li>spark application，跟集群管理器之间，是透明的,不管是哪个集群管理器，只要可以申请到executor进程就可以了,</li><li>driver 进程必须时刻监听着属于它这个spark application 的 executor进程发来的通信和连接,而且driver除了监听，自己也得负责调度整个spark作业的调度和运行，也得大量跟executor进程通信，给它们分派计算任务</li><li>driver要调度task给executor执行，所以driver在网络环境中的位置尽量离spark集群得近一些,driver最好和spark集群在一片网络内</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka python 客户端使用</title>
      <link href="/2017-12-19-kafka-intro.html"/>
      <url>/2017-12-19-kafka-intro.html</url>
      
        <content type="html"><![CDATA[<p>使用 kafka-python 做 kafka 生产者和消费者客户端开发, 安装方式: <code>pip install kafka-python==1.4.2</code>  </p><a id="more"></a><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li><p>消费者(Consumer): 从消息队列中请求消息的客户端应用程序</p></li><li><p>生产者(Producer): 向 broker 发布消息的客户端应用程序</p></li><li><p>AMOP 服务器端(broker): 用来接收生产者发送的消息并将这些消息路由给服务器中的队列  </p></li><li><p>主题(Topic): 类似分类的概念，实际应用中，通常一个业务一个主题  </p></li><li><p>分区(Partition): 一个 topic 中的消息数据按照多个分区组织，分区是 kafka 消息队列组织的最小单位，一个分区可以看做是一个 FIFO 的队列  </p></li></ul><h2 id="kafka-python客户端开发-代码"><a href="#kafka-python客户端开发-代码" class="headerlink" title="kafka python客户端开发 (代码)"></a>kafka python客户端开发 (<a href="https://github.com/waterandair/daily-learning/tree/master/learn-kafka" target="_blank" rel="noopener">代码</a>)</h2><h3 id="生产者编模型"><a href="#生产者编模型" class="headerlink" title="生产者编模型"></a>生产者编模型</h3><h4 id="同步生产"><a href="#同步生产" class="headerlink" title="同步生产"></a>同步生产</h4><p><img src="/images/kafka-intro-sync-producer.png" alt="image"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line"></span><br><span class="line">from kafka import KafkaProducer</span><br><span class="line">from kafka.errors import KafkaError</span><br><span class="line">import msgpack</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">bootstrap_servers: kafka 服务器地址</span><br><span class="line">value_serializer: 指定用于序列化对象的方法 msgpack.dumps</span><br><span class="line">                  eg.</span><br><span class="line">                     producer &#x3D; KafkaProducer(value_serializer&#x3D;msgpack.dumps) </span><br><span class="line">                     producer.send(&#39;msgpack-topic&#39;, &#123;&#39;key&#39;: &#39;value&#39;&#125;)</span><br><span class="line">                     </span><br><span class="line">                     producer &#x3D; KafkaProducer(value_serializer&#x3D;lambda m: json.dumps(m).encode(&#39;ascii&#39;))</span><br><span class="line">                     producer.send(&#39;json-topic&#39;, &#123;&#39;key&#39;: &#39;value&#39;&#125;)</span><br><span class="line">retries: 重试次数</span><br><span class="line">acks: 服务器端返回响应的时机</span><br><span class="line">        0: 不需要返回响应</span><br><span class="line">        1: 服务器写入 local log 后返回响应 (默认)</span><br><span class="line">        all: 消息提交到了集群中所有的副本后，返回响应</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">producer &#x3D; KafkaProducer(</span><br><span class="line">    bootstrap_servers&#x3D;[</span><br><span class="line">        &quot;172.17.0.2:9092&quot;,</span><br><span class="line">        &quot;172.17.0.3:9092&quot;,</span><br><span class="line">        &quot;172.17.0.4:9092&quot;,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># key 用于hash， 相同 key 的 value 提交到相同的 partition，key 的值默认为 None， 表示随机分配</span><br><span class="line">res &#x3D; producer.send(topic&#x3D;&#39;test&#39;, value&#x3D;b&#39;this is a sync test&#39;, key&#x3D;None)</span><br><span class="line"></span><br><span class="line"># producer 默认是异步模式， 调用 send 函数返回对象的 get 方法，可以把异步模式转换为同步模式</span><br><span class="line">try:</span><br><span class="line">    record_metadata &#x3D; res.get(timeout&#x3D;10)</span><br><span class="line"></span><br><span class="line">    # 获取成功返回响应的数据</span><br><span class="line">    print(record_metadata.topic)</span><br><span class="line">    print(record_metadata.partition)</span><br><span class="line">    print(record_metadata.offset)</span><br><span class="line">except KafkaError:</span><br><span class="line">    # 处理异常</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><h4 id="异步生产"><a href="#异步生产" class="headerlink" title="异步生产"></a>异步生产</h4><p><img src="/images/kafka-intro-async-producer.png" alt="image"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line">import time</span><br><span class="line">from kafka import KafkaProducer</span><br><span class="line">import msgpack</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">bootstrap_servers: kafka 服务器地址</span><br><span class="line">value_serializer: 指定用于序列化对象的方法 msgpack.dumps</span><br><span class="line">                  eg.</span><br><span class="line">                     producer &#x3D; KafkaProducer(value_serializer&#x3D;msgpack.dumps) </span><br><span class="line">                     producer.send(&#39;msgpack-topic&#39;, &#123;&#39;key&#39;: &#39;value&#39;&#125;)</span><br><span class="line"></span><br><span class="line">                     producer &#x3D; KafkaProducer(value_serializer&#x3D;lambda m: json.dumps(m).encode(&#39;ascii&#39;))</span><br><span class="line">                     producer.send(&#39;json-topic&#39;, &#123;&#39;key&#39;: &#39;value&#39;&#125;)</span><br><span class="line">retries: 重试次数</span><br><span class="line">acks: 服务器端返回响应的时机</span><br><span class="line">        0: 不需要返回响应</span><br><span class="line">        1: 服务器写入 local log 后返回响应 (默认)</span><br><span class="line">        all: 消息提交到了集群中所有的副本后，返回响应</span><br><span class="line"></span><br><span class="line">异步模式下，producer不会马上把消息发送到 kafka，而是根据触发条件一批一批的发送，batch_size 和 linger_ms 满足其一，就会提交消息</span><br><span class="line">batch_size: 每一批消息累计的最大消息数, 默认 16384</span><br><span class="line">linger_ms: 每一批消息最大累计时长 默认0 ms</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">producer &#x3D; KafkaProducer(</span><br><span class="line">    bootstrap_servers&#x3D;[</span><br><span class="line">        &quot;172.17.0.2:9092&quot;,</span><br><span class="line">        &quot;172.17.0.3:9092&quot;,</span><br><span class="line">        &quot;172.17.0.4:9092&quot;,</span><br><span class="line">    ],</span><br><span class="line">    batch_size&#x3D;5,</span><br><span class="line">    linger_ms&#x3D;3000</span><br><span class="line">)</span><br><span class="line"># key 用于hash， 相同 key 的 value 提交到相同的 partition，key 的值默认为 None， 表示随机分配</span><br><span class="line">for i in range(100):</span><br><span class="line">    mes &#x3D; (&#39;this is a async test &#39; + str(i)).encode()</span><br><span class="line">    producer.send(topic&#x3D;&#39;test&#39;, value&#x3D;mes, key&#x3D;None)</span><br><span class="line">    time.sleep(1)</span><br><span class="line"></span><br><span class="line"># block until all async messages are sent</span><br><span class="line">producer.flush()</span><br></pre></td></tr></table></figure><h4 id="同步生产和异步生产的区别"><a href="#同步生产和异步生产的区别" class="headerlink" title="同步生产和异步生产的区别"></a>同步生产和异步生产的区别</h4><p><strong>同步生产:</strong></p><ol><li>低消息丢失率</li><li>高消息重复率（在未收到确认信息的情况下）</li><li>高延迟</li></ol><p><strong>异步生产:</strong></p><ol><li>低延迟</li><li>高发送性能</li><li>高消息丢失率（无确认机制，发送端队列满）</li></ol><h3 id="消费者编程模型"><a href="#消费者编程模型" class="headerlink" title="消费者编程模型"></a>消费者编程模型</h3><p><img src="/images/kafka-intro-group-consumer.png" alt="image">  </p><p>同一分组内的 consumer 共同消费指定的 topic, 如果一个分组内只有一个 consumer， 那么它会拿到 topic 的全部数据，如果有多个 consumer ，那么每个 consumer 拿到一部分 topic 数据</p><h4 id="自动提交-offset"><a href="#自动提交-offset" class="headerlink" title="自动提交 offset"></a>自动提交 offset</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line">from kafka import KafkaConsumer</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">enable_auto_commit: 是否自动提交 offset 默认True</span><br><span class="line">auto_offset_commit: 当发生 OffsetOutOfRange 时， 重置 offset 的策略，默认 latest</span><br><span class="line">                    latest: 设置 offset 为最近一个消息，如果采用latest，消费者只能得道其启动后生产者生产的消息 </span><br><span class="line">                    earliest: 设置 offset 为存在的时间最长的一条消息</span><br><span class="line">consumer_timeout_ms： 获取不到消息等待的时间，超过这个值，就会抛出  StopIteration 异常</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">consumer &#x3D; KafkaConsumer(</span><br><span class="line">    bootstrap_servers&#x3D;[</span><br><span class="line">        &#39;172.17.0.2:9092&#39;,</span><br><span class="line">        &#39;172.17.0.3:9092&#39;,</span><br><span class="line">        &#39;172.17.0.4:9092&#39;,</span><br><span class="line">    ],</span><br><span class="line">    group_id&#x3D;&#39;group-1&#39;,</span><br><span class="line">    enable_auto_commit&#x3D;True,</span><br><span class="line">    auto_offset_reset&#x3D;&#39;earliest&#39;,</span><br><span class="line">    consumer_timeout_ms&#x3D;1000</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">consumer.subscribe([&quot;test&quot;])</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    for message in consumer:</span><br><span class="line">        print(&quot;%s:%d:%d: key&#x3D;%s value&#x3D;%s&quot; % (message.topic, message.partition, message.offset, message.key, message.value))</span><br></pre></td></tr></table></figure><h4 id="手动提交-offset"><a href="#手动提交-offset" class="headerlink" title="手动提交 offset"></a>手动提交 offset</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line"># -*- coding utf-8 -*-</span><br><span class="line">from kafka import KafkaConsumer</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">enable_auto_commit: 是否自动提交 offset 默认True</span><br><span class="line">auto_offset_commit: 当发生 OffsetOutOfRange 时， 重置 offset 的策略，默认 latest</span><br><span class="line">                    latest: 设置 offset 为最近一个消息，如果采用latest，消费者只能得道其启动后生产者生产的消息 </span><br><span class="line">                    earliest: 设置 offset 为存在的时间最长的一条消息</span><br><span class="line">consumer_timeout_ms： 获取不到消息等待的时间，超过这个值，就会抛出  StopIteration 异常</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">consumer &#x3D; KafkaConsumer(</span><br><span class="line">    bootstrap_servers&#x3D;[</span><br><span class="line">        &#39;172.17.0.2:9092&#39;,</span><br><span class="line">        &#39;172.17.0.3:9092&#39;,</span><br><span class="line">        &#39;172.17.0.4:9092&#39;,</span><br><span class="line">    ],</span><br><span class="line">    group_id&#x3D;&#39;group-3&#39;,</span><br><span class="line">    enable_auto_commit&#x3D;False,</span><br><span class="line">    auto_offset_reset&#x3D;&#39;earliest&#39;,</span><br><span class="line">    consumer_timeout_ms&#x3D;1000</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">consumer.subscribe([&quot;test&quot;])</span><br><span class="line"></span><br><span class="line">size &#x3D; 10</span><br><span class="line">while True:</span><br><span class="line">    records_list &#x3D; []</span><br><span class="line">    for message in consumer:</span><br><span class="line">        print(&quot;%s:%d:%d: key&#x3D;%s value&#x3D;%s&quot; % (message.topic, message.partition, message.offset, message.key, message.value))</span><br><span class="line">        records_list.append(message)</span><br><span class="line"></span><br><span class="line">    if len(records_list) &gt; size:</span><br><span class="line">        # 集中处理这一批消息，处理成功后，再提交 offset， 实现了 at least once 的语义</span><br><span class="line">        # 异步提交 offset</span><br><span class="line">        consumer.commit_async()</span><br><span class="line">        # 清空 records_list</span><br><span class="line">        records_list.clear()</span><br></pre></td></tr></table></figure><h4 id="手动提交-offset-和自动提交-offset-的区别"><a href="#手动提交-offset-和自动提交-offset-的区别" class="headerlink" title="手动提交 offset 和自动提交 offset 的区别"></a>手动提交 offset 和自动提交 offset 的区别</h4><p><strong>分区消费模型更加灵活:</strong>  </p><ol><li>需要自己处理各种异常情况  </li><li>需要自己管理 offset(自己实现消息传递的语义,最少一次或最多一次)</li></ol><p><strong>组消费模型更加简单,但不灵活:</strong>  </p><ol><li>不需要自己处理异常情况,不需要自己管理offset</li><li>只能实现 kafka 默认的最多一次消息传递的语义</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>centos7 本地源方式部署 CDH 大数据服务管理平台</title>
      <link href="/2017-12-15-big-data-cdh-5-14-2-install.html"/>
      <url>/2017-12-15-big-data-cdh-5-14-2-install.html</url>
      
        <content type="html"><![CDATA[<p>cloudera manager 是大数据生态集群的企业级管理工具,可以快速对包括 hadoop, hive, hbase, spark … 等众多服务进行部署,监控</p><a id="more"></a><h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><h4 id="基本条件"><a href="#基本条件" class="headerlink" title="基本条件"></a>基本条件</h4><p>集群: 三台或更多<br>操作系统: centos7.4.1708<br>磁盘阵列: JBOD (Hadoop datanode 推荐存储方式)  </p><h4 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h4><p>最好是同一网段,同意交换机\机架:</p><ul><li>192.168.1.61</li><li>192.168.1.62</li><li>192.168.1.63</li></ul><h4 id="主机名"><a href="#主机名" class="headerlink" title="主机名"></a>主机名</h4><p><strong>永久修改:</strong><br>编辑 <code>/etc/hostname</code> 文件, 修改为自定义的主机名  </p><blockquote><p>注意: 主机名中不能用下划线, 使用 hostname 命令修改主机名,重启后会失效, 在 centos7 之前的版本是编辑  <code>/etc/sysconfig/network</code> 文件</p></blockquote><p>修改后对应上节 IP 地址的顺序, hostname 依次为:  </p><ul><li>cdh-01</li><li>cdh-02</li><li>cdh-03</li></ul><h4 id="配置-etc-hosts"><a href="#配置-etc-hosts" class="headerlink" title="配置 /etc/hosts"></a>配置 /etc/hosts</h4><p>主机间可互相访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">192.168.1.61 cdh-01.com cdh-01</span><br><span class="line">192.168.1.62 cdh-02.com cdh-02</span><br><span class="line">192.168.1.63 cdh-03.com cdh-03</span><br></pre></td></tr></table></figure><h4 id="创建普通用户"><a href="#创建普通用户" class="headerlink" title="创建普通用户"></a>创建普通用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">adduser big-data</span><br><span class="line">passwd big-data</span><br></pre></td></tr></table></figure><h4 id="配置普通用户的-sudo-权限"><a href="#配置普通用户的-sudo-权限" class="headerlink" title="配置普通用户的 sudo 权限"></a>配置普通用户的 sudo 权限</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 安装 sudo</span><br><span class="line">yum -y install sudo</span><br><span class="line"># 对 &#x2F;etc&#x2F;sudoers 添加写权限</span><br><span class="line">chmod u+w &#x2F;etc&#x2F;sudoers</span><br><span class="line"># 编辑 &#x2F;etc&#x2F;sudoers , 在第一行加入用户 big-data 的 sudo 免密权限</span><br><span class="line">    big-data ALL&#x3D;(root) NOPASSWD:ALL</span><br><span class="line"></span><br><span class="line"># 重启用 big-data用户登录</span><br><span class="line"># 收回&quot;写&quot;权限</span><br><span class="line">sudo chmod u-w &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure><h4 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop firewalld.service #停止firewall</span><br><span class="line"></span><br><span class="line">sudo systemctl disable firewalld.service #禁止firewall开机启动</span><br></pre></td></tr></table></figure><h4 id="禁用-SELinux"><a href="#禁用-SELinux" class="headerlink" title="禁用 SELinux"></a>禁用 SELinux</h4><p>编辑 <code>/etc/sysconfig/selinux</code> , 设置 <code>SELINUX=disabled</code>,<br>重启服务器</p><h4 id="卸载-jdk"><a href="#卸载-jdk" class="headerlink" title="卸载 jdk"></a>卸载 jdk</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 查看是否有 jdk</span><br><span class="line">    rpm -qa | grep java</span><br><span class="line"># 如果有,则全部卸载</span><br><span class="line">     rpm -e --nodeps xxx yyy zzz</span><br></pre></td></tr></table></figure><h4 id="最大文件数和最大进程数"><a href="#最大文件数和最大进程数" class="headerlink" title="最大文件数和最大进程数"></a>最大文件数和最大进程数</h4><p>编辑 <code>/etc/security/limits.conf</code> 文件, 加入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* soft nofile 65535</span><br><span class="line">* hard nofile 65535</span><br><span class="line">* soft nproc 32000</span><br><span class="line">* hard nproc 32000</span><br></pre></td></tr></table></figure><h4 id="同步时间-重要"><a href="#同步时间-重要" class="headerlink" title="同步时间(重要)"></a>同步时间(重要)</h4><h5 id="选择一台服务器作为时间服务器"><a href="#选择一台服务器作为时间服务器" class="headerlink" title="选择一台服务器作为时间服务器"></a>选择一台服务器作为时间服务器</h5><p>选择 192.168.1.61</p><h5 id="安装配置-ntp"><a href="#安装配置-ntp" class="headerlink" title="安装配置 ntp"></a>安装配置 ntp</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 检查有没有 ntp</span><br><span class="line">sudo rpm -qa | grep ntp</span><br><span class="line"># 如果没有,需要安装</span><br><span class="line">sudo yum -y install ntp</span><br><span class="line"># 服务器端:</span><br><span class="line">    # 配置 &#x2F;etc&#x2F;ntp.conf</span><br><span class="line">        # 1. 去掉这一行的注释</span><br><span class="line">            restrict 192.168.0.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">        # 2. 注释掉 server 配置</span><br><span class="line">            # server 0.centos.pool.ntp.org iburst</span><br><span class="line">            # server 1.centos.pool.ntp.org iburst</span><br><span class="line">            # server 2.centos.pool.ntp.org iburst</span><br><span class="line">            # server 3.centos.pool.ntp.org iburst</span><br><span class="line">        # 3. 配置 server</span><br><span class="line">            server 127.127.1.0  # local clock</span><br><span class="line">            fudge 127.127.1.0  stratum 10</span><br><span class="line">    # 配置 bois 与系统时间同步 &#x2F;etc&#x2F;sysconfig&#x2F;ntpd</span><br><span class="line">        SYNC_HWCLOCK&#x3D;yes</span><br><span class="line">    # 启动 ntpd</span><br><span class="line">    </span><br><span class="line"># 客户端同步:</span><br><span class="line">    # Linux crontab, 每十分钟同步一次</span><br><span class="line">    # 查看定时任务 </span><br><span class="line">    crontab -l</span><br><span class="line">    # root 用户编辑定时任务</span><br><span class="line">    crontab -e</span><br><span class="line">        0-59&#x2F;10 * * * * &#x2F;usr&#x2F;sbin&#x2F;ntpdate cdh-01.com</span><br><span class="line"></span><br><span class="line"># 修改时区</span><br><span class="line">cp -f &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime</span><br></pre></td></tr></table></figure><h3 id="本地源-yum-安装-cloudera-manager"><a href="#本地源-yum-安装-cloudera-manager" class="headerlink" title="本地源 yum 安装 cloudera-manager"></a>本地源 yum 安装 cloudera-manager</h3><p>选择一个服务器作为 cloudera-manager , 这里选择 cdh-01</p><h4 id="搭建-yum-本地源"><a href="#搭建-yum-本地源" class="headerlink" title="搭建 yum 本地源"></a>搭建 yum 本地源</h4><h5 id="下载安装文件包"><a href="#下载安装文件包" class="headerlink" title="下载安装文件包"></a>下载安装文件包</h5><p>在 <a href="http://archive.cloudera.com/cm5/repo-as-tarball/" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/repo-as-tarball/</a> 地址中选择适合自己的包, 安装 cloudera-manager 所需的包都在这里打包好了<br>这里选择 <a href="http://archive.cloudera.com/cm5/repo-as-tarball/5.14.2/cm5.14.2-centos7.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/repo-as-tarball/5.14.2/cm5.14.2-centos7.tar.gz</a><br>解压到 <code>/home/big-data/www/</code> 目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── www</span><br><span class="line">    ├── cloudera-manager-installer.bin</span><br><span class="line">    └── cm</span><br><span class="line">        ├── 5 -&gt; &#x2F;home&#x2F;big-data&#x2F;www&#x2F;cm&#x2F;5.14.2&#x2F;</span><br><span class="line">        ├── 5.14 -&gt; &#x2F;home&#x2F;big-data&#x2F;www&#x2F;cm&#x2F;5.14.2&#x2F;</span><br><span class="line">        ├── 5.14.2</span><br><span class="line">        │   ├── generated_index.html</span><br><span class="line">        │   ├── mirrors</span><br><span class="line">        │   ├── repodata</span><br><span class="line">        │   │   ├── 59314c12817f04542a9e66385a5cc7c8f88e2636af083feb0d4918d9b69030c6-filelists.sqlite.bz2</span><br><span class="line">        │   │   ├── 7397d19421876e15dcb2824a8b4d365b6a83f33da3309f57d63ba98fb9c6da6a-primary.xml.gz</span><br><span class="line">        │   │   ├── 74923d5bd069a85a6afb2c0269736451c29a164220f49cb9cda9d119675cb633-other.sqlite.bz2</span><br><span class="line">        │   │   ├── 7d97c61995f807b665e606ee95fe7484efe5adca1af3b37e6945f6a493f5b68c-filelists.xml.gz</span><br><span class="line">        │   │   ├── b57f29c4a2e39730d49be03e1159bf94e3ca96efee80a1bc5ce3d82e6f1c83e5-other.xml.gz</span><br><span class="line">        │   │   ├── bb895b74d4be001bd933ccbd5efaabf51828dd42dd8c2612eb9a8295284d481d-primary.sqlite.bz2</span><br><span class="line">        │   │   ├── filelists.xml.gz</span><br><span class="line">        │   │   ├── filelists.xml.gz.asc</span><br><span class="line">        │   │   ├── generated_index.html</span><br><span class="line">        │   │   ├── other.xml.gz</span><br><span class="line">        │   │   ├── other.xml.gz.asc</span><br><span class="line">        │   │   ├── primary.xml.gz</span><br><span class="line">        │   │   ├── primary.xml.gz.asc</span><br><span class="line">        │   │   ├── repomd.xml</span><br><span class="line">        │   │   └── repomd.xml.asc</span><br><span class="line">        │   └── RPMS</span><br><span class="line">        │       ├── generated_index.html</span><br><span class="line">        │       ├── noarch</span><br><span class="line">        │       │   └── generated_index.html</span><br><span class="line">        │       └── x86_64</span><br><span class="line">        │           ├── cloudera-manager-agent-5.14.2-1.cm5142.p0.8.el7.x86_64.rpm</span><br><span class="line">        │           ├── cloudera-manager-daemons-5.14.2-1.cm5142.p0.8.el7.x86_64.rpm</span><br><span class="line">        │           ├── cloudera-manager-server-5.14.2-1.cm5142.p0.8.el7.x86_64.rpm</span><br><span class="line">        │           ├── cloudera-manager-server-db-2-5.14.2-1.cm5142.p0.8.el7.x86_64.rpm</span><br><span class="line">        │           ├── enterprise-debuginfo-5.14.2-1.cm5142.p0.8.el7.x86_64.rpm</span><br><span class="line">        │           ├── generated_index.html</span><br><span class="line">        │           ├── jdk-6u31-linux-amd64.rpm</span><br><span class="line">        │           └── oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm</span><br><span class="line">        ├── cloudera-cm.repo</span><br><span class="line">        ├── generated_index.html</span><br><span class="line">        └── RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line">9 directories, 31 files</span><br></pre></td></tr></table></figure><h5 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h5><p>使用 python 自带的web服务模块启动一个简单的web下载服务器:</p><p>进入 <code>/home/big-data/www/</code> 目录</p><ul><li>python2 版本: 执行 <code>python -m SimpleHttpServer</code></li><li>python3 版本: 执行 <code>python3 -m http.server</code> </li></ul><p>这样就可以启动一个端口为 8000 的 web 服务了, 可以在浏览器中访问<code>http://cdh-01.com:8000</code></p><h5 id="安装-createrepo"><a href="#安装-createrepo" class="headerlink" title="安装 createrepo"></a>安装 createrepo</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install yum-utils createrepo</span><br></pre></td></tr></table></figure><h5 id="在解压目录中重新生成-repodata"><a href="#在解压目录中重新生成-repodata" class="headerlink" title="在解压目录中重新生成 repodata"></a>在解压目录中重新生成 repodata</h5><p>进入目录 <code>/home/big-bata/www/cm/5.14</code> 执行 <code>createrepo .</code> </p><h5 id="配置-yum-源"><a href="#配置-yum-源" class="headerlink" title="配置 yum 源"></a>配置 yum 源</h5><p>复制 <code>/home/big-bata/www/cm/cloudera-cm.repo</code> 到 <code>/etc/yum.repos.d/</code> 目录中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp &#x2F;home&#x2F;big-data&#x2F;www&#x2F;cm&#x2F;cloudera-cm.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br></pre></td></tr></table></figure><p>把原地址修改为本地服务器的地址,注意域名和端口号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo vi &#x2F;etc&#x2F;yum.repos.d&#x2F;cloudera-cm.repo</span><br><span class="line">    [cloudera-cm]</span><br><span class="line">    # Packages for Cloudera‘s Distribution for cm, Version 5, on RedHat or CentOS 7 x86_64</span><br><span class="line">    name&#x3D;Cloudera‘s Distribution for cm, Version 5</span><br><span class="line">    baseurl&#x3D;http:&#x2F;&#x2F;cdh-01.com:8000&#x2F;cm&#x2F;5.14</span><br><span class="line">    gpgcheck &#x3D; 0</span><br></pre></td></tr></table></figure><p>修改后再运行 <code>sudo yum clean all</code></p><h4 id="安装-Cloudera-Manager"><a href="#安装-Cloudera-Manager" class="headerlink" title="安装 Cloudera Manager"></a>安装 Cloudera Manager</h4><p>下载安装程序 <a href="http://archive.cloudera.com/cm5/installer/5.14.2/cloudera-manager-installer.bin" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/installer/5.14.2/cloudera-manager-installer.bin</a> 到 <code>/home/big-data/www</code> 目录</p><p>执行 <code>sudo /home/big-data/www/cloudera-manager-installer.bin</code></p><p>按照图形界面提示安装</p><p>安装完成后,使用 admin 用户登录 web 页面, 看是安装cdh</p><h3 id="安装-cdh-使用推荐的方式-parcels"><a href="#安装-cdh-使用推荐的方式-parcels" class="headerlink" title="安装 cdh (使用推荐的方式: parcels)"></a>安装 cdh (使用推荐的方式: parcels)</h3><h4 id="下载文件-parcels-文件"><a href="#下载文件-parcels-文件" class="headerlink" title="下载文件 parcels 文件"></a>下载文件 parcels 文件</h4><p>下载地址: <a href="https://archive.cloudera.com/cdh5/parcels/" target="_blank" rel="noopener">https://archive.cloudera.com/cdh5/parcels/</a></p><p>这里选择下载:<br><a href="https://archive.cloudera.com/cdh5/parcels/5.14.2/CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel" target="_blank" rel="noopener">https://archive.cloudera.com/cdh5/parcels/5.14.2/CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel</a>  </p><p><a href="https://archive.cloudera.com/cdh5/parcels/5.14.2/CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1" target="_blank" rel="noopener">https://archive.cloudera.com/cdh5/parcels/5.14.2/CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1</a></p><p><a href="http://archive.cloudera.com/cdh5/parcels/5.14.2/manifest.json" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/5.14.2/manifest.json</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">big-data@cdh-01:~$ pwd</span><br><span class="line">&#x2F;home&#x2F;big-data&#x2F;www</span><br><span class="line"></span><br><span class="line">big-data@cdh-01:~$ ll</span><br><span class="line">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel </span><br><span class="line">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1</span><br><span class="line">manifest.json</span><br></pre></td></tr></table></figure><h5 id="1-创建目录"><a href="#1-创建目录" class="headerlink" title="1. 创建目录:"></a>1. 创建目录:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir &#x2F;opt&#x2F;cloudera&#x2F;parcels</span><br><span class="line">sudo mkdir &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br></pre></td></tr></table></figure><h5 id="2-移动文件"><a href="#2-移动文件" class="headerlink" title="2. 移动文件"></a>2. 移动文件</h5><p>将下载好的三个文件移动到 <code>/opt/cloudera/parcel-repo</code> 目录下, 并把 <code>CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1</code> 后缀更名为 <code>sha</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mv &#x2F;home&#x2F;big-data&#x2F;www&#x2F;CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;</span><br><span class="line">sudo mv &#x2F;home&#x2F;big-data&#x2F;www&#x2F;CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1 &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha</span><br><span class="line">sudo mv &#x2F;home&#x2F;big-data&#x2F;www&#x2F;manifest.json &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;manifest.json</span><br></pre></td></tr></table></figure><h4 id="登录-cdh-01-com-7180-进行安装"><a href="#登录-cdh-01-com-7180-进行安装" class="headerlink" title="登录 cdh-01.com:7180, 进行安装"></a>登录 cdh-01.com:7180, 进行安装</h4><h5 id="登录"><a href="#登录" class="headerlink" title="登录"></a>登录</h5><p>用户名密码都是 <code>admin</code>, 登录后在欢迎页面选择接受协议,选择 Cloudera Express(免费) 版本</p><p><img src="/images/cdh-install/cdh-install-01.png" alt="image"></p><h5 id="指定主机"><a href="#指定主机" class="headerlink" title="指定主机"></a>指定主机</h5><p>跟随着页面的引导,接下来要做的就是指定主机,在输入框中输入 <code>cdh-[01-03].com</code> 即可, 点击解析查看节点是否可用</p><h5 id="选择-cdh-版本并且设置自定义存储库"><a href="#选择-cdh-版本并且设置自定义存储库" class="headerlink" title="选择 cdh 版本并且设置自定义存储库"></a>选择 cdh 版本并且设置自定义存储库</h5><p>选择之前下载好的 parcel 版本</p><p><img src="/images/cdh-install/cdh-install-02.png" alt="image"></p><p>指定自定义存储库为 <code>cloudera-cm.repo</code> 中配置的本地服务地址</p><p><img src="/images/cdh-install/cdh-install-03.png" alt="image"></p><h5 id="选择安装jdk"><a href="#选择安装jdk" class="headerlink" title="选择安装jdk"></a>选择安装jdk</h5><p><img src="/images/cdh-install/cdh-install-04.png" alt="image"></p><h5 id="不勾选-Single-User-Mode-继续下一步"><a href="#不勾选-Single-User-Mode-继续下一步" class="headerlink" title="不勾选 Single User Mode , 继续下一步"></a>不勾选 Single User Mode , 继续下一步</h5><p><img src="/images/cdh-install/cdh-install-05.png" alt="image"></p><h5 id="填写用户和密码"><a href="#填写用户和密码" class="headerlink" title="填写用户和密码"></a>填写用户和密码</h5><p><img src="/images/cdh-install/cdh-install-06.png" alt="image"></p><h5 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h5><p><img src="/images/cdh-install/cdh-install-07.png" alt="image"><br><img src="/images/cdh-install/cdh-install-08.png" alt="image"></p><h5 id="分配-解压-激活-parcel"><a href="#分配-解压-激活-parcel" class="headerlink" title="分配,解压,激活 parcel"></a>分配,解压,激活 parcel</h5><p><img src="/images/cdh-install/cdh-install-09.png" alt="image"></p><p>完成后,会自动检查服务器节点, 会找出问题并给出解决办法,一定要让所有的检查项都通过.</p><h5 id="安装-Cloudera-Managerment-Service-服务"><a href="#安装-Cloudera-Managerment-Service-服务" class="headerlink" title="安装 Cloudera Managerment Service 服务"></a>安装 Cloudera Managerment Service 服务</h5><p><img src="/images/cdh-install/cdh-install-10.png" alt="image"><br><img src="/images/cdh-install/cdh-install-11.png" alt="image"><br><img src="/images/cdh-install/cdh-install-12.png" alt="image"><br><img src="/images/cdh-install/cdh-install-13.png" alt="image"></p><h5 id="依次安装-zookeeper-HDFS-Yarn-Hive-spark-等服务"><a href="#依次安装-zookeeper-HDFS-Yarn-Hive-spark-等服务" class="headerlink" title="依次安装 zookeeper, HDFS, Yarn, Hive, spark 等服务"></a>依次安装 zookeeper, HDFS, Yarn, Hive, spark 等服务</h5><p><img src="/images/cdh-install/cdh-install-14.png" alt="image"></p><p><img src="/images/cdh-install/cdh-install-15.png" alt="image"><br><img src="/images/cdh-install/cdh-install-16.png" alt="image"></p><p><img src="/images/cdh-install/cdh-install-17.png" alt="image"><br><img src="/images/cdh-install/cdh-install-18.png" alt="image"><br><img src="/images/cdh-install/cdh-install-19.png" alt="image"><br><img src="/images/cdh-install/cdh-install-20.png" alt="image"><br><img src="/images/cdh-install/cdh-install-21.png" alt="image"><br><img src="/images/cdh-install/cdh-install-22.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> CDH </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 ansible 部署 kafka 集群</title>
      <link href="/2017-12-03-install-kafka.html"/>
      <url>/2017-12-03-install-kafka.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>【必须】kafka 依赖 zookeeper ,所以安装 kafka 前要前搭建一个zookeeper集群,一般这两个集群是在同一集群内,搭建zookeeper集群可以参考<a href="http://waterandair.top/install-zookeeper.html" target="_blank" rel="noopener">zookeeper 环境</a></p></blockquote><a id="more"></a><h4 id="初始化-role"><a href="#初始化-role" class="headerlink" title="初始化 role"></a>初始化 role</h4><p>在任意目录(这里以test文件夹为例)下执行 ansible-galaxy init kafka ,初始化一个 roles  </p><h4 id="设置要用到的变量"><a href="#设置要用到的变量" class="headerlink" title="设置要用到的变量"></a>设置要用到的变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># defaults file for kafka</span><br><span class="line"># kafka 版本</span><br><span class="line">kafka_version: &quot;1.0.1&quot;</span><br><span class="line">scala_version: &quot;2.12&quot;</span><br><span class="line">scala_kafka_version: &quot;&#123;&#123; &#39;kafka_&#39; + scala_version + &#39;-&#39; + kafka_version &#125;&#125;&quot;</span><br><span class="line"># 下载地址</span><br><span class="line">kafka_download_url: &quot;http:&#x2F;&#x2F;mirrors.hust.edu.cn&#x2F;apache&#x2F;kafka&#x2F;&#123;&#123; kafka_version &#125;&#125;&#x2F;&#123;&#123; scala_kafka_version &#125;&#125;.tgz&quot;</span><br><span class="line"># 下载到本地的 ~&#x2F;Downloads</span><br><span class="line">kafka_download_dest: ~&#x2F;Downloads&#x2F;&#123;&#123; scala_kafka_version &#125;&#125;.tgz</span><br><span class="line"></span><br><span class="line"># 安装目录</span><br><span class="line">kafka_install_dir: &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"># kafka 目录</span><br><span class="line">kafka_dir: &quot;&#x2F;usr&#x2F;local&#x2F;kafka_&#123;&#123; scala_version &#125;&#125;-&#123;&#123; kafka_version &#125;&#125;&quot;</span><br><span class="line"># kafka 目录软链接</span><br><span class="line">soft_dir: &#x2F;usr&#x2F;local&#x2F;kafka</span><br><span class="line"># kafka 配置文件目录</span><br><span class="line">kafka_conf_dir: &quot;&#123;&#123; soft_dir &#125;&#125;&#x2F;config&#x2F;&quot;</span><br><span class="line"># kafka 持久化目录</span><br><span class="line">kafka_log_dir: &#x2F;var&#x2F;kafka</span><br><span class="line"># 设置环境变量</span><br><span class="line">env_file: ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><h4 id="准备-kafka-配置文件的模板"><a href="#准备-kafka-配置文件的模板" class="headerlink" title="准备 kafka 配置文件的模板"></a>准备 kafka 配置文件的模板</h4><p>提前在控制节点解压出一份 kafka 默认的配置文件server.properties, 复制到 test/kafka/templates/ 下更名为 server.properties.j2, 在这里主要配置以下几项:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;# kafka节点在集群中的标识, 这里的id, 是在 hosts 文件中配置好的 #&#125;</span><br><span class="line">broker.id&#x3D;&#123;&#123; id &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;#</span><br><span class="line">  因为 ansible 是用 ssh 的,所以 ansible_env.SSH_CONNECTION 变量一定存在,</span><br><span class="line">  ansible_env.SSH_CONNECTION: &quot;172.17.0.1 20742 172.17.0.4 22&quot;可以通过这个字符串获取到 ip</span><br><span class="line">#&#125;</span><br><span class="line">&#123;% set ip &#x3D; ansible_env.SSH_CONNECTION.split(&quot; &quot;)[2] %&#125;</span><br><span class="line">listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123; ip &#125;&#125;:9092</span><br><span class="line"></span><br><span class="line">&#123;# </span><br><span class="line">  一般来说,kafka 集群和依赖的 zookeeper 集群都在同一个集群上,</span><br><span class="line">  所以这里获取到hosts列表,遍历输出就是 kafka 对应的 zookeeper 集群配置 </span><br><span class="line">#&#125;</span><br><span class="line">zookeeper.connect&#x3D;&#123;% for host in ansible_play_batch %&#125;&#123;&#123; host &#125;&#125;:2181,&#123;% endfor %&#125;</span><br></pre></td></tr></table></figure><h4 id="编写-tasks"><a href="#编写-tasks" class="headerlink" title="编写 tasks"></a>编写 tasks</h4><p>如果一个 tasks 有很多步骤,可以把它们分置在不同的文件中,最后在 test/kafka/tasks/main.yml 文件中引用他们,这样做方便调试,方便阅读  </p><h5 id="编写下载过程"><a href="#编写下载过程" class="headerlink" title="编写下载过程"></a>编写下载过程</h5><p>在 test/kafka/tasks/ 下新建文件 download.yml  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># 下载 kafka</span><br><span class="line">- name: 判断 kafka 源文件是否已经下载好了</span><br><span class="line">  command: ls &#123;&#123; kafka_download_dest &#125;&#125;</span><br><span class="line">  ignore_errors: true</span><br><span class="line">  register: file_exist</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 下载文件</span><br><span class="line">  get_url:</span><br><span class="line">    url: &quot;&#123;&#123; kafka_download_url&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; kafka_download_dest &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line">  when: file_exist|failed</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 解压</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123; kafka_download_dest &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; kafka_install_dir &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><h5 id="编写配置和启动过程"><a href="#编写配置和启动过程" class="headerlink" title="编写配置和启动过程"></a>编写配置和启动过程</h5><p>在 test/kafka/tasks/ 下新建文件 init.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- name: 创建 kafka 的软链接</span><br><span class="line">  file:</span><br><span class="line">    src: &quot;&#123;&#123; kafka_dir &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; soft_dir &#125;&#125;&quot;</span><br><span class="line">    state: link</span><br><span class="line"></span><br><span class="line">- name: 设置 kafka 环境变量</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123;env_file&#125;&#125;&quot;</span><br><span class="line">    insertafter: &quot;&#123;&#123;item.position&#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123;item.value&#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">    - &#123;position: EOF, value: &quot;\n&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;&#123;&#123; &#39;export KAFKA_HOME&#x3D;&#39; + soft_dir &#125;&#125;&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;export PATH&#x3D;$KAFKA_HOME&#x2F;bin:$PATH&quot;&#125;</span><br><span class="line"></span><br><span class="line">- name: 设置配置文件</span><br><span class="line">  template:</span><br><span class="line">    src: server.properties.j2</span><br><span class="line">    dest: &quot;&#123;&#123; kafka_conf_dir + &#39;server.properties&#39; &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line"></span><br><span class="line">- name: 启动 kafka 服务器</span><br><span class="line">  shell: &quot;&#123;&#123; &#39;kafka-server-start.sh&#39; + &#39; -daemon &#39; + kafka_conf_dir + &#39;server.properties&#39; &#125;&#125;&quot;</span><br><span class="line">  environment:</span><br><span class="line">    PATH: &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;java&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin</span><br><span class="line">    KAFKA_HOME: usr&#x2F;local&#x2F;kafka</span><br></pre></td></tr></table></figure><h5 id="合并所有tasks"><a href="#合并所有tasks" class="headerlink" title="合并所有tasks"></a>合并所有tasks</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for kafka</span><br><span class="line">- import_tasks: download.yml</span><br><span class="line">- import_tasks: init.yml</span><br></pre></td></tr></table></figure><h4 id="编写-hosts-文件"><a href="#编写-hosts-文件" class="headerlink" title="编写 hosts 文件"></a>编写 hosts 文件</h4><p>在 test/ 目录下新建文件 hosts  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 这里的id很重要,用于配置kafka在集群中的标识</span><br><span class="line">[test]</span><br><span class="line">172.17.0.2 id&#x3D;1</span><br><span class="line">172.17.0.3 id&#x3D;2</span><br><span class="line">172.17.0.4 id&#x3D;3</span><br></pre></td></tr></table></figure><h4 id="编写启动文件"><a href="#编写启动文件" class="headerlink" title="编写启动文件"></a>编写启动文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- hosts: test</span><br><span class="line">  remote_user: root</span><br><span class="line">  roles:</span><br><span class="line">    - kafka</span><br></pre></td></tr></table></figure><h4 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h4><p>在 test/ 目录下执行  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ansible 是通过 ssh 执行命令的,所以最佳实践中推荐的是设置 ssh 免密码登录</span><br><span class="line"># 如果没有设置,而已在执行命令后加 -k 参数,手动输入密码, 这里 root 用户的密码是 root</span><br><span class="line">ansible-playbook -i hosts run.yml -k</span><br></pre></td></tr></table></figure><p>至此,就可以在集群中部署完成 kafka</p><h4 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a><a href="https://github.com/waterandair/ansible-playbook-devops" target="_blank" rel="noopener">项目地址</a></h4><h4 id="kafka-常用配置介绍"><a href="#kafka-常用配置介绍" class="headerlink" title="kafka 常用配置介绍"></a>kafka 常用配置介绍</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">broker.id&#x3D;0  # 当前 kafka 节点在集群中的标识 </span><br><span class="line">port&#x3D;9092  # 对外提供服务的端口,建议不要设置为默认端口</span><br><span class="line">num.network.thread&#x3D;4  # 网络连接的线程数,建议默认</span><br><span class="line">num.io.thread&#x3D;8  # 处理文件io的线程数,这个参数与 log.dirs 参数相关,lo.dir可以配置多个,这个参数至少应该大于 log.dirs 中的目录数量,确保每个目录都有一个线程处理</span><br><span class="line"></span><br><span class="line">socket.send.buffer.bytes&#x3D;1048576  # 发送消息缓冲区,不是一点一点的发送,而是超过buffer再一起发送,可以提高性能</span><br><span class="line">socket.receive.buffer.bytes&#x3D;1048576  # 接收消息缓冲区</span><br><span class="line">socket.request.max.bytes&#x3D;104857600  # 向kafka请求的最大数, 不能超过java的堆栈大小</span><br><span class="line"></span><br><span class="line">log.dirs&#x3D;&#x2F;tmp&#x2F;kafka, &#x2F;tmp&#x2F;kafka2  # kafka 持久化目录</span><br><span class="line">log.retention.hours&#x3D;168  # 持久化保存的最大时间,默认是 7 天</span><br><span class="line">num.partitions&#x3D;2  # topic 的默认分区数</span><br><span class="line">zookeeper.connect&#x3D;ip:port,ip:port  # zookeeper 集群的服务器,用逗号分隔</span><br><span class="line"></span><br><span class="line">message.max.byte&#x3D;5048576  # kafka 每条消息能够容纳的最大大小</span><br><span class="line">default.replication.factor&#x3D;2  # 默认的副本数</span><br><span class="line">replica.fetch.max.bytes&#x3D;5048576  # 取消息的最大长度</span><br><span class="line"></span><br><span class="line">log.segment.bytes&#x3D;536870912  # 消息持久化文件的最大大小</span><br></pre></td></tr></table></figure><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>安装完成后,可以ssh登录集群上的服务器,进行测试，过程如图，注意观察所用的 ip， 只要是在集群中的 ip ， 都可以使用<br><img src="/images/install-kafka-test.png" alt="image"> </p><h4 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 查看当前服务器中的所有topic</span><br><span class="line">kafka-topics.sh --zookeeper 127.0.0.1:2181 --list</span><br><span class="line"></span><br><span class="line"># 创建 topic</span><br><span class="line"># --topic 定义topic名</span><br><span class="line"># --replication-factor  定义副本数</span><br><span class="line"># --partitions  定义分区数</span><br><span class="line">kafka-topics.sh --zookeeper 127.0.0.1:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br><span class="line"></span><br><span class="line"># 删除 topic </span><br><span class="line"># 需要server.properties中设置delete.topic.enable&#x3D;true否则只是标记删除或者直接重启。</span><br><span class="line">kafka-topics.sh --zookeeper 127.0.0.1:2181 --delete --topic first</span><br><span class="line"></span><br><span class="line"># 发送消息</span><br><span class="line">kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first</span><br><span class="line"></span><br><span class="line"># 消费消息</span><br><span class="line"># --from-beginning：会把first主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。</span><br><span class="line">kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic first</span><br><span class="line"></span><br><span class="line"># 查看 topic 详情</span><br><span class="line">kafka-topics.sh --zookeeper 127.0.0.1:2181 --describe --topic first</span><br></pre></td></tr></table></figure><h4 id="常用配置介绍"><a href="#常用配置介绍" class="headerlink" title="常用配置介绍"></a>常用配置介绍</h4><h5 id="Broker-配置"><a href="#Broker-配置" class="headerlink" title="Broker 配置"></a>Broker 配置</h5><table><thead><tr><th>配置项</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>broker.id=0</td><td></td><td>必填参数, 当前 kafka 节点在集群中的标一标识</td></tr><tr><td>port</td><td>9092</td><td>对外提供服务的端口,建议不要设置为默认端口</td></tr><tr><td>num.network.thread</td><td>4</td><td>网络连接的线程数,建议默认</td></tr><tr><td>num.io.thread</td><td>4</td><td>处理文件io的线程数,这个参数与 log.dirs参数相关,lo.dir可以配置多个,这个参数至少应该大于 log.dirs 中的目录数量,确保每个目录都有一个线程处理</td></tr><tr><td>socket.send.buffer.bytes</td><td>1048576</td><td>发送消息缓冲区,不是一点一点的发送,而是超过buffer再一起发送,可以提高性能</td></tr><tr><td>socket.receive.buffer.bytes</td><td>1048576</td><td>接收消息缓冲区</td></tr><tr><td>socket.request.max.bytes</td><td>104857600</td><td>向kafka请求的最大数, 不能超过java的堆栈大小</td></tr><tr><td>log.dirs</td><td>/tmp/kafka, /tmp/kafka2</td><td>kafka 持久化目录 可以指定多个目录，中间用逗号分隔，当新partition被创建的时会被存放到当前存放partition最少的目录。</td></tr><tr><td>log.retention.hours</td><td>168</td><td>持久化保存的最大时间,默认是 7 天</td></tr><tr><td>num.partitions</td><td>2</td><td>topic 的默认分区数</td></tr><tr><td>zookeeper.connect</td><td>ip:port,ip:port</td><td>zookeeper 集群的服务器,用逗号分隔</td></tr><tr><td>message.max.byte</td><td>5048576</td><td>kafka 每条消息能够容纳的最大大小</td></tr><tr><td>default.replication.factor</td><td>2</td><td>默认的副本数</td></tr><tr><td>replica.fetch.max.bytes</td><td>5048576</td><td>取消息的最大长度</td></tr><tr><td>log.segment.bytes</td><td>536870912</td><td>消息持久化文件的最大大小</td></tr><tr><td>message.max.bytes</td><td>1000000</td><td>服务器可以接收到的最大的消息大小。注意此参数要和consumer的maximum.message.size大小一致，否则会因为生产者生产的消息太大导致消费者无法消费。</td></tr><tr><td>num.io.threads</td><td>8</td><td>服务器用来执行读写请求的IO线程数，此参数的数量至少要等于服务器上磁盘的数量。</td></tr><tr><td>queued.max.requests</td><td>500</td><td>I/O线程可以处理请求的队列大小，若实际请求数超过此大小，网络线程将停止接收新的请求。</td></tr><tr><td>socket.send.buffer.bytes</td><td>100 * 1024</td><td>The SO_SNDBUFF buffer the server prefers for socket connections.</td></tr><tr><td>socket.receive.buffer.bytes</td><td>100 * 1024</td><td>The SO_RCVBUFF buffer the server prefers for socket connections.</td></tr><tr><td>socket.request.max.bytes</td><td>100 * 1024 * 1024</td><td>服务器允许请求的最大值， 用来防止内存溢出，其值应该小于 Java heap size.</td></tr><tr><td>num.partitions</td><td>1 默认partition数量</td><td>如果topic在创建时没有指定partition数量，默认使用此值，建议改为5</td></tr><tr><td>log.segment.bytes</td><td>1024 * 1024 * 1024</td><td>Segment文件的大小，超过此值将会自动新建一个segment，此值可以被topic级别的参数覆盖。</td></tr><tr><td>log.roll.{ms,hours}</td><td>24 * 7 hours</td><td>新建segment文件的时间，此值可以被topic级别的参数覆盖。</td></tr><tr><td>log.retention.{ms,minutes,hours}</td><td>7 days</td><td>Kafka segment log的保存周期，保存周期超过此时间日志就会被删除。此参数可以被topic级别参数覆盖。数据量大时，建议减小此值。</td></tr><tr><td>log.retention.bytes</td><td>-1</td><td>每个partition的最大容量，若数据量超过此值，partition数据将会被删除。注意这个参数控制的是每个partition而不是topic。此参数可以被log级别参数覆盖。</td></tr><tr><td>log.retention.check.interval.ms</td><td>5 minutes</td><td>删除策略的检查周期</td></tr><tr><td>auto.create.topics.enable</td><td>true</td><td>自动创建topic参数，建议此值设置为false，严格控制topic管理，防止生产者错写topic。</td></tr><tr><td>default.replication.factor</td><td>1</td><td>默认副本数量，建议改为2。</td></tr><tr><td>replica.lag.time.max.ms</td><td>10000</td><td>在此窗口时间内没有收到follower的fetch请求，leader会将其从ISR(in-sync replicas)中移除。</td></tr><tr><td>replica.lag.max.messages</td><td>4000</td><td>如果replica节点落后leader节点此值大小的消息数量，leader节点就会将其从ISR中移除。</td></tr><tr><td>replica.socket.timeout.ms</td><td>30 * 1000</td><td>replica向leader发送请求的超时时间。</td></tr><tr><td>replica.socket.receive.buffer.bytes</td><td>64 * 1024</td><td>The socket receive buffer for network requests to the leader for replicating data.</td></tr><tr><td>replica.fetch.max.bytes</td><td>1024 * 1024</td><td>The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.</td></tr><tr><td>replica.fetch.wait.max.ms</td><td>500</td><td>The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.</td></tr><tr><td>num.replica.fetchers</td><td>1</td><td>Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.</td></tr><tr><td>fetch.purgatory.purge.interval.requests</td><td>1000</td><td>The purge interval (in number of requests) of the fetch request purgatory.</td></tr><tr><td>zookeeper.session.timeout.ms</td><td>6000</td><td>ZooKeeper session 超时时间。如果在此时间内server没有向zookeeper发送心跳，zookeeper就会认为此节点已挂掉。 此值太低导致节点容易被标记死亡；若太高，.会导致太迟发现节点死亡。</td></tr><tr><td>zookeeper.connection.timeout.ms</td><td>6000</td><td>客户端连接zookeeper的超时时间。</td></tr><tr><td>zookeeper.sync.time.ms</td><td>2000</td><td>H ZK follower落后 ZK leader的时间。</td></tr><tr><td>controlled.shutdown.enable</td><td>true</td><td>允许broker shutdown。如果启用，broker在关闭自己之前会把它上面的所有leaders转移到其它brokers上，建议启用，增加集群稳定性。</td></tr><tr><td>auto.leader.rebalance.enable</td><td>true</td><td>If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available.</td></tr><tr><td>leader.imbalance.per.broker.percentage</td><td>10</td><td>The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.</td></tr><tr><td>leader.imbalance.check.interval.seconds</td><td>300</td><td>The frequency with which to check for leader imbalance.</td></tr><tr><td>offset.metadata.max.bytes</td><td>4096</td><td>The maximum amount of metadata to allow clients to save with their offsets.</td></tr><tr><td>connections.max.idle.ms</td><td>600000</td><td>Idle connections timeout: the server socket processor threads close the connections that idle more than this.</td></tr><tr><td>num.recovery.threads.per.data.dir</td><td>1</td><td>The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.</td></tr><tr><td>unclean.leader.election.enable</td><td>true</td><td>Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.</td></tr><tr><td>delete.topic.enable</td><td>false</td><td>启用deletetopic参数，建议设置为true。</td></tr><tr><td>offsets.topic.num.partitions</td><td>50</td><td>The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200).</td></tr><tr><td>offsets.topic.retention.minutes</td><td>1440</td><td>Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic.</td></tr><tr><td>offsets.retention.check.interval.ms</td><td>600000</td><td>The frequency at which the offset manager checks for stale offsets.</td></tr><tr><td>offsets.topic.replication.factor</td><td>3</td><td>The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas.</td></tr><tr><td>offsets.topic.segment.bytes</td><td>104857600</td><td>Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads.</td></tr><tr><td>offsets.load.buffer.size</td><td>5242880</td><td>An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager’s cache.</td></tr><tr><td>offsets.commit.required.acks</td><td>-1</td><td>The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer’s acknowledgement setting. In general, the default should not be overridden.</td></tr><tr><td>offsets.commit.timeout.ms</td><td>5000</td><td>The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout.</td></tr><tr><td>##### Producer 配置</td><td></td><td></td></tr></tbody></table><table><thead><tr><th>配置项</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>metadata.broker.list</td><td></td><td>启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。</td></tr><tr><td>request.required.acks</td><td>0</td><td></td></tr><tr><td>request.timeout.ms</td><td>10000</td><td>Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。</td></tr><tr><td>producer.type</td><td>sync</td><td>同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。</td></tr><tr><td>serializer.class</td><td>kafka.serializer.DefaultEncoder</td><td>序列号类，.默认序列化成 byte[] 。</td></tr><tr><td>key.serializer.class</td><td></td><td>Key的序列化类，默认同上。</td></tr><tr><td>partitioner.class</td><td>kafka.producer.DefaultPartitioner</td><td>Partition类，默认对key进行hash。</td></tr><tr><td>compression.codec</td><td>none</td><td>指定producer消息的压缩格式，可选参数为： “none”, “gzip” and “snappy”。关于压缩参见4.1节</td></tr><tr><td>compressed.topics</td><td>null</td><td>启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。</td></tr><tr><td>message.send.max.retries</td><td>3</td><td>Producer发送失败时重试次数。若网络出现问题，可能会导致不断重试。</td></tr><tr><td>retry.backoff.ms</td><td>100</td><td>Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.</td></tr><tr><td>topic.metadata.refresh.interval.ms</td><td>600 * 1000</td><td>The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). If you set this to a negative value, metadata will only get refreshed on failure. If you set this to zero, the metadata will get refreshed after each message sent (not recommended). Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed</td></tr><tr><td>queue.buffering.max.ms</td><td>5000</td><td>启用异步模式时，producer缓存消息的时间。比如我们设置成1000时，它会缓存1秒的数据再一次发送出去，这样可以极大的增加broker吞吐量，但也会造成时效性的降低。</td></tr><tr><td>queue.buffering.max.messages</td><td>10000</td><td>采用异步模式时producer buffer 队列里最大缓存的消息数量，如果超过这个数值，producer就会阻塞或者丢掉消息。</td></tr><tr><td>queue.enqueue.timeout.ms</td><td>-1</td><td>当达到上面参数值时producer阻塞等待的时间。如果值设置为0，buffer队列满时producer不会阻塞，消息直接被丢掉。若值设置为-1，producer会被阻塞，不会丢消息。</td></tr><tr><td>batch.num.messages</td><td>200</td><td>采用异步模式时，一个batch缓存的消息数量。达到这个数量值时producer才会发送消息。</td></tr><tr><td>send.buffer.bytes</td><td>100 * 1024</td><td>Socket write buffer size</td></tr><tr><td>client.id</td><td>“”</td><td>The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.</td></tr></tbody></table><h5 id="Consumer-配置"><a href="#Consumer-配置" class="headerlink" title="Consumer 配置"></a>Consumer 配置</h5><table><thead><tr><th>配置项</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>group.id</td><td></td><td>Consumer的组ID，相同goup.id的consumer属于同一个组。</td></tr><tr><td>zookeeper.connect</td><td></td><td>Consumer的zookeeper连接串，要和broker的配置一致。</td></tr><tr><td>consumer.id</td><td>null</td><td>如果不设置会自动生成。</td></tr><tr><td>socket.timeout.ms</td><td>30 * 1000</td><td>网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。</td></tr><tr><td>socket.receive.buffer.bytes</td><td>64 * 1024</td><td>The socket receive buffer for network requests.</td></tr><tr><td>fetch.message.max.bytes</td><td>1024 * 1024</td><td>查询topic-partition时允许的最大消息大小。consumer会为每个partition缓存此大小的消息到内存，因此，这个参数可以控制consumer的内存使用量。这个值应该至少比server允许的最大消息大小大，以免producer发送的消息大于consumer允许的消息。</td></tr><tr><td>num.consumer.fetchers</td><td>1</td><td>The number fetcher threads used to fetch data.</td></tr><tr><td>auto.commit.enable</td><td>true</td><td>如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。</td></tr><tr><td>auto.commit.interval.ms</td><td>60 * 1000</td><td>Consumer提交offset值到zookeeper的周期。</td></tr><tr><td>queued.max.message.chunks</td><td>2</td><td>用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。</td></tr><tr><td>auto.commit.interval.ms</td><td>60 * 1000</td><td>Consumer提交offset值到zookeeper的周期。</td></tr><tr><td>queued.max.message.chunks</td><td>2</td><td>用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。</td></tr><tr><td>fetch.min.bytes</td><td>1</td><td>The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.</td></tr><tr><td>fetch.wait.max.ms</td><td>100</td><td>The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy fetch.min.bytes.</td></tr><tr><td>rebalance.backoff.ms</td><td>2000</td><td>Backoff time between retries during rebalance.</td></tr><tr><td>refresh.leader.backoff.ms</td><td>200</td><td>Backoff time to wait before trying to determine the leader of a partition that has just lost its leader.</td></tr><tr><td>auto.offset.reset</td><td>largest</td><td>What to do when there is no initial offset in ZooKeeper or if an offset is out of range ;smallest : automatically reset the offset to the smallest offset; largest : automatically reset the offset to the largest offset;anything else: throw exception to the consumer</td></tr><tr><td>consumer.timeout.ms</td><td>-1</td><td>若在指定时间内没有消息消费，consumer将会抛出异常。</td></tr><tr><td>exclude.internal.topics</td><td>true</td><td>Whether messages from internal topics (such as offsets) should be exposed to the consumer.</td></tr><tr><td>zookeeper.session.timeout.ms</td><td>6000</td><td>ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur.</td></tr><tr><td>zookeeper.connection.timeout.ms</td><td>6000</td><td>The max time that the client waits while establishing a connection to zookeeper.</td></tr><tr><td>zookeeper.sync.time.ms</td><td>2000</td><td>How far a ZK follower can be behind a ZK leader</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 ansible 部署 zookeeper 环境</title>
      <link href="/2017-11-28-install-zookeeper.html"/>
      <url>/2017-11-28-install-zookeeper.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>使用 ansible 部署 zookeeper 环境  <a href="https://github.com/waterandair/ansible-playbook-devops" target="_blank" rel="noopener">项目地址</a></p></blockquote><a id="more"></a><blockquote><p>zookeeper 依赖 java ,所以安装 zookeeper 前要前确定有没有java,测试方法就是在命令行中运行 <code>java</code>, 看能不能找到命令.如果没有安装java(jdk)环境的话,可以参考<a href="http://waterandair.top/2018/03/22/install-jdk/" target="_blank" rel="noopener">java(jdk) 环境</a>, 如果使用 docker ,可以直接获取一个具有 ssh 和 java 环境的镜像,这里用我自己提交的 <code>waterandair/sshd</code> 镜像,具体操作如下:</p></blockquote><h4 id="创建容器-用虚拟机或服务器可忽略"><a href="#创建容器-用虚拟机或服务器可忽略" class="headerlink" title="创建容器 (用虚拟机或服务器可忽略)"></a>创建容器 (用虚拟机或服务器可忽略)</h4><h5 id="拉取基础镜像"><a href="#拉取基础镜像" class="headerlink" title="拉取基础镜像"></a>拉取基础镜像</h5><p>获取一个具备 ssh和java 的 ubuntu linux<br>基础镜像(<a href="http://waterandair.top/docker-base-use.html/" target="_blank" rel="noopener">关于docker的基本使用</a>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull waterandair&#x2F;jdk</span><br></pre></td></tr></table></figure><h5 id="创建基础容器-数量随机-学习中建议-3-个"><a href="#创建基础容器-数量随机-学习中建议-3-个" class="headerlink" title="创建基础容器(数量随机,学习中建议 3 个)"></a>创建基础容器(数量随机,学习中建议 3 个)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 登录名&#x2F;密码:  root&#x2F;root</span><br><span class="line">docker run -d --name&#x3D;zookeeper1 waterandair&#x2F;jdk</span><br><span class="line">docker run -d --name&#x3D;zookeeper2 waterandair&#x2F;jdk</span><br><span class="line">docker run -d --name&#x3D;zookeeper3 waterandair&#x2F;jdk</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="初始化-role"><a href="#初始化-role" class="headerlink" title="初始化 role"></a>初始化 role</h4><p>在任意目录(这里以test文件夹为例)下执行  <code>ansible-galaxy init zookeeper</code> ,初始化一个 roles </p><h4 id="设置要用到的变量"><a href="#设置要用到的变量" class="headerlink" title="设置要用到的变量"></a>设置要用到的变量</h4><p>在 test/zookeeper/defaults/main.yml 中设置变量  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># defaults file for zookeeper</span><br><span class="line"># 版本</span><br><span class="line">zookeeper_version: 3.4.10</span><br><span class="line"># 下载地址</span><br><span class="line">zookeeper_download_url: &quot;http:&#x2F;&#x2F;mirrors.shu.edu.cn&#x2F;apache&#x2F;zookeeper&#x2F;stable&#x2F;zookeeper-&#123;&#123; zookeeper_version &#125;&#125;.tar.gz&quot;</span><br><span class="line"># 下载到本地的 ~&#x2F;Downloads</span><br><span class="line">zookeeper_download_dest: ~&#x2F;Downloads&#x2F;zookeeper-&#123;&#123; zookeeper_version &#125;&#125;.tar.gz</span><br><span class="line"># 安装目录</span><br><span class="line">install_dir: &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"># 软连接地址</span><br><span class="line">soft_dir: &quot;&#123;&#123; install_dir &#125;&#125;zookeeper&quot;</span><br><span class="line"># zookeeper 快照文件目录</span><br><span class="line">zookeeper_data_dir: &#x2F;var&#x2F;zookeeper&#x2F;</span><br><span class="line"># 设置环境变量的文件</span><br><span class="line">env_file: ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><h4 id="准备-zookeeper-配置文件的模板"><a href="#准备-zookeeper-配置文件的模板" class="headerlink" title="准备 zookeeper 配置文件的模板"></a>准备 zookeeper 配置文件的模板</h4><p>先了解一下集群的集中模式</p><h5 id="集群的模式"><a href="#集群的模式" class="headerlink" title="集群的模式"></a>集群的模式</h5><p>zookeeper 有三种集群模式,配置简单,只需要在配置文件中修改集群中节点列表就可以,示例:</p><h6 id="集群模式"><a href="#集群模式" class="headerlink" title="集群模式:"></a>集群模式:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># zoo.cfg</span><br><span class="line"># 省略其他配置项......</span><br><span class="line"></span><br><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br><span class="line">server.2&#x3D;172.17.0.3:2888:3888</span><br><span class="line">server.3&#x3D;172.17.0.4:2888:3888</span><br></pre></td></tr></table></figure><h6 id="伪集群模式"><a href="#伪集群模式" class="headerlink" title="伪集群模式:"></a>伪集群模式:</h6><p>在单节点运行多个 zookeeper 服务, 端口设置为不同的端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># zoo.cfg</span><br><span class="line"># 省略其他配置项......</span><br><span class="line"></span><br><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br><span class="line">server.2&#x3D;172.17.0.2:2889:3889</span><br><span class="line">server.3&#x3D;172.17.0.2:2890:3890</span><br></pre></td></tr></table></figure><h6 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式:"></a>单机模式:</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># zoo.cfg</span><br><span class="line"># 省略其其他部分......</span><br><span class="line"></span><br><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br></pre></td></tr></table></figure><h5 id="修改模板文件"><a href="#修改模板文件" class="headerlink" title="修改模板文件"></a>修改模板文件</h5><p>在 test/zookeeper/templates/ 目录下新建文件 zoo_sample.cfg.j2  </p><p>这里主要配置zookeeper快照目录 data_dir 和 zookeeper 集群节点列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># The number of milliseconds of each tick</span><br><span class="line">tickTime&#x3D;2000</span><br><span class="line"># The number of ticks that the initial </span><br><span class="line"># synchronization phase can take</span><br><span class="line">initLimit&#x3D;10</span><br><span class="line"># The number of ticks that can pass between </span><br><span class="line"># sending a request and getting an acknowledgement</span><br><span class="line">syncLimit&#x3D;5</span><br><span class="line"># the directory where the snapshot is stored.</span><br><span class="line"># do not use &#x2F;tmp for storage, &#x2F;tmp here is just </span><br><span class="line"># example sakes.</span><br><span class="line"># zookeeper 的快照文件目录</span><br><span class="line">dataDir&#x3D;&#123;&#123; zookeeper_data_dir &#125;&#125;</span><br><span class="line"># the port at which the clients will connect</span><br><span class="line">clientPort&#x3D;2181</span><br><span class="line"># the maximum number of client connections.</span><br><span class="line"># increase this if you need to handle more clients</span><br><span class="line">#maxClientCnxns&#x3D;60</span><br><span class="line">#</span><br><span class="line"># Be sure to read the maintenance section of the</span><br><span class="line"># administrator guide before turning on autopurge.</span><br><span class="line">#</span><br><span class="line"># http:&#x2F;&#x2F;zookeeper.apache.org&#x2F;doc&#x2F;current&#x2F;zookeeperAdmin.html#sc_maintenance</span><br><span class="line">#</span><br><span class="line"># The number of snapshots to retain in dataDir</span><br><span class="line">#autopurge.snapRetainCount&#x3D;3</span><br><span class="line"># Purge task interval in hours</span><br><span class="line"># Set to &quot;0&quot; to disable auto purge feature</span><br><span class="line">#autopurge.purgeInterval&#x3D;1</span><br><span class="line">&#123;#</span><br><span class="line">zookeeper 集群中的所有节点的信息</span><br><span class="line">ansible_play_batch 变量可以获得当前任务中的hosts列表</span><br><span class="line">执行后,形如:</span><br><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br><span class="line">server.2&#x3D;172.17.0.3:2888:3888</span><br><span class="line">server.3&#x3D;172.17.0.4:2888:3888</span><br><span class="line">#&#125;</span><br><span class="line">&#123;% for host in ansible_play_batch %&#125;</span><br><span class="line">server.&#123;&#123; id &#125;&#125;&#x3D;&#123;&#123; host &#125;&#125;:2888:3888</span><br><span class="line">&#123;% endfor %&#125;</span><br></pre></td></tr></table></figure><h4 id="编写-tasks"><a href="#编写-tasks" class="headerlink" title="编写 tasks"></a>编写 tasks</h4><p>如果一个 tasks 有很多步骤,可以把它们分置在不同的文件中,最后在 test/z ookeeper/tasks/main.yml 文件中引用他们,这样做方便调试,方便阅读</p><h5 id="编写下载过程"><a href="#编写下载过程" class="headerlink" title="编写下载过程"></a>编写下载过程</h5><p>把 zookeeper 安装文件下载到控制节点, 然后解压到目标主机指定目录下.<br>在 test/zookeeper/tasks/ 下新建文件 download.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># 把文件下载到本地</span><br><span class="line">- name: 判断 zookeeper 源文件是否已经下载好了</span><br><span class="line">  command: ls &#123;&#123; zookeeper_download_dest &#125;&#125;</span><br><span class="line">  ignore_errors: true</span><br><span class="line">  register: file_exist</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 下载文件</span><br><span class="line">  get_url:</span><br><span class="line">    url: &quot;&#123;&#123; zookeeper_download_url&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; zookeeper_download_dest &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line">  when: file_exist|failed</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line"># 解压到远程服务器的安装目录</span><br><span class="line">- name: 解压</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123; zookeeper_download_dest &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br></pre></td></tr></table></figure><h5 id="编写配置和启动过程"><a href="#编写配置和启动过程" class="headerlink" title="编写配置和启动过程"></a>编写配置和启动过程</h5><p>在 test/zookeeper/tasks/ 下新建文件 init.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- name: 创建软连接</span><br><span class="line">  file:</span><br><span class="line">    src: &quot;&#123;&#123; install_dir + &#39;zookeeper-&#39; + zookeeper_version&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; soft_dir &#125;&#125;&quot;</span><br><span class="line">    state: link</span><br><span class="line"></span><br><span class="line">- name: 设置zookeeper环境变量</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123;env_file&#125;&#125;&quot;</span><br><span class="line">    insertafter: &quot;&#123;&#123;item.position&#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123;item.value&#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">    - &#123;position: EOF, value: &quot;\n&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;&#123;&#123; &#39;export ZOOKEEPER_HOME&#x3D;&#39; + soft_dir &#125;&#125;&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;export PATH&#x3D;$ZOOKEEPER_HOME&#x2F;bin:$PATH&quot;&#125;</span><br><span class="line"></span><br><span class="line">#- name: 使设置的环境变量生效</span><br><span class="line">#  shell: source &#123;&#123;env_file&#125;&#125;</span><br><span class="line">#  args:</span><br><span class="line">#     executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">- name: 设置配置文件</span><br><span class="line">  template:</span><br><span class="line">    src: zoo_sample.cfg.j2</span><br><span class="line">    dest: &quot;&#123;&#123; soft_dir + &#39;&#x2F;conf&#x2F;zoo.cfg&#39; &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">- name: 设置服务器标识(重要)</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123;zookeeper_data_dir + &#39;myid&#39;&#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123; id &#125;&#125;&quot;</span><br><span class="line">    create: yes</span><br><span class="line">    state: present</span><br><span class="line"></span><br><span class="line"># 这里启动需要指定 zookeeper 和 java 的路径</span><br><span class="line">- name: 启动 zookeeper 服务器</span><br><span class="line">  shell: zkServer.sh start</span><br><span class="line">  environment:</span><br><span class="line">    PATH: &#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;java&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin</span><br></pre></td></tr></table></figure><h4 id="合并所有tasks"><a href="#合并所有tasks" class="headerlink" title="合并所有tasks"></a>合并所有tasks</h4><p>在 test/zookeeper/tasks/ 下修改 main.yml 文件  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for zookeeper</span><br><span class="line">#</span><br><span class="line">- import_tasks: download.yml</span><br><span class="line">####</span><br><span class="line">- import_tasks: init.yml</span><br></pre></td></tr></table></figure><h4 id="编写-hosts-文件"><a href="#编写-hosts-文件" class="headerlink" title="编写 hosts 文件"></a>编写 hosts 文件</h4><p>在 test/ 目录下新建文件 hosts</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 这里的id很重要,用于配置zookeeper在集群中的标识</span><br><span class="line">[test]</span><br><span class="line">172.17.0.2 id&#x3D;1</span><br><span class="line">172.17.0.3 id&#x3D;2</span><br><span class="line">172.17.0.4 id&#x3D;3</span><br></pre></td></tr></table></figure><h4 id="编写启动文件"><a href="#编写启动文件" class="headerlink" title="编写启动文件"></a>编写启动文件</h4><p>在 test/ 目录下新建 run.yml 文件  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  remote_user: root</span><br><span class="line">  roles:</span><br><span class="line">    - zookeeper</span><br></pre></td></tr></table></figure><h4 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h4><p>在 test/ 目录下执行  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ansible 是通过 ssh 执行命令的,所以最佳实践中推荐的是设置 ssh 免密码登录</span><br><span class="line"># 如果没有设置,而已在执行命令后加 -k 参数,手动输入密码, 这里 root 用户的密码是 root</span><br><span class="line">ansible-playbook -i hosts run.yml -k</span><br></pre></td></tr></table></figure><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>安装完成后,可以ssh登录集群上的服务器,执行 zkServer status 查看 zookeeper 服务运行的情况,注意查看 mode 项, 集群模式下, zookeeper 会自动选择一个节点为 leader, 其余节点为 follower<br><img src="/images/install-zookeeper-test.png" alt="image"> </p><h4 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h4><h5 id="客户端常用操作"><a href="#客户端常用操作" class="headerlink" title="客户端常用操作"></a>客户端常用操作</h5><h6 id="启动客户端"><a href="#启动客户端" class="headerlink" title="启动客户端"></a>启动客户端</h6><p><code>shell&gt; bin/zkCli.sh</code></p><h6 id="显示所有操作命令"><a href="#显示所有操作命令" class="headerlink" title="显示所有操作命令"></a>显示所有操作命令</h6><p><code>zookeeper&gt; help</code></p><h6 id="查看当前znode中所包含的内容"><a href="#查看当前znode中所包含的内容" class="headerlink" title="查看当前znode中所包含的内容"></a>查看当前znode中所包含的内容</h6><p><code>zookeeper&gt; ls /</code></p><h6 id="查看当前节点数据并能看到更新次数等数据"><a href="#查看当前节点数据并能看到更新次数等数据" class="headerlink" title="查看当前节点数据并能看到更新次数等数据"></a>查看当前节点数据并能看到更新次数等数据</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; ls2 &#x2F;</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls2 &#x2F;</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid &#x3D; 0x0</span><br><span class="line">ctime &#x3D; Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid &#x3D; 0x0</span><br><span class="line">mtime &#x3D; Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid &#x3D; 0x0</span><br><span class="line">cversion &#x3D; -1</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 0</span><br><span class="line">numChildren &#x3D; 1</span><br></pre></td></tr></table></figure><h6 id="创建普通节点"><a href="#创建普通节点" class="headerlink" title="创建普通节点"></a>创建普通节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; create &#x2F;test1 &quot;this is test1&quot;</span><br><span class="line">Created &#x2F;test1</span><br><span class="line">zookeeper&gt; create &#x2F;test1&#x2F;server-01 &quot;127.0.0.1&quot;</span><br><span class="line">Created &#x2F;test1&#x2F;server-01</span><br></pre></td></tr></table></figure><h6 id="获得节点的值"><a href="#获得节点的值" class="headerlink" title="获得节点的值"></a>获得节点的值</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; get &#x2F;test1</span><br><span class="line">this is test1</span><br><span class="line">cZxid &#x3D; 0x10000000a</span><br><span class="line">ctime &#x3D; Mon Aug 20 18:05:42 CST 2018</span><br><span class="line">mZxid &#x3D; 0x10000000a</span><br><span class="line">mtime &#x3D; Mon Aug 20 18:05:42 CST 2018</span><br><span class="line">pZxid &#x3D; 0x10000000b</span><br><span class="line">cversion &#x3D; 1</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 13</span><br><span class="line">numChildren &#x3D; 1</span><br><span class="line"></span><br><span class="line">zookeeper&gt; get &#x2F;test1&#x2F;server-01</span><br><span class="line">127.0.0.1</span><br><span class="line">cZxid &#x3D; 0x10000000b</span><br><span class="line">ctime &#x3D; Mon Aug 20 18:06:28 CST 2018</span><br><span class="line">mZxid &#x3D; 0x10000000b</span><br><span class="line">mtime &#x3D; Mon Aug 20 18:06:28 CST 2018</span><br><span class="line">pZxid &#x3D; 0x10000000b</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 9</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure><h6 id="创建临时节点"><a href="#创建临时节点" class="headerlink" title="创建临时节点"></a>创建临时节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; create -e &#x2F;test-emphemeral &quot;this is emphemeral znode&quot;</span><br><span class="line">Created &#x2F;test-emphemeral</span><br><span class="line"></span><br><span class="line">zookeeper&gt; ls &#x2F;</span><br><span class="line">[zookeeper, test1， test-emphemeral]</span><br><span class="line"></span><br><span class="line">zookeeper&gt; quit </span><br><span class="line"></span><br><span class="line"># 客户端退出后重新连接服务器，发现临时节点已经删除</span><br><span class="line">zookeeper&gt; ls &#x2F;</span><br><span class="line">[zookeeper, test1]</span><br></pre></td></tr></table></figure><h6 id="创建带序号的节点"><a href="#创建带序号的节点" class="headerlink" title="创建带序号的节点"></a>创建带序号的节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; create -s &#x2F;test1&#x2F;a 10</span><br><span class="line">Created &#x2F;test1&#x2F;a0000000001</span><br><span class="line">zookeeper&gt; create -s &#x2F;test1&#x2F;b 10</span><br><span class="line">Created &#x2F;test1&#x2F;b0000000002</span><br><span class="line">zookeeper&gt; create -s &#x2F;test1&#x2F;c 10</span><br><span class="line">Created &#x2F;test1&#x2F;c0000000003</span><br></pre></td></tr></table></figure><h6 id="修改节点数据值"><a href="#修改节点数据值" class="headerlink" title="修改节点数据值"></a>修改节点数据值</h6><p><code>set /test1 &quot;updated value&quot;</code></p><h6 id="节点的值变化监听"><a href="#节点的值变化监听" class="headerlink" title="节点的值变化监听"></a>节点的值变化监听</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 打开 客户端1， 在主机上注册监听 &#x2F;test1 节点数据变化</span><br><span class="line">zookeeper1&gt; get &#x2F;test1 watch</span><br><span class="line"># 打开 客户端2， 修改 &#x2F;test1 的值</span><br><span class="line">zookeeper2&gt; set &#x2F;test1 &quot;change data&quot;</span><br><span class="line"># 观察 客户端1 收到数据变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:&#x2F;test1</span><br></pre></td></tr></table></figure><h6 id="节点的子节点变化监听（路径变化）"><a href="#节点的子节点变化监听（路径变化）" class="headerlink" title="节点的子节点变化监听（路径变化）"></a>节点的子节点变化监听（路径变化）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 打开 客户端1，注册监听 &#x2F;test1 节点的子节点变化</span><br><span class="line">zookeeper&gt;  ls &#x2F;test1 watch</span><br><span class="line">[server-01, b0000000002, a0000000001, c0000000003, a0000000004]</span><br><span class="line"></span><br><span class="line"># 打开 客户端2，在 &#x2F;test1 下创建子节点</span><br><span class="line">zookeeper&gt; create &#x2F;test1&#x2F;test &quot;test&quot;</span><br><span class="line">Created &#x2F;test1&#x2F;test</span><br><span class="line"></span><br><span class="line"># 观察 客户端1 收到子节点变化的监听</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:&#x2F;test1</span><br></pre></td></tr></table></figure><h6 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h6><p> <code>zookeeper&gt; delete /test1/test</code></p><h6 id="递归删除节点"><a href="#递归删除节点" class="headerlink" title="递归删除节点"></a>递归删除节点</h6><p><code>zookeeper&gt; rmr /test2</code></p><h6 id="查看节点状态"><a href="#查看节点状态" class="headerlink" title="查看节点状态"></a>查看节点状态</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&gt; stat &#x2F;test1</span><br><span class="line">cZxid &#x3D; 0x10000000a</span><br><span class="line">ctime &#x3D; Mon Aug 20 18:05:42 CST 2018</span><br><span class="line">mZxid &#x3D; 0x100000022</span><br><span class="line">mtime &#x3D; Mon Aug 20 18:32:06 CST 2018</span><br><span class="line">pZxid &#x3D; 0x100000023</span><br><span class="line">cversion &#x3D; 6</span><br><span class="line">dataVersion &#x3D; 5</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 12</span><br><span class="line">numChildren &#x3D; 6</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zookeeper概述(应用场景及基本概念)</title>
      <link href="/2017-11-25-zookeeper-intro.html"/>
      <url>/2017-11-25-zookeeper-intro.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>ZooKeeper 是一个高性能的分布式数据一致性解决方案,它将那些复杂的,容易出错的分布式一致性服务封装起来,构成一个高效可靠的原语集,并提供一系列简单的接口给用户使用  </p></blockquote><a id="more"></a>  <h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><h5 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h5><ul><li>leader 是整个 zookeeper 集群的工作机制中的核心</li><li>follower 是 Zookeeper 集群状态的跟随者</li><li>observer 充当一个观察者的角色</li></ul><h5 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h5><p>客户端与 zookeeper 服务器建立一个 tcp 长连接来维持一个 session 会话,客户端能够通过心跳检测与服务器保持有效的会话,也能向 zookeeper 服务器发送请求并获得响应。</p><h5 id="数据节点"><a href="#数据节点" class="headerlink" title="数据节点"></a>数据节点</h5><p>zookeeper 的数据模型是树形结构,树的节点称为 <code>znode</code>, znode中可以保存信息。</p><h6 id="两种类型"><a href="#两种类型" class="headerlink" title="两种类型"></a>两种类型</h6><ul><li>临时节点(ephemeral): 客户端和服务器断开连接后，创建的节点自动删除</li><li>持久节点(persistent): 客户端和服务器断开连接后，创建的节点不删除</li></ul><h6 id="四种形式-默认persistent"><a href="#四种形式-默认persistent" class="headerlink" title="四种形式(默认persistent)"></a>四种形式(默认persistent)</h6><ul><li>持久化目录节点(persistent)<br>断开连接后，该节点依旧存在</li><li>持久化顺序编号目录节点(persistent_sequential)<br>断开连接后，该节点依旧存在，只是 zookeeper 给该节点名称进行顺序编号</li><li>临时目录节点(ephemeral)<br>断开连接后，该节点被删除</li><li>临时顺序编号目录节点(ephemeral_sequential)<br>断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号</li></ul><p>创建 znode 时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。<br>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序。 </p><h6 id="悲观锁和乐观锁"><a href="#悲观锁和乐观锁" class="headerlink" title="悲观锁和乐观锁"></a>悲观锁和乐观锁</h6><p>悲观锁是数据库中一种非常严格的锁策略,具有强烈的排他性,悲观锁认为事务访问相同数据的时候一定会出现相互干扰,所以简单粗暴的使用排他访问的方式。  </p><p>乐观锁认为不同事务访问相同资源是很少出现相互干扰的情况,因此在事务处理期间不需要进行并发控制,只有真正出现不一致的情况,才会控制。  </p><p>乐观锁,为了减少并发控制,对于数据库通常的做法是在每个表中增加一个 version 版本字段,事务修改数据之前先读出数据（包含版本号）,然后把这个读取出来的版本号加入到更新语句的条件中,如果更新失败了,就说明这个数据从读取出到更新前,已经被修改过了,这时候可以交由客户端抛出异常,或者选择重新获取数据。  </p><h6 id="zookeeper-的版本"><a href="#zookeeper-的版本" class="headerlink" title="zookeeper 的版本"></a>zookeeper 的版本</h6><p>zookeeper 的版本号就是起到并发状态下数据的一致性作用,zookeeper 中的版本类型有:  </p><ul><li>version: 当前数据节点数据内容的版本号</li><li>cversion: 当前数据节点子节点的版本号</li><li>aversion: 当前数据节点 ACL 变更版本号  </li></ul><h5 id="watcher"><a href="#watcher" class="headerlink" title="watcher"></a>watcher</h5><p>zookeeper 允许用户在指定节点上注册一些 watcher,当数据节点发生变化的时候,zookeeper 服务器会把这个变化的通知发送给感兴趣的客户端。</p><h5 id="ACL-权限控制"><a href="#ACL-权限控制" class="headerlink" title="ACL 权限控制"></a>ACL 权限控制</h5><p>ACL 是 Access Control Lists 的简写,Zookeeper 采用 ACL 策略来进行权限控制,有以下权限:  </p><ul><li>CREATE: 创建子节点的权限  </li><li>READ: 获取节点数据和子节点列表的权限  </li><li>WRITE: 更新及诶单数据的权限</li><li>DELETE: 删除子节点的权限</li><li>ADMIN: 设置节点 ACL 的权限  </li></ul><h5 id="stat-节点状态"><a href="#stat-节点状态" class="headerlink" title="stat 节点状态"></a>stat 节点状态</h5><ul><li>czxid: 引起这个 znode 创建的 zxid，每次修改 zookeeper 状态都会收到一个 zxid 形式的时间戳，也就是 zookeeper 事务id。  </li><li>ctime: znode 被创建的毫秒数(从1970年开始)</li><li>mzxid: znode 最后更新的 zxid</li><li>mtime: znode最后修改的毫秒数(从1970年开始)</li><li>pzxid: znode最后更新的子节点zxid</li><li>cversion: znode子节点变化号，znode子节点修改次数</li><li>dataversion - znode数据变化号</li><li>aclVersion - znode访问控制列表的变化号</li><li>ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id,如果不是临时节点则是0。</li><li>dataLength- znode的数据长度</li><li>numChildren - znode子节点数量</li></ul><h4 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h4><h5 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h5><ul><li>半数机制（Paxos 协议）: 集群中半数以上机器存活时集群可用，所以zookeeper适合装在奇数台机器上。</li><li>zookeeper 工作时，有一个节点为leader，其他为follower，leader是通过内部的选举机制临时产生的。  </li></ul><p>假设有 5 台服务器组成的 zookeeper 集群，它们的 id 为1-5，这些服务器依序启动:</p><ol><li><code>服务器1</code>启动，此时它发出去的报没有任何响应，所以它的选举状态一直是 LOOKING 状态。  </li><li><code>服务器2</code>启动，与<code>服务器1</code>进行通信，互相交换选举结果，由于两者都没有历史数据，所以 id 值较大的<code>服务器2</code>胜出，但是没有达到半数以上的服务器都同意选举<code>服务器2</code>,所以<code>服务器1、2</code>还是继续保持 LOOKING 状态。</li><li><code>服务器3</code>启动，根据 id 值选举，此时有三台服务器选举了<code>服务器3</code>，它成为了这次选举的 leader。</li><li><code>服务器4</code>启动，由于前面已经有半数以上的服务器选举了服务器3，所以它只能成为 follower。</li><li><code>服务器5</code>启动，同第 4 步，成为 follower。</li></ol><h5 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h5><ol><li>假设 client 向 zookeeper 集群的 server1 上发送一个写请求。</li><li>如果 server1 不是 leader，则会把接受到的请求转发给 leader，leader 会将写请求广播给各个server， 各个 server 写成功后会通知 leader。</li><li>当 leader 收到大多数 server 数据写成功的消息，就认为数据写成功了，之后 leader 会通知 server1 数据写成功了。</li><li>server1 收到 leader 返回的成功消息后，会通知 client 数据写成功了，整个写操作成功。</li></ol><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><h5 id="数据发布-订阅"><a href="#数据发布-订阅" class="headerlink" title="数据发布/订阅"></a>数据发布/订阅</h5><p>发布者将数据发布到 zk 集群上,客户端订阅感兴趣节点,当服务器在这些节点的数据发生变化时,就通知客户端,客户端得到通知后就可以到服务器获取更新的信息。  </p><h6 id="统一配置管理"><a href="#统一配置管理" class="headerlink" title="统一配置管理"></a>统一配置管理</h6><ul><li>把配置文件的数据发布到 zk 集群的某个节点上</li><li>应用启动时主动到 zk 上获取配置信息，并注册 watcher 监听</li><li>当配置需要修改时，直接修改 zk 上存储的数据</li><li>zk 推送变更给应用，触发 watcher 的回调函数</li><li>应用根据逻辑，主动获取新的配置信息</li></ul><h5 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h5><p>用 db 举例:  </p><ol><li>DB 在启动的时候为自己在 zk 上注册一个临时节点,临时节点在服务器出现问题的时候会自动的从 zk 上删除, 这样 zk 上永远都是最新可用的节点列表。</li><li>客户端在需要读写 DB 的时候,先去 zookeeper 得到所有可用的 DB 的连接的列表。</li><li>客户端随机选择一个节点与之建立连接。</li><li>当客户端发现链接不可用的时候,再次从 zk 上获取可用的节点,也可以在刚刚获取的那个列表里移除掉不可以给的节点,再随机选择一个进行连接。</li></ol><h5 id="命名服务"><a href="#命名服务" class="headerlink" title="命名服务"></a>命名服务</h5><p>以数据库表的 id 举例:<br>数据库的id可以是自增id 或 UUID, 自增id只能在单库单表中使用,UUID 可以在分布式中使用但字段没有规律，难于理解。可以借助 zk 生成一个可以在集群中使用的，顺序增长的,命名易于理解的 id。</p><h5 id="分布式协调-通知"><a href="#分布式协调-通知" class="headerlink" title="分布式协调/通知"></a>分布式协调/通知</h5><p>在分布式系统中,常常需要知道某个机器是否可用,传统的开发中,可以通过 ping 某个主机来实现,ping得通说明对方是可用的. zk 中让所有机器都注册一个临时节点,判断一个机器是否可用,只需要判断这个节点在 zk 中是否存在就可以了,不需要直接去连接需要检查的服务器,降低系统的复杂度。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 分布式协作框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis Replication 原理是实践</title>
      <link href="/2017-11-15-redis-replication.html"/>
      <url>/2017-11-15-redis-replication.html</url>
      
        <content type="html"><![CDATA[<p>Redis 的高可用策略包括持久化,复制,哨兵和集群,本文讲解redis的复制策略.一般redis的单机QPS可以轻松过万,对于一般的系统足够了,但对于一些高并发的场景,比如商品详情页这种读多写少的应用,QPS可能会达到十万百万,但写操作甚少.为了使Redis缓存满足需求,需求对其进行水平扩展.</p><a id="more"></a><h3 id="redis-replication-简介"><a href="#redis-replication-简介" class="headerlink" title="redis replication 简介"></a>redis replication 简介</h3><p>redis 采用master/slave架构,用异步方式复制数据到slave节点，master节点可以配置多个 salve 节点,slave节点只能有一个master,slave 节点也可以作为其他slave节点的master.<br>Redis 复制可以起到热备份,故障恢复,负载均衡等作用,复制也是哨兵和集群模式的基础.</p><h4 id="复制流程"><a href="#复制流程" class="headerlink" title="复制流程"></a>复制流程</h4><h5 id="连接建立阶段"><a href="#连接建立阶段" class="headerlink" title="连接建立阶段"></a>连接建立阶段</h5><ol><li>slave节点运行 slaveof 命令, 会首先读取配置文件中的 master 节点的信息并保存在masterhost和maserport字段中</li><li>slave 节点定时任务每秒检查是否有 master 节点可以连接，如果有，就与 master 节点建立连接并建立一个处理复制工作的文件事件处理器</li><li>slave 节点发送心跳 ping 给 master 节点,如果收到了 pong 相应,则继续复制过程,否则需要重连</li><li>如果 master 设置了 requirepass，那么 salve 节点必须发送 masterauth 的口令过去进行认证,如果认证不通过,则断开重连</li></ol><h5 id="数据同步阶段"><a href="#数据同步阶段" class="headerlink" title="数据同步阶段"></a>数据同步阶段</h5><ol><li>salve 向 master 发送 psync runid offset 命令(2.8版本以前是 sync 命令)</li><li>master 根据收到的 runid 决定是全量复制还是部分复制,如果是部分复制,则根据 offset 确定从何处开始复制</li></ol><h5 id="命令复制阶段"><a href="#命令复制阶段" class="headerlink" title="命令复制阶段"></a>命令复制阶段</h5><ol><li>master 将写操作的命令发送给 slave, 同时写入 backlog 缓存</li><li>slave 接收到写操作命令并执行,复制完成.</li></ol><h4 id="全量复制"><a href="#全量复制" class="headerlink" title="全量复制"></a>全量复制</h4><ol><li>初次复制或者由于主机重启等原因,进行全量复制</li><li>master 接到全量复制的命令后,执行bgsave, 在后台生成一份 rdb 文件,并开启一个缓冲区记录新的写操作</li><li>master 将 rdb 文件发送给 salve, slave 清空旧数据并载入新数据</li><li>master 将缓冲区中记录的新的写命令发送给 slave 并执行</li></ol><h4 id="部分复制"><a href="#部分复制" class="headerlink" title="部分复制"></a>部分复制</h4><p>在正常复制过程中，假设 master和slave 网络连接断掉，当 salve 重连 master 时，会触发部分复制.</p><p>从redis 2.8开始,支持部分复制,master和slave会维护一个offset,master 在复制过程中会同时把数据缓存到backlog中,包括offset,当连接中断重连后,master 根据 slave 发送的 psync 中的 offset 找到上次中断的位置并从这里开始复制</p><h4 id="redis复制中的一些核心概念"><a href="#redis复制中的一些核心概念" class="headerlink" title="redis复制中的一些核心概念"></a>redis复制中的一些核心概念</h4><ul><li>复制过程中,master和salve会维护一个offset, slave 每秒都会向 master 报告自己offset,offset 起到维护数据一致的作用</li><li>master 在同步数据给 salve 的时候,会同时将数据写在本地的 backlog 中,backlog 用于中断重连后的增量复制</li><li>master 节点维护了一个 runid, salve 根据 runid 判断是否进行全量复制,redis重启后runid 会发生变化,如果不希望变,可以使用 <code>redis-cli debug reload</code> 重启</li><li>slave 节点运行 psync runid offset 向 master 请求复制, 如果runid为-1或与 master 的runid不同,则会出发全量复制</li><li>master 和 slave 节点会互相发送心跳检测,master每隔10s发一次,slave每秒发一次</li></ul><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><h4 id="配置项说明"><a href="#配置项说明" class="headerlink" title="配置项说明"></a>配置项说明</h4><h5 id="从节点配置"><a href="#从节点配置" class="headerlink" title="从节点配置"></a>从节点配置</h5><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>slaveof <masterip> <masterport></td><td>在redis启动时指定master建立主从关系，可以在配置文件中配置，也可以启动后在命令行中配置</td></tr><tr><td>repl-timeout 60</td><td>主从节点连接超时阈值</td></tr><tr><td>slave-read-only yes</td><td>从节点是否设置为只读</td></tr><tr><td>slave-serve-stale-data yes</td><td>当从节点与主节点失去联系的时候，是否还能处理请求。默认<code>yes</code>表示可以，如果对数据一致性要求比较高，应该设置为<code>no</code>，这样从节点就只能响应 info, slaveof 等少数命令</td></tr></tbody></table><h5 id="主节点配置"><a href="#主节点配置" class="headerlink" title="主节点配置"></a>主节点配置</h5><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>repl-diskless-sync no</td><td>无硬盘复制，表示在全量复制阶段，数据只写入内存，不写入文件</td></tr><tr><td>repl-diskless-sync-delay 5</td><td>开启无硬盘复制后，这个参数才生效， 表示开始复制前等待的时间，这个等待是为了让所有salve都完成连接，因为复制开始后，新的slave连接要等到前面所有的slave传输完成后才能开始复制。</td></tr><tr><td>client-output-buffer-limit slave 256MB 64MB 60</td><td>配置全量复制时主节点的缓冲区大小</td></tr><tr><td>repl-disable-tcp-nodelay no</td><td>设置为<code>yes</code>时，TCP会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差</td></tr><tr><td>masterauth <master-password></td><td>从节点身份验证</td></tr><tr><td>repl-ping-slave-period 10</td><td>ping命令间隔</td></tr><tr><td>repl-backlog-size 1mb</td><td>复制积压缓冲区，命令复制给slave的同时会存到backlog中，为了避免因为网络中断引起的全量复制</td></tr><tr><td>repl-backlog-ttl 3600</td><td>当主节点没有从节点时，复制积压缓冲区保留的时间</td></tr><tr><td>min-slaves-to-write 3</td><td>最少的处于连接状态的从节点数目</td></tr><tr><td>min-slaves-max-lag</td><td>所有从节点延迟超时的阈值，与 <code>min-slaves-to-write 3</code> 同时使用，可以避免数据丢失</td></tr></tbody></table><h4 id="完整配置过程"><a href="#完整配置过程" class="headerlink" title="完整配置过程"></a>完整配置过程</h4><h5 id="准备三个节点"><a href="#准备三个节点" class="headerlink" title="准备三个节点"></a>准备三个节点</h5><table><thead><tr><th>ip</th><th>角色</th></tr></thead><tbody><tr><td>172.17.0.2</td><td>master</td></tr><tr><td>172.17.0.3</td><td>slave</td></tr><tr><td>172.17.0.4</td><td>slave</td></tr></tbody></table><h5 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h5><p><strong>修改master配置文件：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bind 0.0.0.0  # 允许任意ip连接</span><br><span class="line">protected-mode no  # 关闭安全模式</span><br></pre></td></tr></table></figure><p><strong>修改两个slave配置文件：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">salveof 172.17.0.2 6379</span><br></pre></td></tr></table></figure><h5 id="启动三个Redis服务器观察效果"><a href="#启动三个Redis服务器观察效果" class="headerlink" title="启动三个Redis服务器观察效果"></a>启动三个Redis服务器观察效果</h5><p><img src="/images/redis/replication-1.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 持久化介绍(RDB 和 AOF)</title>
      <link href="/2017-11-07-redis-persistence.html"/>
      <url>/2017-11-07-redis-persistence.html</url>
      
        <content type="html"><![CDATA[<h3 id="RDB-方式简介"><a href="#RDB-方式简介" class="headerlink" title="RDB 方式简介"></a>RDB 方式简介</h3><p>　　RDB方式的持久化是通过快照（snapshotting）完成的,当符合一定条件时 Redis 会自动将内存中的所有数据生成一份副本并存储在硬盘上，这个过程即为“快照”。redis会在以下几种情况下对数据进行快照：</p><a id="more"></a><ul><li>根据配置规则进行自动快照</li><li>用户执行 save 或 bgsave 命令</li><li>执行 flushall 命令</li><li>执行复制 （replication） 时</li></ul><h4 id="根据配置规则进行自动快照"><a href="#根据配置规则进行自动快照" class="headerlink" title="根据配置规则进行自动快照"></a>根据配置规则进行自动快照</h4><p>redis 允许用户在配置文件中自定义快照条件，预置条件及含义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save 900 1          # 表示 900 秒内有一个或一个以上的键被更改，则进行快照。</span><br><span class="line">save 300 10         # 表示 300 秒内有 10 个或 10 个以上键被更改则进行快照。</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure><h4 id="用户执行-save-或-bgsave-命令"><a href="#用户执行-save-或-bgsave-命令" class="headerlink" title="用户执行 save 或 bgsave 命令"></a>用户执行 save 或 bgsave 命令</h4><p>除了自动快照外，当进行服务重启、手动迁移以及备份时，也会需要手动执行快照操作。redis 提供了两个命令来完成手动快照操作。 </p><p>(1) save 命令<br>　　当执行 save 命令时，redis 同步的进行快照操作，在快照执行的过程中会阻塞所有客户端请求。当数据较多时，会导致redis较长时间不能响应，要避免在生产环境使用 save 命令。  </p><p>(2) bgsave 命令<br>　　bgsave 命令可以在后台异步的进行快照操作，快照的同时服务器还可以仅需响应来自客户端的请求。如果想知道快照是否完成，可以使用 lastsave 命令获取最近一次成功执行快照的时间。（执行自动快照时，redis 采用的策略是异步快照）  </p><h4 id="执行-flushall-命令"><a href="#执行-flushall-命令" class="headerlink" title="执行 flushall 命令"></a>执行 flushall 命令</h4><p>　　当 redis 配置文件中有定义快照条件时，执行 flushall 命令时，才会进行快照。  </p><h4 id="执行复制时"><a href="#执行复制时" class="headerlink" title="执行复制时"></a>执行复制时</h4><p>　　当设置了主从模式时，redis 会在复制初始化时进行自动快照，即使没有定义自动快照条件，也没有手动执行快照操作。</p><h4 id="快照原理"><a href="#快照原理" class="headerlink" title="快照原理"></a>快照原理</h4><h5 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h5><ul><li>Redis 使用 fork 函数复制一份当前进程（父进程）的副本（子进程）；  </li><li>父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；</li><li>当子进程写入完所有数据后会用该临时文件替换掉旧的 RDB 文件，至此一次快照操作完成。  </li></ul><h5 id="从操作系统的角度看-redis-快照"><a href="#从操作系统的角度看-redis-快照" class="headerlink" title="从操作系统的角度看 redis 快照"></a>从操作系统的角度看 redis 快照</h5><p>　　在执行 fork 的时候操作系统（类 Unix 操作系统）会使用写时复制（copy-on-write）策略，即 fork 函数发生的一刻父子进程共享一内存数据，当父进程要更改其中某片数据时（如执行一个写命令），操作系统会将该片数据复制一份以保证子进程的数据不受影响，所以新的 RDB 文件存储的是执行 fork 一刻的内存数据。<br>　　写时复制策略也保证了在 fork 的时刻虽然看上去生成了两份内存副本，但实际上内存的占用量并不会增加一倍。这就意味着当系统内存只有 2GB，而 Redis 数据库的内存只有 1.5GB 时，执行 fork 后内存使用量并不会增加到 3GB （超出物理内存）。为此需要确保 Linux 系统允许应用程序申请超过可用内存（物理内存和交换分区）的空间，方法是在 /etc/sysctl.conf 文件加入 vm.overcommit_memory = 1,然后重启系统或者执行 sysctl_vm.overcommit_memory=1确保设置生效。<br>　　另外需要注意的是，当进行快照的过程中，如果写入操作较多，造成 fork 前后数据差异较大，是会使得内存使用量显著超过实际数据大小的，因为内存中不仅保存了当前的redis 数据库的数据，而且还保存着 fork 时刻的内存数据。进行内存用量估算时很容易忽略这一问题，造成内存用量超限。  </p><h4 id="RDB-小结"><a href="#RDB-小结" class="headerlink" title="RDB 小结"></a>RDB 小结</h4><p>　　Redis 在进行快照的过程中不会修改 RDB 文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候 RDB 文件都是完整的。这使得我们可以通过定时备份 RDB 文件来实现 Redis 数据库备份。RDB 文件是经过压缩（可以配置 rdbcompression）参数以禁用压缩节省 CPU 占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。<br>　　通过 RDB 方式实现持久化，一旦 Redis 异常退出，就会丢失最后一次快照以后更改的所有数据。为了应对这种情况，可以通过组合设置快照条件降低数据损失的数量，也可以使用 AOF 方式进行持久化。</p><h3 id="AOF-方式简介"><a href="#AOF-方式简介" class="headerlink" title="AOF 方式简介"></a>AOF 方式简介</h3><p>　　AOF 可以将 Redis 执行的每一条写命令追加到硬盘文件中，这种持久化方式会一定程度降低 Redis 性能。</p><h4 id="开启-AOF"><a href="#开启-AOF" class="headerlink" title="开启 AOF"></a>开启 AOF</h4><p>　　默认情况下 redis 没有开启 AOF(append only file) 方式的持久化，可以通过 appendonly 参数启用：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">appendonly yes</span><br></pre></td></tr></table></figure><p>　　开启 AOF 持久化后每执行一条会非读的操作命令，都会被 redis 写入到硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，默认文件名为 appendonly.aof  　　</p><h4 id="AOF-文件优化"><a href="#AOF-文件优化" class="headerlink" title="AOF 文件优化"></a>AOF 文件优化</h4><p>　　AOF 文件的内容是 Redis 客户端向 Redis 发送的原始通信协议的内容，所以如果对同一个数据进行多次修改，就是在 AOF 文件中记录多次，但我们想要的仅是最后一次修改后的数据，所以最好可以将无用的记录删除掉。redis 支持用户设置这样的条件，当达到条件的时候，就会自动重写 AOF 文件，这个条件可以在配置文件中设置：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line"># 表示当目前 AOF 文件大小超过上一次重写时的 AOF 文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时的 AOF 文件大小为依据。</span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"># 表示允许进行重写的 AOF 文件的最小大小，在这个范围内的 AOF 文件一直不会被重写</span><br></pre></td></tr></table></figure><p>　　还可以使用 bgrewriteaof 命令手动执行 AOF 文件重写。重写过程只与内存中的数据有关，aof文件中的已有内容无关。 </p><p><strong>AOF rewrite 过程:</strong></p><ul><li>redis fork一个子进程</li><li>子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志</li><li>redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件</li><li>子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中</li><li>用新的日志文件替换掉旧的日志文件　　<h4 id="同步硬盘数据"><a href="#同步硬盘数据" class="headerlink" title="同步硬盘数据"></a>同步硬盘数据</h4>　　虽然每次执行更改数据的操作 redis 都会将命令记录在 AOF 文件中，但是事实上，<strong>由于操作系统的缓存机制，数据并没有真正的写入硬盘，而是进入了系统的硬盘缓存。</strong> 在默认情况下系统每 30 秒会执行一次同步操作，以便将硬盘缓存中的内容真正的写入硬盘，在这 30 秒的过程中如果系统异常退出则会导致硬盘缓存中的数据丢失。可以通过配置 appendfsync 参数设置同步：  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># appendfsync always</span><br><span class="line">appendfsync everysync </span><br><span class="line"># appendffync no</span><br></pre></td></tr></table></figure><ul><li>everysec 每秒执行一次同步操作（默认）</li><li>always 每次执行修改数据的操作都会执行同步（最安全最慢的方式）</li><li>no 表示不主动同步，即由操作系统来做（30秒一次,不可控） </li></ul><h4 id="AOF-破损文件修复"><a href="#AOF-破损文件修复" class="headerlink" title="AOF 破损文件修复"></a>AOF 破损文件修复</h4><p>如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损,用 <code>redis-check-aof --fix</code>命令可以修复破损的AOF文件</p><h3 id="RDB-和-AOF-的优缺点"><a href="#RDB-和-AOF-的优缺点" class="headerlink" title="RDB 和 AOF 的优缺点"></a>RDB 和 AOF 的优缺点</h3><h4 id="RDB-的优缺点"><a href="#RDB-的优缺点" class="headerlink" title="RDB 的优缺点"></a>RDB 的优缺点</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul><li>RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备</li><li>RDB对redis对外提供的读写服务影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可</li><li>相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速</li></ul><h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul><li>redis故障时，会丢失数据.一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据</li><li>RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒</li></ul><h4 id="AOF-的优缺点"><a href="#AOF-的优缺点" class="headerlink" title="AOF 的优缺点"></a>AOF 的优缺点</h4><h5 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h5><ul><li><p>AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据</p></li><li><p>AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复</p></li><li><p>AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。</p></li><li><p>AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据</p><h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5></li><li><p>对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大</p></li><li><p>AOF开启后，支持的写QPS会比RDB支持的写QPS低</p></li><li><p>数据恢复的时候，会比较慢，还有做冷备，定期的备份，不太方便，可能要自己手写复杂的脚本去做，做冷备不太合适</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>　　配置合理的 RDB 持久化方式最快，适用于对数据丢失在一定范围内可以容忍的缓存应用。AOF 持久化方式更安全，但可以较高程度的保证系统异常不会对数据造成破坏，但需要付出性能的代价。redis 也支持同时使用 RDB 和 AOF 的方式持久化，在重启后恢复数据的时候，会选择 AOF 文件进行恢复。  </p></li></ul><h3 id="冷备策略"><a href="#冷备策略" class="headerlink" title="冷备策略"></a>冷备策略</h3><h4 id="每小时做一次备份"><a href="#每小时做一次备份" class="headerlink" title="每小时做一次备份"></a>每小时做一次备份</h4><p>编写脚本 <code>redis_rdb_copy_hourly.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh </span><br><span class="line"></span><br><span class="line">cur_date&#x3D;&#96;date +%Y%m%d%k&#96;</span><br><span class="line">rm -rf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line">cp &#x2F;var&#x2F;redis&#x2F;6379&#x2F;dump.rdb &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line"></span><br><span class="line">del_date&#x3D;&#96;date -d -48hour +%Y%m%d%k&#96;</span><br><span class="line">rm -rf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$del_date</span><br></pre></td></tr></table></figure><p>添加脚本到定时任务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">0 * * * * sh &#x2F;usr&#x2F;local&#x2F;redis&#x2F;copy&#x2F;redis_rdb_copy_hourly.sh</span><br></pre></td></tr></table></figure><h4 id="每天做一次备份"><a href="#每天做一次备份" class="headerlink" title="每天做一次备份"></a>每天做一次备份</h4><p>编写脚本 <code>redis_rdb_copy_daily.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh </span><br><span class="line"></span><br><span class="line">cur_date&#x3D;&#96;date +%Y%m%d&#96;</span><br><span class="line">rm -rf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line">cp &#x2F;var&#x2F;redis&#x2F;6379&#x2F;dump.rdb &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$cur_date</span><br><span class="line"></span><br><span class="line">del_date&#x3D;&#96;date -d -1month +%Y%m%d&#96;</span><br><span class="line">rm -rf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;snapshotting&#x2F;$del_date</span><br></pre></td></tr></table></figure><p>添加脚本到定时任务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">0 0 * * * sh &#x2F;usr&#x2F;local&#x2F;redis&#x2F;copy&#x2F;redis_rdb_copy_daily.sh</span><br></pre></td></tr></table></figure><h3 id="数据恢复方案"><a href="#数据恢复方案" class="headerlink" title="数据恢复方案"></a>数据恢复方案</h3><ul><li>如果是 redis 进程挂掉，那么重启 redis 进程即可，直接基于AOF日志文件恢复数据, 最多就丢一秒的数</li><li>如果是 redis 进程所在机器挂掉，那么重启机器后，尝试重启 redis 进程，尝试直接基于AOF日志文件进行数据恢复,如果AOF文件破损，那么用 <code>redis-check-aof fix</code></li><li>如果 redis 当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复</li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
            <tag> Nosql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 ansible 部署 hive （远程 mysql 存储 metadata）</title>
      <link href="/2017-10-23-install-hive.html"/>
      <url>/2017-10-23-install-hive.html</url>
      
        <content type="html"><![CDATA[<p>使用 ansible 快速部署 hadoop 集群，服务器节点须有 jdk ,sshd, hadoop, 关于 hadoop 的部署可以参考<a href="http://www.waterandair.top/install-hadoop.html" target="_blank" rel="noopener">使用 ansible 部署 hadoop 集群</a></p><a id="more"></a><h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><p>节点：</p><ul><li>172.17.0.2</li><li>172.17.0.3</li><li>172.17.0.4</li></ul><p>选择 172.17.0.2 作为 namenode 和 resourcemanager, 在 172.17.0.2 节点安装hive</p><p><code>/etc/hosts</code> :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">172.17.0.2 namenode.com resourcemanager.com</span><br><span class="line">172.17.0.3 secondarynode.com worker-01.com</span><br><span class="line">172.17.0.4 worker-02.com</span><br></pre></td></tr></table></figure><p>数据库地址： 192.168.1.102:3306  （用于存储hive元数据）</p><p>hadoop 版本: 3.1.0<br>hive 版本:2.3.3<br>jdk 版本: 8<br>mysql-connector 版本: 5.1.46</p><h3 id="初始化-role"><a href="#初始化-role" class="headerlink" title="初始化 role"></a>初始化 role</h3><p>在任意目录(这里以test文件夹为例)下执行 ansible-galaxy init hive ,初始化一个 roles</p><h3 id="设置要用到的变量"><a href="#设置要用到的变量" class="headerlink" title="设置要用到的变量"></a>设置要用到的变量</h3><p>test/hive/defaults/main.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># defaults file for hive</span><br><span class="line"># 版本</span><br><span class="line">hive_version: 2.3.3</span><br><span class="line"># 下载地址</span><br><span class="line">hive_download_url: &quot;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;hive&#x2F;stable-2&#x2F;apache-hive-&#123;&#123; hive_version &#125;&#125;-bin.tar.gz&quot;</span><br><span class="line"># 下载到本地的 ~&#x2F;Downloads</span><br><span class="line">hive_download_dest: ~&#x2F;Downloads&#x2F;apache-hive-&#123;&#123; hive_version &#125;&#125;-bin.tar.gz</span><br><span class="line"># 下载 mysql-connector 地址</span><br><span class="line">mysql_connector_name: mysql-connector-java-5.1.46</span><br><span class="line">mysql_connector_url: &quot;https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;Downloads&#x2F;Connector-J&#x2F;&#123;&#123; mysql_connector_name &#125;&#125;.tar.gz&quot;</span><br><span class="line">mysql_connector_download_dest: ~&#x2F;Downloads&#x2F;&#123;&#123; mysql_connector_name &#125;&#125;.tar.gz</span><br><span class="line"># 安装目录</span><br><span class="line">install_dir: &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"># 软连接地址</span><br><span class="line">soft_dir: &quot;&#123;&#123; install_dir &#125;&#125;hive&quot;</span><br><span class="line"># 设置环境变量的文件</span><br><span class="line">env_file: &#x2F;etc&#x2F;profile</span><br><span class="line"># 部分操作要在 hosts 中设置 hive 为 yes 时才能执行</span><br><span class="line">hive: no</span><br><span class="line"># hive 配置目录</span><br><span class="line">hive_conf_dir: &quot;&#123;&#123; soft_dir &#125;&#125;&#x2F;conf&#x2F;&quot;</span><br><span class="line">hive_lib_dir: &quot;&#123;&#123; install_dir &#125;&#125;apache-hive-&#123;&#123; hive_version &#125;&#125;-bin&#x2F;lib&#x2F;&quot;</span><br><span class="line"></span><br><span class="line"># 配置相关</span><br><span class="line"># mysql 元数据管理 注意字符编码必须是 latin1</span><br><span class="line">mysql_metadata_url: jdbc:mysql:&#x2F;&#x2F;192.168.1.102:3306&#x2F;docker_hive_metadata?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;latin1</span><br><span class="line"># mysql 用户</span><br><span class="line">mysql_user: root</span><br><span class="line"># mysql 密码</span><br><span class="line">mysql_password: &#39;000000&#39;</span><br><span class="line">hive_metastore_uris: thrift:&#x2F;&#x2F;172.17.0.2:9083</span><br></pre></td></tr></table></figure><h3 id="准备-hive-配置文件"><a href="#准备-hive-配置文件" class="headerlink" title="准备 hive 配置文件"></a>准备 hive 配置文件</h3><p>test/hive/templates/</p><ul><li>hive-site.xml.j2 (主要包括 metadata 的 mysql地址， hiveserver 地址)</li><li>hive-log4j2.properties.j2 (日志地址， 日志级别)</li><li>llap-cli-log4j2.properties.j2 </li></ul><p><a href="https://github.com/waterandair/ansible-playbook-devops/tree/master/hive/templates" target="_blank" rel="noopener">这部分配置可以点击这里参考</a></p><h3 id="编写-tasks"><a href="#编写-tasks" class="headerlink" title="编写 tasks"></a>编写 tasks</h3><p>如果一个 tasks 有很多步骤,可以把它们分置在不同的文件中,最后在 test/hadoop/tasks/main.yml 文件中引用他们,这样做方便调试,方便阅读</p><h4 id="编写-hive-mysql-connector-的下载安装过程"><a href="#编写-hive-mysql-connector-的下载安装过程" class="headerlink" title="编写 hive,mysql-connector 的下载安装过程"></a>编写 hive,mysql-connector 的下载安装过程</h4><p>在 test/hive/tasks/ 下新建文件 download.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># 把文件下载到本地</span><br><span class="line">- name: 判断 hive 源文件是否已经下载好了</span><br><span class="line">  command: ls &#123;&#123; hive_download_dest &#125;&#125;</span><br><span class="line">  ignore_errors: true</span><br><span class="line">  register: file_exist</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 下载文件</span><br><span class="line">  get_url:</span><br><span class="line">    url: &quot;&#123;&#123; hive_download_url&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; hive_download_dest &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line">  when: file_exist|failed</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line"># 解压到远程服务器的安装目录</span><br><span class="line">- name: 解压</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123; hive_download_dest &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- name: 判断 mysql connecrtor 源文件是否已经下载好了</span><br><span class="line">  command: ls &#123;&#123; mysql_connector_download_dest &#125;&#125;</span><br><span class="line">  ignore_errors: true</span><br><span class="line">  register: file_exist</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 下载文件</span><br><span class="line">  get_url:</span><br><span class="line">    url: &quot;&#123;&#123; mysql_connector_url&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; mysql_connector_download_dest &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line">  when: file_exist|failed</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line"># 解压到远程服务器的安装目录</span><br><span class="line">- name: 解压 mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123; mysql_connector_download_dest &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br><span class="line"></span><br><span class="line">- name: 移动　mysql-connector-java-5.1.46-bin.jar 到 hive 下的 lib 目录中</span><br><span class="line">  shell: &quot;mv &#123;&#123; install_dir + mysql_connector_name + &#39;&#x2F;&#39; + mysql_connector_name + &#39;-bin.jar&#39; &#125;&#125; &#123;&#123; hive_lib_dir &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br><span class="line"></span><br><span class="line">- name: 删除 mysql-connector-java-5.1.46 文件夹</span><br><span class="line">  shell: &quot;rm -rf &#123;&#123; install_dir + mysql_connector_name &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br></pre></td></tr></table></figure><h4 id="编写-hive-配置、启动过程"><a href="#编写-hive-配置、启动过程" class="headerlink" title="编写 hive 配置、启动过程"></a>编写 hive 配置、启动过程</h4><p>在 test/hive/tasks/ 下新建文件 init.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- name: 创建软连接</span><br><span class="line">  file:</span><br><span class="line">    src: &quot;&#123;&#123; install_dir + &#39;apache-hive-&#39; + hive_version + &#39;-bin&#39;&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; soft_dir &#125;&#125;&quot;</span><br><span class="line">    state: link</span><br><span class="line"></span><br><span class="line">- name: 设置 hive 环境变量</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123;env_file&#125;&#125;&quot;</span><br><span class="line">    insertafter: &quot;&#123;&#123;item.position&#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123;item.value&#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">  - &#123;position: EOF, value: &quot;\n&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;# hive environment&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HIVE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hive&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export PATH&#x3D;$PATH:$&#123;HIVE_HOME&#125;&#x2F;bin&quot;&#125;</span><br><span class="line"></span><br><span class="line"># 创建 &#x2F;tmp 和 &#x2F;user&#x2F;hive&#x2F;warehouse (必须提前启动好 hadoop)</span><br><span class="line">- name: 创建 &#x2F;tmp</span><br><span class="line">  shell: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop fs -mkdir &#x2F;tmp</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line"></span><br><span class="line">- name: 创建 &#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line">  shell: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line"></span><br><span class="line">- name: 为 &#x2F;tmp 添加权限</span><br><span class="line">  shell: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop fs -chmod g+w &#x2F;tmp</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line"></span><br><span class="line">- name: 为 &#x2F;user&#x2F;hive&#x2F;warehouse 添加权限</span><br><span class="line">  shell: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line"></span><br><span class="line">- name: 配置 hive-site.xml, 主要包括 metadata 的 mysql地址， hiveserver 地址， Thrift 地址</span><br><span class="line">  template:</span><br><span class="line">    src: hive-site.xml.j2</span><br><span class="line">    dest: &quot;&#123;&#123; hive_conf_dir &#125;&#125;hive-site.xml&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 llap-cli-log4j2.properties</span><br><span class="line">  template:</span><br><span class="line">    src: llap-cli-log4j2.properties.j2</span><br><span class="line">    dest: &quot;&#123;&#123; hive_conf_dir &#125;&#125;llap-cli-log4j2.properties&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 hive-log4j2.properties, 日志地址， 日志级别</span><br><span class="line">  template:</span><br><span class="line">    src: hive-log4j2.properties.j2</span><br><span class="line">    dest: &quot;&#123;&#123; hive_conf_dir &#125;&#125;hive-log4j2.properties&quot;</span><br><span class="line"></span><br><span class="line">- name: 初始化 metadata</span><br><span class="line">  shell: &quot;source &#x2F;etc&#x2F;profile &amp;&amp; schematool -dbType mysql -initSchema&quot;</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">- name: 启动metaStore服务</span><br><span class="line">  shell: &quot;source &#123;&#123; env_file &#125;&#125; &amp;&amp; nohup &#x2F;usr&#x2F;local&#x2F;hive&#x2F;bin&#x2F;hive --service metastore 1&gt;2&amp;&quot;</span><br><span class="line">  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">#- name: 启动thrift2服务</span><br><span class="line">#  shell: &quot;source &#123;&#123; env_file &#125;&#125; &amp;&amp; &#x2F;usr&#x2F;local&#x2F;hive&#x2F;bin&#x2F;hive --service hiveserver2 &amp;&quot;</span><br><span class="line">#  when: hive &#x3D;&#x3D; &#39;yes&#39;</span><br><span class="line">#  args:</span><br><span class="line">#    executable: &#x2F;bin&#x2F;bash</span><br><span class="line">#  become: true</span><br></pre></td></tr></table></figure><h4 id="合并所有-tasks"><a href="#合并所有-tasks" class="headerlink" title="合并所有 tasks"></a>合并所有 tasks</h4><p>在 test/hive/tasks/ 下修改文件 main.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for hive</span><br><span class="line">- import_tasks: download.yml</span><br><span class="line">- import_tasks: init.yml</span><br></pre></td></tr></table></figure><h3 id="编写-hosts-文件"><a href="#编写-hosts-文件" class="headerlink" title="编写 hosts 文件"></a>编写 hosts 文件</h3><p>在 test/ 目录下新建文件 hosts</p><p>变量 namenode=yes， resourcemanager=yes 分别表示启动 namenode 和 resourcemanager 的节点，即主节， 变量 hive=yes 表示 hive server主节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[test]</span><br><span class="line">172.17.0.2 namenode&#x3D;yes resourcemanager&#x3D;yes hive&#x3D;yes</span><br><span class="line">172.17.0.3 </span><br><span class="line">172.17.0.4</span><br></pre></td></tr></table></figure><h3 id="编写启动文件"><a href="#编写启动文件" class="headerlink" title="编写启动文件"></a>编写启动文件</h3><p>在 test/ 目录下新建文件 run.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  remote_user: root</span><br><span class="line">  roles:</span><br><span class="line">    - hive</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>在 test/ 目录下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ansible 是通过 ssh 执行命令的,所以最佳实践中推荐的是设置 ssh 免密码登录</span><br><span class="line"># 如果没有设置,而已在执行命令后加 -k 参数,手动输入密码, 这里 root 用户的密码是 root</span><br><span class="line">ansible-playbook -i hosts run.yml -k</span><br></pre></td></tr></table></figure><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a><a href="https://github.com/waterandair/ansible-playbook-devops/tree/master/hive" target="_blank" rel="noopener">项目地址</a></h3><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><h4 id="字符集的问题"><a href="#字符集的问题" class="headerlink" title="字符集的问题"></a>字符集的问题</h4><h5 id="ERROR-信息"><a href="#ERROR-信息" class="headerlink" title="ERROR 信息"></a>ERROR 信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">FAILED: Error in metadata: MetaException(message:Got exception: javax.jdo.JDODataStoreException An exception was thrown while adding&#x2F;validating class(es) : Specified key was too long; max key length is 767 bytes</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br></pre></td></tr></table></figure><h5 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h5><p>这个问题是由于 mysql 数据库数据表的字符集引起的， 应该把字符集改为 latin1  </p>]]></content>
      
      
      <categories>
          
          <category> 环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> 环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 ansible 部署 hadoop 集群</title>
      <link href="/2017-10-16-install-hadoop.html"/>
      <url>/2017-10-16-install-hadoop.html</url>
      
        <content type="html"><![CDATA[<p>使用 ansible 快速部署 hadoop 集群，服务器节点须有 jdk 和 sshd 服务</p><a id="more"></a><h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><p>节点：</p><ul><li>172.17.0.2</li><li>172.17.0.3</li><li>172.17.0.4</li></ul><p>选择 172.17.0.2 作为 namenode 和 resourcemanager<br>修改 172.17.0.2 节点的 <code>/etc/hosts</code> :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">172.17.0.2 namenode.com resourcemanager.com</span><br><span class="line">172.17.0.3 secondarynode.com worker-01.com</span><br><span class="line">172.17.0.4 worker-02.com</span><br></pre></td></tr></table></figure><p>配置主节点 172.17.0.2 到每个节点的 ssh 免密登录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; ssh-copy-id 172.17.0.2</span><br><span class="line">shell&gt; ssh-copy-id 172.17.0.3</span><br><span class="line">shell&gt; ssh-copy-id 172.17.0.4</span><br></pre></td></tr></table></figure><h3 id="初始化-role"><a href="#初始化-role" class="headerlink" title="初始化 role"></a>初始化 role</h3><p>在任意目录(这里以test文件夹为例)下执行 <code>ansible-galaxy init hadoop</code> ,初始化一个 roles</p><h3 id="设置要用到的变量"><a href="#设置要用到的变量" class="headerlink" title="设置要用到的变量"></a>设置要用到的变量</h3><p>test/hadoop/defaults/main.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># defaults file for hadoop</span><br><span class="line"># 版本</span><br><span class="line">hadoop_version: 3.1.0</span><br><span class="line"># 下载地址</span><br><span class="line">hadoop_download_url: &quot;http:&#x2F;&#x2F;mirrors.shu.edu.cn&#x2F;apache&#x2F;hadoop&#x2F;common&#x2F;hadoop-&#123;&#123; hadoop_version &#125;&#125;&#x2F;hadoop-&#123;&#123; hadoop_version &#125;&#125;.tar.gz&quot;</span><br><span class="line"># 下载到本地的 ~&#x2F;Downloads</span><br><span class="line">hadoop_download_dest: ~&#x2F;Downloads&#x2F;hadoop-&#123;&#123; hadoop_version &#125;&#125;.tar.gz</span><br><span class="line"># 安装目录</span><br><span class="line">install_dir: &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"># 软连接地址</span><br><span class="line">soft_dir: &quot;&#123;&#123; install_dir &#125;&#125;hadoop&quot;</span><br><span class="line"># 设置环境变量的文件</span><br><span class="line">env_file: &#x2F;etc&#x2F;profile</span><br><span class="line"># hadoop 配置文件地址</span><br><span class="line">hadoop_config_dir: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop</span><br><span class="line"># hdfs 副本数量</span><br><span class="line">dfs_replication: 1</span><br><span class="line"># hdfs 的 namenode 地址</span><br><span class="line">namenode_host: namenode.com:9000</span><br><span class="line"># secondary_node 地址</span><br><span class="line">secondarynode_host: secondarynode.com:9001</span><br><span class="line"># resourcemanager 地址</span><br><span class="line">resourcemanager_host: resourcemanager.com</span><br><span class="line"># jobhistory 地址</span><br><span class="line">jobhistory_web: resourcemanager.com:19888</span><br><span class="line">jobhistory_address: resourcemanager.com:10020</span><br><span class="line"># worker 地址</span><br><span class="line">worker_hosts:</span><br><span class="line"> - worker-01.com</span><br><span class="line"> - worker-02.com</span><br><span class="line"></span><br><span class="line">JAVA_HOME: &#x2F;usr&#x2F;local&#x2F;java</span><br><span class="line">HADOOP_HOME: &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line"></span><br><span class="line"># 默认把所有节点的 namenode 和 resourcemanager 设置为no， 在 hosts 文件中把 namenode, resourcemanager 节点的该值设置为 yes</span><br><span class="line">namenode: no</span><br><span class="line">resourcemanager: no</span><br></pre></td></tr></table></figure><h3 id="准备-hadoop-的配置文件"><a href="#准备-hadoop-的配置文件" class="headerlink" title="准备 hadoop 的配置文件"></a>准备 hadoop 的配置文件</h3><p>test/hadoop/templates/</p><ul><li>core-site.xml.j2  （指定hdfs的namenode的host地址, 指定hadoop临时目录）</li><li>hadoop-env.sh.j2 （hadoop 启动环境， 通常在这里指定一些环境变量）</li><li>hdfs-site.xml.j2 （指定HDFS副本的数量, 指定 secondarynod 的地址）</li><li>mapred-site.xml.j2 （指定mr运行在yarn上, 指定 resourcemanager 的 host 地址, 指定 jobhistory 的 host 地址）</li><li>workers.j2  （ 配置 workers 节点， 过去版本名为 slaves）</li><li>yarn-site.xml.j2 （指定 ResourceManager 的地址， 指定 reducer获取数据的方式）</li></ul><p><a href="https://github.com/waterandair/ansible-playbook-devops/tree/master/hadoop/templates" target="_blank" rel="noopener">这部分配置可以点击这里参考</a></p><h3 id="编写-tasks"><a href="#编写-tasks" class="headerlink" title="编写 tasks"></a>编写 tasks</h3><p>如果一个 tasks 有很多步骤,可以把它们分置在不同的文件中,最后在 test/hadoop/tasks/main.yml 文件中引用他们,这样做方便调试,方便阅读</p><h4 id="编写-hadoop-下载、安装过程"><a href="#编写-hadoop-下载、安装过程" class="headerlink" title="编写 hadoop 下载、安装过程"></a>编写 hadoop 下载、安装过程</h4><p>在 test/hadoop/tasks/ 下新建文件 download.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for .&#x2F;hadoop</span><br><span class="line"># 把文件下载到本地</span><br><span class="line">- name: 判断 hadoop 源文件是否已经下载好了</span><br><span class="line">  command: ls &#123;&#123; hadoop_download_dest &#125;&#125;</span><br><span class="line">  ignore_errors: true</span><br><span class="line">  register: file_exist</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line">- name: 下载文件</span><br><span class="line">  get_url:</span><br><span class="line">    url: &quot;&#123;&#123; hadoop_download_url&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; hadoop_download_dest &#125;&#125;&quot;</span><br><span class="line">    force: yes</span><br><span class="line">  when: file_exist|failed</span><br><span class="line">  connection: local</span><br><span class="line"></span><br><span class="line"># 解压到远程服务器的安装目录</span><br><span class="line">- name: 解压</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123; hadoop_download_dest &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line">  become: true</span><br></pre></td></tr></table></figure><h4 id="编写-hadoop-配置、启动过程"><a href="#编写-hadoop-配置、启动过程" class="headerlink" title="编写 hadoop 配置、启动过程"></a>编写 hadoop 配置、启动过程</h4><p>在 test/hadoop/tasks/ 下新建文件 init.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for .&#x2F;hadoop</span><br><span class="line"># install hadoop</span><br><span class="line"></span><br><span class="line">- name: 安装依赖 pdsh</span><br><span class="line">  apt:</span><br><span class="line">    name: &quot;&#123;&#123;item&#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">  - pdsh</span><br><span class="line"></span><br><span class="line">- name: 创建 rcmd_default 配置文件</span><br><span class="line">  file:</span><br><span class="line">    path: &#x2F;etc&#x2F;pdsh&#x2F;rcmd_default</span><br><span class="line">    state: touch</span><br><span class="line"></span><br><span class="line">- name: 写入 rcmd_default ssh</span><br><span class="line">  shell: echo &quot;ssh&quot; &gt; &#x2F;etc&#x2F;pdsh&#x2F;rcmd_default</span><br><span class="line"></span><br><span class="line">- name: 创建 hadoop 软连接</span><br><span class="line">  file: </span><br><span class="line">    src: &#x2F;usr&#x2F;local&#x2F;hadoop-&#123;&#123; hadoop_version &#125;&#125;</span><br><span class="line">    dest: &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">    state: link</span><br><span class="line"></span><br><span class="line">- name: 创建 hadoop logs 目录</span><br><span class="line">  file:</span><br><span class="line">    path: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;logs</span><br><span class="line">    mode: 0755</span><br><span class="line">    state: directory</span><br><span class="line"></span><br><span class="line">- name: 设置 hadoop 相关环境变量</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123;env_file&#125;&#125;&quot;</span><br><span class="line">    insertafter: &quot;&#123;&#123;item.position&#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123;item.value&#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">  - &#123;position: EOF, value: &quot;\n&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;# hadoop environment&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export PATH&#x3D;$PATH:$&#123;HADOOP_HOME&#125;&#x2F;bin:$&#123;HADOOP_HOME&#125;&#x2F;sbin&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HADOOP_LOG_DIR&#x3D;$&#123;HADOOP_HOME&#125;&#x2F;logs&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export YARN_RESOURCEMANAGER_USER&#x3D;root&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export YARN_NODEMANAGER_USER&#x3D;root&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HDFS_NAMENODE_USER&#x3D;root&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HDFS_DATANODE_USER&#x3D;root&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HDFS_SECONDARYNAMENODE_USER&#x3D;root&quot;&#125;</span><br><span class="line">  - &#123;position: EOF, value: &quot;export HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&quot;&#125;</span><br><span class="line"></span><br><span class="line"># 配置 hadoop</span><br><span class="line">- name: 配置 hadoop-env.sh, 保持默认</span><br><span class="line">  template:</span><br><span class="line">    src: hadoop-env.sh.j2</span><br><span class="line">    dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;hadoop-env.sh&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 core-site.xml, 指定hdfs的namenode的host地址, 指定hadoop临时目录</span><br><span class="line">  template:</span><br><span class="line">    src: core-site.xml.j2</span><br><span class="line">    dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;core-site.xml&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 hdfs-site.xml, 指定HDFS副本的数量, 指定 secondarynod 的地址</span><br><span class="line">  template:</span><br><span class="line">    src: hdfs-site.xml.j2</span><br><span class="line">    dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;hdfs-site.xml&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 mapred-site.xml, 指定mr运行在yarn上, 指定 resourcemanager 的 host 地址, 指定 jobhistory 的 host 地址</span><br><span class="line">  template:</span><br><span class="line">    src: mapred-site.xml.j2</span><br><span class="line">    dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;mapred-site.xml&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 yarn-site.xml, 指定 ResourceManager 的地址， 指定 reducer获取数据的方式</span><br><span class="line">  template:</span><br><span class="line">      src: yarn-site.xml.j2</span><br><span class="line">      dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;yarn-site.xml&quot;</span><br><span class="line"></span><br><span class="line">- name: 配置 workers， 过去版本名为 slaves</span><br><span class="line">  template:</span><br><span class="line">    src: workers.j2</span><br><span class="line">    dest: &quot;&#123;&#123;hadoop_config_dir&#125;&#125;&#x2F;workers&quot;</span><br><span class="line"></span><br><span class="line">- name: 格式化 active namenode hdfs</span><br><span class="line">  shell: &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hdfs namenode -format</span><br><span class="line">  when: namenode&#x3D;&#x3D;&#39;yes&#39;</span><br><span class="line">  args:</span><br><span class="line">     executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">- name: 启动 hdfs</span><br><span class="line">  shell: &quot;source &#123;&#123; env_file &#125;&#125; &amp;&amp;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;sbin&#x2F;start-dfs.sh&quot;</span><br><span class="line">  when: namenode&#x3D;&#x3D;&#39;yes&#39;</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">- name: 启动 yarn</span><br><span class="line">  shell: &quot;source &#123;&#123; env_file &#125;&#125; &amp;&amp; &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;sbin&#x2F;start-yarn.sh&quot;</span><br><span class="line">  when: resourcemanager&#x3D;&#x3D;&#39;yes&#39;</span><br><span class="line">  args:</span><br><span class="line">    executable: &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">#- name: 启动 historyserver</span><br><span class="line">#  shell: &quot;source &#123;&#123; env_file &#125;&#125; &amp;&amp; &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;mapred --daemon start historyserver&quot;</span><br><span class="line">#  when: resourcemanager&#x3D;&#x3D;&#39;yes&#39;</span><br><span class="line">#  args:</span><br><span class="line">#    executable: &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h4 id="合并所有-tasks"><a href="#合并所有-tasks" class="headerlink" title="合并所有 tasks"></a>合并所有 tasks</h4><p>在 test/hadoop/tasks/ 下修改文件 main.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for hadoop</span><br><span class="line">##</span><br><span class="line">- import_tasks: download.yml</span><br><span class="line">#####</span><br><span class="line">- import_tasks: init.yml</span><br></pre></td></tr></table></figure><h3 id="编写-hosts-文件"><a href="#编写-hosts-文件" class="headerlink" title="编写 hosts 文件"></a>编写 hosts 文件</h3><p>在 test/ 目录下新建文件 hosts  </p><p>变量 namenode， resourcemanager 分别表示启动 namenode 和 resourcemanager 的节点，即主节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[test]</span><br><span class="line">172.17.0.2 namenode&#x3D;yes resourcemanager&#x3D;yes </span><br><span class="line">172.17.0.3 </span><br><span class="line">172.17.0.4</span><br></pre></td></tr></table></figure><h3 id="编写启动文件"><a href="#编写启动文件" class="headerlink" title="编写启动文件"></a>编写启动文件</h3><p>在 test/ 目录下新建文件 run.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  remote_user: root</span><br><span class="line">  roles:</span><br><span class="line">    - hadoop</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>在 test/ 目录下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ansible 是通过 ssh 执行命令的,所以最佳实践中推荐的是设置 ssh 免密码登录</span><br><span class="line"># 如果没有设置,而已在执行命令后加 -k 参数,手动输入密码, 这里 root 用户的密码是 root</span><br><span class="line">ansible-playbook -i hosts run.yml -k</span><br></pre></td></tr></table></figure><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a><a href="https://github.com/waterandair/ansible-playbook-devops/tree/master/hadoop" target="_blank" rel="noopener">项目地址</a></h3><h3 id="记录遇到的问题"><a href="#记录遇到的问题" class="headerlink" title="记录遇到的问题"></a>记录遇到的问题</h3><h4 id="Resource-Manager-不能启动"><a href="#Resource-Manager-不能启动" class="headerlink" title="Resource Manager 不能启动"></a>Resource Manager 不能启动</h4><h5 id="日志："><a href="#日志：" class="headerlink" title="日志："></a>日志：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.google.inject.ProvisionException: Unable to provision, see the following errors:</span><br><span class="line">402</span><br><span class="line">403 1) Error injecting constructor, java.lang.NoClassDefFoundError: javax&#x2F;activation&#x2F;DataSource</span><br><span class="line">404   at org.apache.hadoop.yarn.server.resourcemanager.webapp.</span><br><span class="line">.&lt;init&gt;(JAXBContextResolver.java:41)</span><br><span class="line">405   at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:50)</span><br><span class="line">406   while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver</span><br><span class="line">407</span><br><span class="line">408 1 error</span><br></pre></td></tr></table></figure><h5 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h5><p>我这里的情况是由于 java 9 版本引起的， hadoop3.1.0 不兼容 java 9 ，解决方法有两种 </p><p>一、 修改环境中 jdk 的版本为 8（推荐）<br>二、修改配置文件 <code>$HADOOP_HOME/etc/hadoop/yarn-env.sh</code>  </p><p>加入:<br><code>export YARN_RESOURCEMANAGER_OPTS=&quot;--add-modules=ALL-SYSTEM&quot;</code><br><code>export YARN_NODEMANAGER_OPTS=&quot;--add-modules=ALL-SYSTEM&quot;</code> </p><h3 id="Hadoop-启动停止方式"><a href="#Hadoop-启动停止方式" class="headerlink" title="Hadoop 启动停止方式"></a>Hadoop 启动停止方式</h3><h4 id="各个服务组件逐一启动"><a href="#各个服务组件逐一启动" class="headerlink" title="各个服务组件逐一启动"></a>各个服务组件逐一启动</h4><ul><li>分别启动hdfs组件:<br><code>hadoop-daemon.sh  start|stop  namenode|datanode|secondarynamenode</code></li><li>启动yarn<br><code>yarn-daemon.sh  start|stop  resourcemanager|nodemanager</code><h4 id="各个模块分开启动（配置ssh是前提）常用"><a href="#各个模块分开启动（配置ssh是前提）常用" class="headerlink" title="各个模块分开启动（配置ssh是前提）常用"></a>各个模块分开启动（配置ssh是前提）常用</h4></li><li>整体启动/停止hdfs<br><code>start-dfs.sh</code><br><code>stop-dfs.sh</code></li><li>整体启动/停止yarn<br><code>start-yarn.sh</code><br><code>stop-yarn.sh</code><h4 id="全部启动（不建议使用）"><a href="#全部启动（不建议使用）" class="headerlink" title="全部启动（不建议使用）"></a>全部启动（不建议使用）</h4><code>start-all.sh</code><br><code>stop-all.sh</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> 环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 环境搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 ansible 部署 java(jdk) 环境</title>
      <link href="/2017-10-13-install-jdk.html"/>
      <url>/2017-10-13-install-jdk.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>使用 ansible 在本地 docker 容器集群中安装 java 环境  </p></blockquote><a id="more"></a><h4 id="创建容器-用虚拟机或服务器可忽略"><a href="#创建容器-用虚拟机或服务器可忽略" class="headerlink" title="创建容器 (用虚拟机或服务器可忽略)"></a>创建容器 (用虚拟机或服务器可忽略)</h4><h5 id="拉取基础镜像"><a href="#拉取基础镜像" class="headerlink" title="拉取基础镜像"></a>拉取基础镜像</h5><p>获取一个具备 ssh 服务的 ubuntu linux<br>基础镜像(<a href="http://waterandair.top/docker-base-use.html" target="_blank" rel="noopener">关于docker的基本使用</a>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull waterandair&#x2F;sshd</span><br></pre></td></tr></table></figure><h5 id="创建基础容器-数量随机-学习中建议-3-个"><a href="#创建基础容器-数量随机-学习中建议-3-个" class="headerlink" title="创建基础容器(数量随机,学习中建议 3 个)"></a>创建基础容器(数量随机,学习中建议 3 个)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 登录名&#x2F;密码:  root&#x2F;root</span><br><span class="line">docker run -d --name&#x3D;jdk1 waterandair&#x2F;sshd</span><br><span class="line">docker run -d --name&#x3D;jdk2 waterandair&#x2F;sshd</span><br><span class="line">docker run -d --name&#x3D;jdk3 waterandair&#x2F;sshd</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="初始化-role"><a href="#初始化-role" class="headerlink" title="初始化 role"></a>初始化 role</h4><p>在任意目录(这里以test文件夹为例)下执行  <code>ansible-galaxy init jdk</code> ,初始化一个 roles </p><h4 id="设置要用到的变量"><a href="#设置要用到的变量" class="headerlink" title="设置要用到的变量"></a>设置要用到的变量</h4><p>在 test/jdk/defaults/main.yml 中设置变量  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># defaults file for jdk</span><br><span class="line"># jdk 的版本</span><br><span class="line">jdk_version: jdk-9.0.4</span><br><span class="line"># jdk 文件较大,建议提前下好,并放到 jdk&#x2F;files&#x2F; 目录下</span><br><span class="line">jdk_package_name: jdk-9.0.4_linux-x64_bin.tar.gz</span><br><span class="line"># 设置环境变量的文件</span><br><span class="line">env_file: ~&#x2F;.bashrc</span><br><span class="line"># 安装目录</span><br><span class="line">install_dir: &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"># 软连接地址</span><br><span class="line">soft_dir: &#123;&#123; install_dir &#125;&#125;java</span><br></pre></td></tr></table></figure><h4 id="编写-tasks"><a href="#编写-tasks" class="headerlink" title="编写 tasks"></a>编写 tasks</h4><p>在 test/jdk/tasks/main.yml 中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for jdk</span><br><span class="line">- name: 创建java安装目录</span><br><span class="line">  file:</span><br><span class="line">    path: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line">    state: directory</span><br><span class="line">    mode: 0755</span><br><span class="line"></span><br><span class="line">- name: 复制并解压 jdk包 到安装目录</span><br><span class="line">  unarchive:</span><br><span class="line">    src: &quot;&#123;&#123;jdk_package_name&#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; install_dir &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">- name: 创建软连接</span><br><span class="line">  file:</span><br><span class="line">    src: &quot;&#123;&#123; install_dir + jdk_version &#125;&#125;&quot;</span><br><span class="line">    dest: &quot;&#123;&#123; soft_dir &#125;&#125;&quot;</span><br><span class="line">    state: link</span><br><span class="line"></span><br><span class="line">- name: 设置环境变量</span><br><span class="line">  lineinfile:</span><br><span class="line">    dest: &quot;&#123;&#123; env_file &#125;&#125;&quot;</span><br><span class="line">    insertafter: &quot;&#123;&#123; item.position &#125;&#125;&quot;</span><br><span class="line">    line: &quot;&#123;&#123; item.value &#125;&#125;&quot;</span><br><span class="line">    state: present</span><br><span class="line">  with_items:</span><br><span class="line">    - &#123;position: EOF, value: &quot;\n&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;&#123;&#123; &#39;export JAVA_HOME&#x3D;&#39; + soft_dir&#125;&#125;&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH&quot;&#125;</span><br><span class="line">    - &#123;position: EOF, value: &quot;export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar&quot;&#125;</span><br></pre></td></tr></table></figure><h4 id="编写启动文件"><a href="#编写启动文件" class="headerlink" title="编写启动文件"></a>编写启动文件</h4><p>在 test/ 下创建一个文件 run.yml </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  remote_user: root</span><br><span class="line">  roles:</span><br><span class="line">    - jdk</span><br></pre></td></tr></table></figure><h4 id="编写-hosts-文件"><a href="#编写-hosts-文件" class="headerlink" title="编写 hosts 文件"></a>编写 hosts 文件</h4><p>在 test/ 下创建一个文件 hosts</p><h5 id="查看-容器-ip"><a href="#查看-容器-ip" class="headerlink" title="查看 容器 ip"></a>查看 容器 ip</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># docker inspect --format&#x3D;&quot;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&quot; $CONTAINER_ID</span><br><span class="line">docker inspect --format&#x3D;&quot;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&quot; jdk1</span><br><span class="line">docker inspect --format&#x3D;&quot;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&quot; jdk2</span><br><span class="line">docker inspect --format&#x3D;&quot;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&quot; jdk3</span><br></pre></td></tr></table></figure><h5 id="创建-hosts-文件"><a href="#创建-hosts-文件" class="headerlink" title="创建 hosts 文件"></a>创建 hosts 文件</h5><p>这里定义要管理的远程主机,这里其实就是管理上面创建的 3 个 docker 容器的 IP 地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[test]</span><br><span class="line">172.17.0.2</span><br><span class="line">172.17.0.3</span><br><span class="line">172.17.0.4</span><br></pre></td></tr></table></figure><h4 id="执行-ansible-playbook"><a href="#执行-ansible-playbook" class="headerlink" title="执行 ansible-playbook"></a>执行 ansible-playbook</h4><p>在 test/文件夹下,执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ansible 是通过 ssh 执行命令的,所以最佳实践中推荐的是设置 ssh 免密码登录</span><br><span class="line"># 如果没有设置,而已在执行命令后加 -k 参数,手动输入密码, 这里 root 用户的密码是 root</span><br><span class="line"></span><br><span class="line">ansible-playbook -i hosts run.yml -k</span><br></pre></td></tr></table></figure><h4 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a><a href="https://github.com/waterandair/ansible-playbook-devops" target="_blank" rel="noopener">项目地址</a></h4>]]></content>
      
      
      <categories>
          
          <category> 环境搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据的数据采集流程</title>
      <link href="/2017-10-06-big-data-collect.html"/>
      <url>/2017-10-06-big-data-collect.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>大数据开发的数据来源基本介绍</p></blockquote><a id="more"></a><h3 id="离线数据采集"><a href="#离线数据采集" class="headerlink" title="离线数据采集"></a>离线数据采集</h3><p><img src="/images/offline-big-data-collect.png" alt="image"></p><h4 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h4><p>网站/APP向后台发送请求,获取数据,执行业务逻辑,都会经过服务器(nginx,apache,tomcat,jetty等),服务器会把每一次请求相关的信息,都保存起来,一般都是按照一定格式和命名规则保存到文件中.后台负责处理请求的代码(python,php,ruby,java等等),也会根据不同业务逻辑,按照一定格式记录特定的日志.</p><h4 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h4><p>几乎所有的互联网公司后端都不会只有一台服务器了,所以,每天生成的日志文件,会分布在多台服务器上,这时候,就需要用一个分布式的日志收集工具把日志定时收集到分布式存储系统(HDFS)上.简单来说,就是每天凌晨(或其他时间)把当天或前一天生成的日志文件,转移到flume监听的目录下, flume 在根据 sink 的配置进行后续转移,通常 sink 都设置为 HDFS</p><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>原始日志文件存储在 HDFS 中后,可能会有一些数据是不符合预期的脏数据,在进行正式的大数据计算之前,通常都需要执行一些 MapReduce 操作,或 hive sql 操作进行数据清洗</p><h4 id="数据建模"><a href="#数据建模" class="headerlink" title="数据建模"></a>数据建模</h4><p>日志文件中包含全部的信息,但是后续执行的特定业务场景的计算并不需要全部的数据,所以要将日志文件这个原始表,转换为符合特定业务场景的数据仓库,可能转换为几十张表,也可能是几百张</p><h4 id="大数据平台计算"><a href="#大数据平台计算" class="headerlink" title="大数据平台计算"></a>大数据平台计算</h4><p>Spark 的数据来源,通常都是符合特定需求的 Hive 表, spark 利用 hive 中的数据源,提供快速的,符合业务需求的大数据计算和分析</p><h3 id="实时数据采集"><a href="#实时数据采集" class="headerlink" title="实时数据采集"></a>实时数据采集</h3><p><img src="/images/online-big-data-collect.png" alt="image"></p><h4 id="数据源-1"><a href="#数据源-1" class="headerlink" title="数据源"></a>数据源</h4><p>和离线日志收集类似,实时数据的数据源也来自用户在各个平台上发送的请求,不同的一点时,实时流也有很多是通过”点击流”获取的.</p><h4 id="后端接收并处理请求"><a href="#后端接收并处理请求" class="headerlink" title="后端接收并处理请求"></a>后端接收并处理请求</h4><p>所谓点击流,举个例子,比如在网页中加入特定的ajax,用户执行某个点击操作的同时,向后端发送 ajax 请求,后端接收到请求之后,直接发送给 kafka 等消息队列,或者也可以写到 flume 监听的文件(使用tail命令监听的方式,可以监听到日志文件内容每一次的增加)或目录中</p><h4 id="实时收集日志"><a href="#实时收集日志" class="headerlink" title="实时收集日志"></a>实时收集日志</h4><p>实时数据,通常都是从分布式消息队列中读取的,比如 kafka </p><h4 id="大数据平台实时处理数据"><a href="#大数据平台实时处理数据" class="headerlink" title="大数据平台实时处理数据"></a>大数据平台实时处理数据</h4><p>Storm(毫秒级),Spark streaming(秒级)等系统可以实时的从 kafka 中获取数据,然后对数据进行处理和计算,比如不同维度实时的访问量,甚至一些复杂的机器学习,数据挖掘,实时推荐等.</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 基本操作</title>
      <link href="/2017-10-03-docker-base-use.html"/>
      <url>/2017-10-03-docker-base-use.html</url>
      
        <content type="html"><![CDATA[<p>早先学习一些集群知识, 都是用虚拟机, 后来又用了 vagrant, 都相当耗资源,直到遇见了 docker ……</p><a id="more"></a><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="安装Docker维护的版本"><a href="#安装Docker维护的版本" class="headerlink" title="安装Docker维护的版本"></a>安装Docker维护的版本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.docker.com | sudo</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">If you would like to use Docker as a non-root user, you should now consider</span><br><span class="line">adding your user to the &quot;docker&quot; group with something like:</span><br><span class="line"></span><br><span class="line">sudo usermod -aG docker your-user</span><br><span class="line"></span><br><span class="line">or:</span><br><span class="line"></span><br><span class="line">sudo groupadd docker</span><br><span class="line">sudo gpasswd -a $&#123;USER&#125; docker</span><br><span class="line">sudo service docker restart</span><br><span class="line"></span><br><span class="line">需要注销重新登录</span><br></pre></td></tr></table></figure><h3 id="容器操作"><a href="#容器操作" class="headerlink" title="容器操作"></a>容器操作</h3><h4 id="启动一个容器"><a href="#启动一个容器" class="headerlink" title="启动一个容器"></a>启动一个容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker run -i -t IMAGE [COMMAND] [ARG...]</span><br><span class="line"></span><br><span class="line">    -i interactive&#x3D;true | false 默认是 false</span><br><span class="line">    -t --tty &#x3D; true | false 默认是false  开启一个终端</span><br></pre></td></tr></table></figure><h4 id="启动一个守护式容器"><a href="#启动一个守护式容器" class="headerlink" title="启动一个守护式容器"></a>启动一个守护式容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 1.先启动一个交互式容器，然后按下 Ctrl+P Ctrl+Q</span><br><span class="line">docker run -i -t IMAGE &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 2. 直接启动守护式容器（推荐）</span><br><span class="line">docker run -d 镜像名 [command] [arg...] </span><br><span class="line"></span><br><span class="line">-d 表示后台执行容器中的命令，执行完后会容器会退出转为stop状态</span><br><span class="line">    如果想保持容器一直运行，就需要保持容器中前台有一个程序一直运行，比如 top，tail等等</span><br><span class="line">    docker run --name&#x3D;zj -d [image] &#x2F;bin&#x2F;sh -c &quot;while true; do echo hello &gt; &#x2F;dev&#x2F;null; sleep 10; done&quot;\</span><br></pre></td></tr></table></figure><h4 id="附加到运行中的容器"><a href="#附加到运行中的容器" class="headerlink" title="附加到运行中的容器"></a>附加到运行中的容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker attach 容器名</span><br></pre></td></tr></table></figure><h4 id="查看容器"><a href="#查看容器" class="headerlink" title="查看容器"></a>查看容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker ps [-a] [-l]</span><br><span class="line">    无选项 查看正在运行的容器</span><br><span class="line">    -a 查看所有的容器</span><br><span class="line">    -l 查看最近的容器</span><br><span class="line">    </span><br><span class="line">docker inspect [容器的id或者容器的名字]</span><br></pre></td></tr></table></figure><h4 id="自定义容器名"><a href="#自定义容器名" class="headerlink" title="自定义容器名"></a>自定义容器名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name&#x3D;自定义名 -i -t IMAGE &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h4 id="重新启动停止的容器"><a href="#重新启动停止的容器" class="headerlink" title="重新启动停止的容器"></a>重新启动停止的容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start [-i] 容器名</span><br></pre></td></tr></table></figure><h4 id="删除已经停止的容器"><a href="#删除已经停止的容器" class="headerlink" title="删除已经停止的容器"></a>删除已经停止的容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器名</span><br></pre></td></tr></table></figure><h4 id="查看容器日志"><a href="#查看容器日志" class="headerlink" title="查看容器日志"></a>查看容器日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker logs [-f] [-t] [--tail] 容器名</span><br><span class="line">    -f --follows&#x3D;true | false 默认为false  一直跟踪日志的变化并返回结果</span><br><span class="line">    -t --timstamps&#x3D;true | false 默认为false  返回的结果前加上时间戳</span><br><span class="line">    --tail&#x3D;&quot;all&quot;  不指定则返回所有的日志</span><br></pre></td></tr></table></figure><h4 id="查看容器内的进程"><a href="#查看容器内的进程" class="headerlink" title="查看容器内的进程"></a>查看容器内的进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker top 容器名</span><br></pre></td></tr></table></figure><h4 id="在运行中的容器中启动新进程"><a href="#在运行中的容器中启动新进程" class="headerlink" title="在运行中的容器中启动新进程"></a>在运行中的容器中启动新进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker exec [-d] [-i] [-t] 容器名 [command] [arg...]</span><br><span class="line">    -d </span><br><span class="line">    -i</span><br><span class="line">    -t</span><br></pre></td></tr></table></figure><h4 id="停止守护式容器"><a href="#停止守护式容器" class="headerlink" title="停止守护式容器"></a>停止守护式容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker stop 容器名 （发送一个kil命令，等待停止）</span><br><span class="line"></span><br><span class="line">docker kill 容器名 （快速结束容器，不等待）</span><br></pre></td></tr></table></figure><h4 id="删除所有容器"><a href="#删除所有容器" class="headerlink" title="删除所有容器"></a>删除所有容器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rm $(docker ps -aq)</span><br></pre></td></tr></table></figure><h4 id="自定义-hostname-和-hosts"><a href="#自定义-hostname-和-hosts" class="headerlink" title="自定义 hostname 和 hosts"></a>自定义 hostname 和 hosts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 运行容器时 加 -h [hostname] , 加 --add-host&#x3D;[host:ip]</span><br><span class="line">docker run --name&#x3D;cdh-4 -h chd-04 --add-host&#x3D;cdh-04:172.17.0.5 waterandair&#x2F;centos7-ssh &#x2F;usr&#x2F;sbin&#x2F;sshd -D</span><br></pre></td></tr></table></figure><h3 id="镜像操作"><a href="#镜像操作" class="headerlink" title="镜像操作"></a>镜像操作</h3><h4 id="列出镜像"><a href="#列出镜像" class="headerlink" title="列出镜像"></a>列出镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker images [OPTIONS] [REPOSITORY]</span><br><span class="line">    -a, --all&#x3D;false  显示所有镜像 默认不显示中间层镜像</span><br><span class="line">    -f, --filter&#x3D;[]  显示时的过滤条件</span><br><span class="line">    --no-trunc&#x3D;false  不使用截断的形式显示数据</span><br><span class="line">    -q, --quiet&#x3D;false  只显示镜像id</span><br></pre></td></tr></table></figure><h4 id="查看镜像详细信息"><a href="#查看镜像详细信息" class="headerlink" title="查看镜像详细信息"></a>查看镜像详细信息</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect [OPTIONS] CONTAINER | IMAGE [CONTAINER | IMAGE]</span><br></pre></td></tr></table></figure><h4 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker rmi [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line">    -f, --force&#x3D;false  Force removal of the image</span><br><span class="line">    --no-prune&#x3D;false  Do not delete untagged parents</span><br><span class="line">    </span><br><span class="line">eg.</span><br><span class="line"></span><br><span class="line">docker rmi ubuntu:14.04 </span><br><span class="line"></span><br><span class="line"># 删除resository 为 ubuntu 的所有镜像</span><br><span class="line">docke rmi $(docker images -q ubuntu)</span><br></pre></td></tr></table></figure><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><h5 id="镜像地址-docker-hub：-https-registry-hub-docker-com"><a href="#镜像地址-docker-hub：-https-registry-hub-docker-com" class="headerlink" title="镜像地址 docker hub： https://registry.hub.docker.com"></a>镜像地址 docker hub： <a href="https://registry.hub.docker.com" target="_blank" rel="noopener">https://registry.hub.docker.com</a></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker search [OPTIONS] TERM</span><br><span class="line">    -automated&#x3D;false  Only slow automated builds</span><br><span class="line">    --no-trunc&#x3D;false  Don&#39;t truncate output</span><br><span class="line">    -s, --stars&#x3D;0  Only displays with at least [num] stars</span><br></pre></td></tr></table></figure><h4 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker pull [OPTIONS] NAME [:TAG]</span><br><span class="line">    -a, --all-tags&#x3D;false Download all tagged images in the repository</span><br><span class="line">    </span><br><span class="line">修改获取镜像地址：</span><br><span class="line">    使用 --registy-mirror 选项  </span><br><span class="line">        1. 修改: &#x2F;etc&#x2F;default&#x2F;docker</span><br><span class="line">        2. 添加： DOCKER_OPTS&#x3D; &quot;--registy-mirror&#x3D;https:&#x2F;&#x2F;www.daocloud.io&quot;</span><br></pre></td></tr></table></figure><h4 id="构建镜象"><a href="#构建镜象" class="headerlink" title="构建镜象"></a>构建镜象</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">docker commit     通过容器构建</span><br><span class="line">docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</span><br><span class="line">    -a, --author&#x3D;&quot;&quot;    Author</span><br><span class="line">    -m, --message&#x3D;&quot;&quot;   Commit message</span><br><span class="line">    -p, --pause&#x3D;true   Pause container during commit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker build      通过 Dockerfile 文件构建</span><br><span class="line">docker build [OPTIONS] PATH</span><br><span class="line">    --force-rm&#x3D;false</span><br><span class="line">    --no-cache&#x3D;false  不使用缓存</span><br><span class="line">    --pull&#x3D;false</span><br><span class="line">    -q, --quiet&#x3D;false</span><br><span class="line">    --rm&#x3D;true</span><br><span class="line">    -t, --tag&#x3D;&quot;&quot;</span><br><span class="line"></span><br><span class="line">创建一个Dockerfile</span><br><span class="line"># First Dockerfile</span><br><span class="line">FROM ubuntu:14.04</span><br><span class="line">MAINTAINER zj &quot;zj@zj.com&quot;</span><br><span class="line">RUN apt-get update</span><br><span class="line">RUN apt-get install -y nginx</span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure><h4 id="查看镜像构建的过程"><a href="#查看镜像构建的过程" class="headerlink" title="查看镜像构建的过程"></a>查看镜像构建的过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker history [image]</span><br></pre></td></tr></table></figure><h4 id="提交自己的镜像到-docker-hub"><a href="#提交自己的镜像到-docker-hub" class="headerlink" title="提交自己的镜像到 docker hub"></a>提交自己的镜像到 docker hub</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push username&#x2F;imagename[:tag]</span><br></pre></td></tr></table></figure><h4 id="修改镜像的tag"><a href="#修改镜像的tag" class="headerlink" title="修改镜像的tag"></a>修改镜像的tag</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag username&#x2F;imagename[:tag]  username&#x2F;imagename[:tag]</span><br></pre></td></tr></table></figure><h3 id="Docker-容器网络"><a href="#Docker-容器网络" class="headerlink" title="Docker 容器网络"></a>Docker 容器网络</h3><p><a href="http://www.atjiang.com/docker-single-host-network-exposure/" target="_blank" rel="noopener">http://www.atjiang.com/docker-single-host-network-exposure/</a></p><h4 id="网桥"><a href="#网桥" class="headerlink" title="网桥"></a>网桥</h4><p>网桥是基于OSI七层模型中的数据链路层的<br>特点：</p><ul><li>可以设置IP地址</li><li>相当于拥有一个隐藏的虚拟网卡</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker0 默认的地址划分</span><br><span class="line"></span><br><span class="line">IP: 172.17.42.1    子网掩码: 255.255.0.0</span><br><span class="line">MAC: 02:42:AC:11:00:00 到 02:42:ac:11:ff:ff</span><br><span class="line">总共提供了 65534 个地址</span><br></pre></td></tr></table></figure><h4 id="网桥管理工具："><a href="#网桥管理工具：" class="headerlink" title="网桥管理工具："></a>网桥管理工具：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install bridge-utils  &amp;&amp; brctl show</span><br></pre></td></tr></table></figure><h4 id="修改docker0-地址："><a href="#修改docker0-地址：" class="headerlink" title="修改docker0 地址："></a>修改docker0 地址：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ifconfig docker0 192.168.200.1 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><h4 id="添加虚拟网桥："><a href="#添加虚拟网桥：" class="headerlink" title="添加虚拟网桥："></a>添加虚拟网桥：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo brctl addbr br0</span><br><span class="line">sudo ifconfig br0 192.168.100.1 netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line"># 更改 docker 守护进程的启动配置</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;default&#x2F;docker  中添加 DOCKER_OPS 值</span><br><span class="line">-b &#x3D; br0</span><br></pre></td></tr></table></figure><h4 id="启动容器关于网络的选项"><a href="#启动容器关于网络的选项" class="headerlink" title="启动容器关于网络的选项"></a>启动容器关于网络的选项</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--link 给容器设置一个host别名</span><br><span class="line">    docker run --link&#x3D;[CONTAINER_NAME]:[ALIAS] [IMAGE] [COMMOND]</span><br><span class="line"></span><br><span class="line">--icc&#x3D;false  拒绝所有容器间互联</span><br><span class="line">--iptables&#x3D;true  添加防火墙</span><br><span class="line">--ip-forward&#x3D;true  # sysctl net.ipv4.conf.all.forwarding</span><br></pre></td></tr></table></figure><h4 id="获取容器的ip地址"><a href="#获取容器的ip地址" class="headerlink" title="获取容器的ip地址"></a>获取容器的ip地址</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker inspect --format&#x3D;&#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#39; $CONTAINER_ID</span><br><span class="line"></span><br><span class="line">获取 ip hostname</span><br><span class="line">docker inspect --format&#x3D;&#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125; &#123;&#123;.Config.Hostname&#125;&#125;&#39; app1</span><br><span class="line"></span><br><span class="line">获取容器ip 和对应的 name</span><br><span class="line">docker inspect --format&#x3D;&#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125; &#123;&#123;index (split .Name &quot;&#x2F;&quot;) 1&#125;&#125;&#39; $(docker ps -aq)</span><br></pre></td></tr></table></figure><h3 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h3><h4 id="数据卷的特点"><a href="#数据卷的特点" class="headerlink" title="数据卷的特点"></a>数据卷的特点</h4><ul><li>数据卷再容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中</li><li>数据卷可以再容器之间共享和重用</li><li>可以对数据卷里的内容直接进行修改</li><li>数据卷的变化不会影响镜像的更新</li><li>卷会一直存在，即使挂载数据卷的容器已经被删除</li></ul><h4 id="为容器添加数据卷"><a href="#为容器添加数据卷" class="headerlink" title="为容器添加数据卷"></a>为容器添加数据卷</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这是命令行的方式，docker 更推荐使用 数据卷容器 的方法</span><br><span class="line">sudo docker run -v ~&#x2F;local_data:&#x2F;container_data -it ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h4 id="为数据卷添加访问权限"><a href="#为数据卷添加访问权限" class="headerlink" title="为数据卷添加访问权限"></a>为数据卷添加访问权限</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -v ~&#x2F;datavolume:&#x2F;data:ro -it ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h4 id="使用-dockerfile-构建包含数据卷的镜像"><a href="#使用-dockerfile-构建包含数据卷的镜像" class="headerlink" title="使用 dockerfile 构建包含数据卷的镜像"></a>使用 dockerfile 构建包含数据卷的镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VOLUME [&quot;&#x2F;data&quot;]</span><br></pre></td></tr></table></figure><h4 id="挂载数据卷容器"><a href="#挂载数据卷容器" class="headerlink" title="挂载数据卷容器"></a>挂载数据卷容器</h4><p>命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --volumes-form [CONTAINER NAME]</span><br><span class="line">数据卷容器仅仅起传递配置的作用</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible 一些基本使用方法的记录</title>
      <link href="/2017-09-30-ansible-base-use%20copy.html"/>
      <url>/2017-09-30-ansible-base-use%20copy.html</url>
      
        <content type="html"><![CDATA[<p>ansible是一款轻量级自动化运维工具，由Python语言开发，结合了多种自动化运维工具的特性，实现了批量系统配置、批量程序部署、批量命令执行等功能,这里记录一些使用过程中常用到的功能模块和小技巧……</p><p>使用 ansible 于我有几点好处, 第一 ansible 本身写法要求每一步都要有 “name”, 一方面运行时可以查看log,一方面也相当于记录了笔记.第二,ansible 本身就是应用于自动化运维,在部署集群的时候,比执行shell脚本方便很多.</p><a id="more"></a><h4 id="执行远程命令的几种方式以及区别"><a href="#执行远程命令的几种方式以及区别" class="headerlink" title="执行远程命令的几种方式以及区别"></a>执行远程命令的几种方式以及区别</h4><p>按照功能有小到大排序依次是: command,raw,shell,script </p><ul><li>command 模块在执行 Linux 命令的时候不能使用管道</li><li>raw 模块可以使用管道, raw 相当于使用 SSH 直接执行 Linux 命令</li><li>shell 模块也可以使用管道,还可以执行远程服务器上的 shell 脚本文件(绝对路径)</li><li>script 模块可以在远程服务器上执行主控节点的脚本文件,其功能相当于 scp + shell 的组合,脚本执行完后会在远程服务器上删除脚本文件</li></ul><h4 id="判断某个文件-文件夹是否存在"><a href="#判断某个文件-文件夹是否存在" class="headerlink" title="判断某个文件/文件夹是否存在"></a>判断某个文件/文件夹是否存在</h4><p>ansible 常用模块中没有这个功能,可以使用 register 参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">- name: 判断文件是否存在</span><br><span class="line">  comman: ls &#x2F;path&#x2F;file</span><br><span class="line">  ignore_errors: True</span><br><span class="line">  register: result</span><br><span class="line"></span><br><span class="line">- name: 文件存在执行的操作</span><br><span class="line">  command: echo &quot;file exit&quot;</span><br><span class="line">  when: result|succeeded</span><br><span class="line"></span><br><span class="line">- name: 文件不存在执行的操作</span><br><span class="line">  command: echo &quot;file not exit&quot;</span><br><span class="line">  when: result|failed</span><br></pre></td></tr></table></figure><h4 id="三种方式操作主控节点"><a href="#三种方式操作主控节点" class="headerlink" title="三种方式操作主控节点"></a>三种方式操作主控节点</h4><h5 id="delegate-to"><a href="#delegate-to" class="headerlink" title="delegate_to"></a>delegate_to</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># delegate_to 更多的是用于把某个特别的任务指派给远程服务器中的某个服务器</span><br><span class="line">delegate_to: 127.0.0.1</span><br></pre></td></tr></table></figure><h5 id="local-action"><a href="#local-action" class="headerlink" title="local_action"></a>local_action</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 给予的步骤仅仅会在本地机器上运行</span><br><span class="line">local_action: command ls &#x2F;path&#x2F;file</span><br></pre></td></tr></table></figure><h5 id="connection-local"><a href="#connection-local" class="headerlink" title="connection: local"></a>connection: local</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 在某一段 action 后面加上 connection: local 指明当前操作是在管理本地主机</span><br><span class="line">connection: local</span><br></pre></td></tr></table></figure><h4 id="设定python解释器的位置"><a href="#设定python解释器的位置" class="headerlink" title="设定python解释器的位置"></a>设定python解释器的位置</h4><p>默认是 /usr/bin/python,但是在 ubuntu 中可能是 /usr/bin/python3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 设置变量</span><br><span class="line"></span><br><span class="line">ansible_python_interpreter&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><h4 id="设置变量的集中方式-对于-ansible-galaxy-最佳实践形式来说"><a href="#设置变量的集中方式-对于-ansible-galaxy-最佳实践形式来说" class="headerlink" title="设置变量的集中方式(对于 ansible-galaxy 最佳实践形式来说)"></a>设置变量的集中方式(对于 ansible-galaxy 最佳实践形式来说)</h4><h5 id="可以写到-hosts-文件中"><a href="#可以写到-hosts-文件中" class="headerlink" title="可以写到 hosts 文件中"></a>可以写到 hosts 文件中</h5><p>注意是 key=value 形式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test:vars]</span><br><span class="line">ansible_python_interpreter&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><h5 id="可以写到执行-ansible-playbook-的-yml-文件中"><a href="#可以写到执行-ansible-playbook-的-yml-文件中" class="headerlink" title="可以写到执行 ansible-playbook 的 ***.yml 文件中"></a>可以写到执行 ansible-playbook 的 ***.yml 文件中</h5><p>注意是 key:value 形式 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  vars:</span><br><span class="line">    ansible_python_interpreter:&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><p>如果变量有很多的话,建议写到一个文件中统一管理,对于ansible-galaxy , 是在 vars/main.yml 中管理变量</p><p>如果不是 ansible-galaxy 形式(这种形式是最佳实践), 可以使用 vars_files 参数引入一个文件</p><p>***.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- hosts: test</span><br><span class="line">  vars: </span><br><span class="line">    color: blue</span><br><span class="line">  vars_files:</span><br><span class="line">   - &#x2F;vars&#x2F;external_vars.yml</span><br></pre></td></tr></table></figure><p>/vars/external_vars.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--- </span><br><span class="line">someval: val</span><br><span class="line">pwd: qwe000</span><br></pre></td></tr></table></figure><h5 id="保存到-vars-main-yml-文件中"><a href="#保存到-vars-main-yml-文件中" class="headerlink" title="保存到 vars/main.yml 文件中"></a>保存到 vars/main.yml 文件中</h5><p>注意是 key:value 形式</p><h4 id="复制主控节点的文件到目标服务器"><a href="#复制主控节点的文件到目标服务器" class="headerlink" title="复制主控节点的文件到目标服务器"></a>复制主控节点的文件到目标服务器</h4><p>使用 copy 模块, 功能给类似于 scp ,但同事具有设置文件权限和所有者的功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">copy:</span><br><span class="line">  src:  # 源文件地址(可以是绝对路径和相对路径)</span><br><span class="line">  dest:  # 文件复制的目的地,必须是一个绝对路径,如果源文件是一个目录,这里也必须是一个目录</span><br><span class="line">  force:  # 默认是 yes, 表示目标主机包含该文件,但内容不同时,会强行覆盖,如果为 no, 表示只有当目标主机不存在该文件时,才会复制</span><br><span class="line">  backup: # 默认是 no,如果为 yes, 在覆盖之前将原文件进行备份</span><br><span class="line">  </span><br><span class="line">  others: 所有 file 模块的选项都可以在这里使用</span><br></pre></td></tr></table></figure><h4 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h4><p>unarchive, 类似于 linux 中的 tar, 作用是将主控节点的压缩包复制到远程服务器,然后解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unarchive:</span><br><span class="line">  remote_src: yes&#x2F;no, 表示解压的文件在远程服务器中,还是在控制节点中,默认为 no, 在控制节点中,先将文件复制到远程服务器中,再进行解压 (这里就不需要多用一次 copy 了)</span><br><span class="line">  src: 指定压缩文件的路径,该选项的取值取决于 remote_src 的值, 如果为 yes, 则 src 表示的是远程服务器中压缩包的地址,如果为 no, 则 src 指向的是主控节点中的位置</span><br><span class="line">  dest: 远程服务器上的一个绝对路径,表示压缩文件解压的位置</span><br><span class="line">  list_files: 默认为 yes, 表示在 ansible 中返回解压文件的文件列表</span><br><span class="line">  exclude: 解压文件时,排除 exclude 选项指定的文件或目录</span><br></pre></td></tr></table></figure><h4 id="使用-templates-向远程服务器发送自定义的文件内容"><a href="#使用-templates-向远程服务器发送自定义的文件内容" class="headerlink" title="使用 templates 向远程服务器发送自定义的文件内容"></a>使用 templates 向远程服务器发送自定义的文件内容</h4><p>template使用了Jinjia2格式作为文件模版，进行文档内变量的替换的模块.</p><p>在使用 ansible-galaxy 的最佳实践中,把模板文件放到 templates 下, task 中引用的时候,直接写文件名就可以</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># foo.j2文件经过填写参数后，复制到远程节点的&#x2F;etc&#x2F;file.conf，文件权限相关略过</span><br><span class="line">- template: src&#x3D;foo.j2 dest&#x3D;&#x2F;etc&#x2F;file.conf owner&#x3D;bin group&#x3D;wheel mode&#x3D;0644</span><br><span class="line"></span><br><span class="line"># 跟上面一样的效果，不一样的文件权限设置方式</span><br><span class="line">- template: src&#x3D;foo.j2 dest&#x3D;&#x2F;etc&#x2F;file.conf owner&#x3D;bin group&#x3D;wheel mode&#x3D;&quot;u&#x3D;rw,g&#x3D;r,o&#x3D;r&quot;</span><br></pre></td></tr></table></figure><h4 id="file-模块的使用-很常用"><a href="#file-模块的使用-很常用" class="headerlink" title="file 模块的使用(很常用)"></a>file 模块的使用(很常用)</h4><p>file 模块主要用于对远程服务器上的文件(包括链接和目录)进行操作,包括修改文件的权限,修改文件的所有者,创建文件,删除文件等.  </p><p>常用选项如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">path:  # 指定文件&#x2F;目录的路径</span><br><span class="line">recurse:  # 递归设置文件属性,只对目录有效</span><br><span class="line">group:  # 定义文件&#x2F;目录的组</span><br><span class="line">mode:  # 定义文件&#x2F;目录的权限</span><br><span class="line">owner:  # 定义文件&#x2F;目录的所有者</span><br><span class="line">src:  # 要被链接的源文件路径,只应用于 state 为 link 的情况</span><br><span class="line">dest:  # 被链接到的路径,只应用于 state 为 link 的情况</span><br><span class="line">force:  # 在两种情况下强制创建软链接, 一种是源文件不存在,但之后会建立;二是目标软链接已经存在,会删除重建</span><br><span class="line">state: 多个取值:</span><br><span class="line">       directory:  # 目录不存在,创建目录</span><br><span class="line">       file:      # 文件不存在,也不会创建</span><br><span class="line">       link:      # 创建软链接</span><br><span class="line">       hard:      # 创建硬链接</span><br><span class="line">       touch:     # 如果文件不存在,创建一个新的文件,如果文件或目录已存在,更新其访问时间和修改时间</span><br><span class="line">       absent:    # 删除目录,文件或链接</span><br></pre></td></tr></table></figure><h4 id="读取-inventory-中的ip列表"><a href="#读取-inventory-中的ip列表" class="headerlink" title="读取 inventory 中的ip列表"></a>读取 inventory 中的ip列表</h4><p>使用内置变量 <code>ansible_play_batch</code>  </p><p>比如在配置 zookeeper 的 配置文件的时候, 需要指定集群中所有的节点:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br><span class="line">server.2&#x3D;172.17.0.3:2888:3888</span><br><span class="line">server.3&#x3D;172.17.0.4:2888:3888</span><br></pre></td></tr></table></figure><p>在 ansible 中可以在 templates 目录下创建一个模板,模板中使用 jinja2 的循环完成这个任务:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% for host in ansible_play_batch %&#125;</span><br><span class="line">server.&#123;&#123; loop.index &#125;&#125;&#x3D;&#123;&#123; host &#125;&#125;:2888:3888</span><br><span class="line">&#123;% endfor %&#125;</span><br></pre></td></tr></table></figure><p>这样就避免了每次配置集群都要改模板的尴尬了</p><h4 id="获取当前服务器的ip"><a href="#获取当前服务器的ip" class="headerlink" title="获取当前服务器的ip"></a>获取当前服务器的ip</h4><p>一般获取ip都会从 facts 中获取,但有时候这种方法获取不到,可以使用另一种变通的方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 因为 ansible 是用 ssh 的,所以 ansible_env.SSH_CONNECTION 变量一定存在, </span><br><span class="line"># ansible_env.SSH_CONNECTION: &quot;172.17.0.1 20742 172.17.0.4 22&quot;</span><br><span class="line"># 可以通过这个字符串获取到 ip</span><br><span class="line"># 在 ansible-playbook 的 templates 中可以这样使用:</span><br><span class="line">&#123;% set ip &#x3D; ansible_env.SSH_CONNECTION.split(&quot; &quot;)[2] %&#125;</span><br><span class="line">listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123; ip &#125;&#125;:9092</span><br></pre></td></tr></table></figure><h4 id="获取目标主机的环境变量"><a href="#获取目标主机的环境变量" class="headerlink" title="获取目标主机的环境变量"></a>获取目标主机的环境变量</h4><p>ansible 在执行过程中,用到的环境变量和服务器中设置的不一样,造成在执行一些命令的时候找不到目标文件<br>可能是因为 login shell &amp; nonlogin shell 的原因(<a href="http://blog.csdn.net/u010871982/article/details/78525367" target="_blank" rel="noopener">http://blog.csdn.net/u010871982/article/details/78525367</a>)<br>但我在本地 docker 容器中试了<code>/etc/bashrc</code> 和 <code>~/.bashrc</code> 中加入环境变量,都没有用,所以只好在需要环境变量的地方使用 <code>environment</code> 设定 <code>PATH</code> 为执行远程主机命令需要的值, 这种方法的一个缺点是,如果远程服务器集群中的环境变量不一致,就不能用了,但这种情况,应该比较少把……</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># 这样,就可以把目标主机的环境变量传递给 get_path 变量, 可以在后续的 task 中使用</span><br><span class="line">- name: 获取服务器的环境变量</span><br><span class="line">  shell: java --version</span><br><span class="line">  environment:</span><br><span class="line">    PATH: &#x2F;usr&#x2F;local&#x2F;java&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible 一些基本使用方法的记录</title>
      <link href="/2017-09-30-ansible-base-use.html"/>
      <url>/2017-09-30-ansible-base-use.html</url>
      
        <content type="html"><![CDATA[<p>ansible是一款轻量级自动化运维工具，由Python语言开发，结合了多种自动化运维工具的特性，实现了批量系统配置、批量程序部署、批量命令执行等功能,这里记录一些使用过程中常用到的功能模块和小技巧……</p><p>使用 ansible 于我有几点好处, 第一 ansible 本身写法要求每一步都要有 “name”, 一方面运行时可以查看log,一方面也相当于记录了笔记.第二,ansible 本身就是应用于自动化运维,在部署集群的时候,比执行shell脚本方便很多.</p><a id="more"></a><h4 id="执行远程命令的几种方式以及区别"><a href="#执行远程命令的几种方式以及区别" class="headerlink" title="执行远程命令的几种方式以及区别"></a>执行远程命令的几种方式以及区别</h4><p>按照功能有小到大排序依次是: command,raw,shell,script </p><ul><li>command 模块在执行 Linux 命令的时候不能使用管道</li><li>raw 模块可以使用管道, raw 相当于使用 SSH 直接执行 Linux 命令</li><li>shell 模块也可以使用管道,还可以执行远程服务器上的 shell 脚本文件(绝对路径)</li><li>script 模块可以在远程服务器上执行主控节点的脚本文件,其功能相当于 scp + shell 的组合,脚本执行完后会在远程服务器上删除脚本文件</li></ul><h4 id="判断某个文件-文件夹是否存在"><a href="#判断某个文件-文件夹是否存在" class="headerlink" title="判断某个文件/文件夹是否存在"></a>判断某个文件/文件夹是否存在</h4><p>ansible 常用模块中没有这个功能,可以使用 register 参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">- name: 判断文件是否存在</span><br><span class="line">  comman: ls &#x2F;path&#x2F;file</span><br><span class="line">  ignore_errors: True</span><br><span class="line">  register: result</span><br><span class="line"></span><br><span class="line">- name: 文件存在执行的操作</span><br><span class="line">  command: echo &quot;file exit&quot;</span><br><span class="line">  when: result|succeeded</span><br><span class="line"></span><br><span class="line">- name: 文件不存在执行的操作</span><br><span class="line">  command: echo &quot;file not exit&quot;</span><br><span class="line">  when: result|failed</span><br></pre></td></tr></table></figure><h4 id="三种方式操作主控节点"><a href="#三种方式操作主控节点" class="headerlink" title="三种方式操作主控节点"></a>三种方式操作主控节点</h4><h5 id="delegate-to"><a href="#delegate-to" class="headerlink" title="delegate_to"></a>delegate_to</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># delegate_to 更多的是用于把某个特别的任务指派给远程服务器中的某个服务器</span><br><span class="line">delegate_to: 127.0.0.1</span><br></pre></td></tr></table></figure><h5 id="local-action"><a href="#local-action" class="headerlink" title="local_action"></a>local_action</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 给予的步骤仅仅会在本地机器上运行</span><br><span class="line">local_action: command ls &#x2F;path&#x2F;file</span><br></pre></td></tr></table></figure><h5 id="connection-local"><a href="#connection-local" class="headerlink" title="connection: local"></a>connection: local</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 在某一段 action 后面加上 connection: local 指明当前操作是在管理本地主机</span><br><span class="line">connection: local</span><br></pre></td></tr></table></figure><h4 id="设定python解释器的位置"><a href="#设定python解释器的位置" class="headerlink" title="设定python解释器的位置"></a>设定python解释器的位置</h4><p>默认是 /usr/bin/python,但是在 ubuntu 中可能是 /usr/bin/python3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 设置变量</span><br><span class="line"></span><br><span class="line">ansible_python_interpreter&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><h4 id="设置变量的集中方式-对于-ansible-galaxy-最佳实践形式来说"><a href="#设置变量的集中方式-对于-ansible-galaxy-最佳实践形式来说" class="headerlink" title="设置变量的集中方式(对于 ansible-galaxy 最佳实践形式来说)"></a>设置变量的集中方式(对于 ansible-galaxy 最佳实践形式来说)</h4><h5 id="可以写到-hosts-文件中"><a href="#可以写到-hosts-文件中" class="headerlink" title="可以写到 hosts 文件中"></a>可以写到 hosts 文件中</h5><p>注意是 key=value 形式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test:vars]</span><br><span class="line">ansible_python_interpreter&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><h5 id="可以写到执行-ansible-playbook-的-yml-文件中"><a href="#可以写到执行-ansible-playbook-的-yml-文件中" class="headerlink" title="可以写到执行 ansible-playbook 的 ***.yml 文件中"></a>可以写到执行 ansible-playbook 的 ***.yml 文件中</h5><p>注意是 key:value 形式 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- hosts: test</span><br><span class="line">  vars:</span><br><span class="line">    ansible_python_interpreter:&#x2F;usr&#x2F;bin&#x2F;python3</span><br></pre></td></tr></table></figure><p>如果变量有很多的话,建议写到一个文件中统一管理,对于ansible-galaxy , 是在 vars/main.yml 中管理变量</p><p>如果不是 ansible-galaxy 形式(这种形式是最佳实践), 可以使用 vars_files 参数引入一个文件</p><p>***.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">- hosts: test</span><br><span class="line">  vars: </span><br><span class="line">    color: blue</span><br><span class="line">  vars_files:</span><br><span class="line">   - &#x2F;vars&#x2F;external_vars.yml</span><br></pre></td></tr></table></figure><p>/vars/external_vars.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--- </span><br><span class="line">someval: val</span><br><span class="line">pwd: qwe000</span><br></pre></td></tr></table></figure><h5 id="保存到-vars-main-yml-文件中"><a href="#保存到-vars-main-yml-文件中" class="headerlink" title="保存到 vars/main.yml 文件中"></a>保存到 vars/main.yml 文件中</h5><p>注意是 key:value 形式</p><h4 id="复制主控节点的文件到目标服务器"><a href="#复制主控节点的文件到目标服务器" class="headerlink" title="复制主控节点的文件到目标服务器"></a>复制主控节点的文件到目标服务器</h4><p>使用 copy 模块, 功能给类似于 scp ,但同事具有设置文件权限和所有者的功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">copy:</span><br><span class="line">  src:  # 源文件地址(可以是绝对路径和相对路径)</span><br><span class="line">  dest:  # 文件复制的目的地,必须是一个绝对路径,如果源文件是一个目录,这里也必须是一个目录</span><br><span class="line">  force:  # 默认是 yes, 表示目标主机包含该文件,但内容不同时,会强行覆盖,如果为 no, 表示只有当目标主机不存在该文件时,才会复制</span><br><span class="line">  backup: # 默认是 no,如果为 yes, 在覆盖之前将原文件进行备份</span><br><span class="line">  </span><br><span class="line">  others: 所有 file 模块的选项都可以在这里使用</span><br></pre></td></tr></table></figure><h4 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h4><p>unarchive, 类似于 linux 中的 tar, 作用是将主控节点的压缩包复制到远程服务器,然后解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unarchive:</span><br><span class="line">  remote_src: yes&#x2F;no, 表示解压的文件在远程服务器中,还是在控制节点中,默认为 no, 在控制节点中,先将文件复制到远程服务器中,再进行解压 (这里就不需要多用一次 copy 了)</span><br><span class="line">  src: 指定压缩文件的路径,该选项的取值取决于 remote_src 的值, 如果为 yes, 则 src 表示的是远程服务器中压缩包的地址,如果为 no, 则 src 指向的是主控节点中的位置</span><br><span class="line">  dest: 远程服务器上的一个绝对路径,表示压缩文件解压的位置</span><br><span class="line">  list_files: 默认为 yes, 表示在 ansible 中返回解压文件的文件列表</span><br><span class="line">  exclude: 解压文件时,排除 exclude 选项指定的文件或目录</span><br></pre></td></tr></table></figure><h4 id="使用-templates-向远程服务器发送自定义的文件内容"><a href="#使用-templates-向远程服务器发送自定义的文件内容" class="headerlink" title="使用 templates 向远程服务器发送自定义的文件内容"></a>使用 templates 向远程服务器发送自定义的文件内容</h4><p>template使用了Jinjia2格式作为文件模版，进行文档内变量的替换的模块.</p><p>在使用 ansible-galaxy 的最佳实践中,把模板文件放到 templates 下, task 中引用的时候,直接写文件名就可以</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># foo.j2文件经过填写参数后，复制到远程节点的&#x2F;etc&#x2F;file.conf，文件权限相关略过</span><br><span class="line">- template: src&#x3D;foo.j2 dest&#x3D;&#x2F;etc&#x2F;file.conf owner&#x3D;bin group&#x3D;wheel mode&#x3D;0644</span><br><span class="line"></span><br><span class="line"># 跟上面一样的效果，不一样的文件权限设置方式</span><br><span class="line">- template: src&#x3D;foo.j2 dest&#x3D;&#x2F;etc&#x2F;file.conf owner&#x3D;bin group&#x3D;wheel mode&#x3D;&quot;u&#x3D;rw,g&#x3D;r,o&#x3D;r&quot;</span><br></pre></td></tr></table></figure><h4 id="file-模块的使用-很常用"><a href="#file-模块的使用-很常用" class="headerlink" title="file 模块的使用(很常用)"></a>file 模块的使用(很常用)</h4><p>file 模块主要用于对远程服务器上的文件(包括链接和目录)进行操作,包括修改文件的权限,修改文件的所有者,创建文件,删除文件等.  </p><p>常用选项如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">path:  # 指定文件&#x2F;目录的路径</span><br><span class="line">recurse:  # 递归设置文件属性,只对目录有效</span><br><span class="line">group:  # 定义文件&#x2F;目录的组</span><br><span class="line">mode:  # 定义文件&#x2F;目录的权限</span><br><span class="line">owner:  # 定义文件&#x2F;目录的所有者</span><br><span class="line">src:  # 要被链接的源文件路径,只应用于 state 为 link 的情况</span><br><span class="line">dest:  # 被链接到的路径,只应用于 state 为 link 的情况</span><br><span class="line">force:  # 在两种情况下强制创建软链接, 一种是源文件不存在,但之后会建立;二是目标软链接已经存在,会删除重建</span><br><span class="line">state: 多个取值:</span><br><span class="line">       directory:  # 目录不存在,创建目录</span><br><span class="line">       file:      # 文件不存在,也不会创建</span><br><span class="line">       link:      # 创建软链接</span><br><span class="line">       hard:      # 创建硬链接</span><br><span class="line">       touch:     # 如果文件不存在,创建一个新的文件,如果文件或目录已存在,更新其访问时间和修改时间</span><br><span class="line">       absent:    # 删除目录,文件或链接</span><br></pre></td></tr></table></figure><h4 id="读取-inventory-中的ip列表"><a href="#读取-inventory-中的ip列表" class="headerlink" title="读取 inventory 中的ip列表"></a>读取 inventory 中的ip列表</h4><p>使用内置变量 <code>ansible_play_batch</code>  </p><p>比如在配置 zookeeper 的 配置文件的时候, 需要指定集群中所有的节点:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1&#x3D;172.17.0.2:2888:3888</span><br><span class="line">server.2&#x3D;172.17.0.3:2888:3888</span><br><span class="line">server.3&#x3D;172.17.0.4:2888:3888</span><br></pre></td></tr></table></figure><p>在 ansible 中可以在 templates 目录下创建一个模板,模板中使用 jinja2 的循环完成这个任务:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% for host in ansible_play_batch %&#125;</span><br><span class="line">server.&#123;&#123; loop.index &#125;&#125;&#x3D;&#123;&#123; host &#125;&#125;:2888:3888</span><br><span class="line">&#123;% endfor %&#125;</span><br></pre></td></tr></table></figure><p>这样就避免了每次配置集群都要改模板的尴尬了</p><h4 id="获取当前服务器的ip"><a href="#获取当前服务器的ip" class="headerlink" title="获取当前服务器的ip"></a>获取当前服务器的ip</h4><p>一般获取ip都会从 facts 中获取,但有时候这种方法获取不到,可以使用另一种变通的方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 因为 ansible 是用 ssh 的,所以 ansible_env.SSH_CONNECTION 变量一定存在, </span><br><span class="line"># ansible_env.SSH_CONNECTION: &quot;172.17.0.1 20742 172.17.0.4 22&quot;</span><br><span class="line"># 可以通过这个字符串获取到 ip</span><br><span class="line"># 在 ansible-playbook 的 templates 中可以这样使用:</span><br><span class="line">&#123;% set ip &#x3D; ansible_env.SSH_CONNECTION.split(&quot; &quot;)[2] %&#125;</span><br><span class="line">listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123; ip &#125;&#125;:9092</span><br></pre></td></tr></table></figure><h4 id="获取目标主机的环境变量"><a href="#获取目标主机的环境变量" class="headerlink" title="获取目标主机的环境变量"></a>获取目标主机的环境变量</h4><p>ansible 在执行过程中,用到的环境变量和服务器中设置的不一样,造成在执行一些命令的时候找不到目标文件<br>可能是因为 login shell &amp; nonlogin shell 的原因(<a href="http://blog.csdn.net/u010871982/article/details/78525367" target="_blank" rel="noopener">http://blog.csdn.net/u010871982/article/details/78525367</a>)<br>但我在本地 docker 容器中试了<code>/etc/bashrc</code> 和 <code>~/.bashrc</code> 中加入环境变量,都没有用,所以只好在需要环境变量的地方使用 <code>environment</code> 设定 <code>PATH</code> 为执行远程主机命令需要的值, 这种方法的一个缺点是,如果远程服务器集群中的环境变量不一致,就不能用了,但这种情况,应该比较少把……</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># 这样,就可以把目标主机的环境变量传递给 get_path 变量, 可以在后续的 task 中使用</span><br><span class="line">- name: 获取服务器的环境变量</span><br><span class="line">  shell: java --version</span><br><span class="line">  environment:</span><br><span class="line">    PATH: &#x2F;usr&#x2F;local&#x2F;java&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理工具 supervisor</title>
      <link href="/2017-09-25-supervisor-intro.html"/>
      <url>/2017-09-25-supervisor-intro.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Supervisor是一个Linux下用Python开发的进程管理工具，提供了web管理界面，通过配置需要监控的进程，可以很方便的监控并管理进程，更厉害的是，当监控的进程因为各种原因断开的时候，能自动重启该进程。 </p></blockquote><a id="more"></a><p>在使用 docker 时, 苦于 docker 启动容器时都是只启动一个后台服务,所以如果需要运行多个服务,就不得不启动容器后登录到容器中,并手动启动需要的服务.但是,借助 supervisor 管理服务,可以在容器启动时,启动 supervisor , 借由 supervisor 启动所需要的所有服z务 .它同时具有监控进程的功能,当进程意外挂掉,它可以自动启动该进程</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>supervisor 是用 python 开发的,所以可以用 <code>pip install supervisor</code> 安装  </p><p>Ubuntu 中也可以使用 <code>apt-get install supervisor</code> 安装</p><p>Centos 中也可以使用 <code>yum -y install supervisor</code> 安装</p><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><h5 id="查看并重定向默认配置项"><a href="#查看并重定向默认配置项" class="headerlink" title="查看并重定向默认配置项"></a>查看并重定向默认配置项</h5><p>执行 <code>echo_supervisord_conf</code> 可以查看到默认的配置项<br>执行 <code>echo_supervisord_conf &gt; /etc/supervisord.conf</code> 把配置重定向到 <code>/etc/supervisord.conf</code> 中</p><h5 id="部分配置项说明"><a href="#部分配置项说明" class="headerlink" title="部分配置项说明"></a>部分配置项说明</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[unix_http_server]</span><br><span class="line">file&#x3D;&#x2F;tmp&#x2F;supervisor.sock   ; UNIX socket 文件，supervisorctl 会使用</span><br><span class="line">;chmod&#x3D;0700                 ; socket 文件的 mode，默认是 0700</span><br><span class="line">;chown&#x3D;nobody:nogroup       ; socket 文件的 owner，格式： uid:gid</span><br><span class="line"> </span><br><span class="line">;[inet_http_server]         ; HTTP 服务器，提供 web 管理界面</span><br><span class="line">;port&#x3D;127.0.0.1:9001        ; Web 管理后台运行的 IP 和端口，如果开放到公网，需要注意安全性</span><br><span class="line">;username&#x3D;supervisor_admin     ; 登录管理后台的用户名</span><br><span class="line">;password&#x3D;000000               ; 登录管理后台的密码</span><br><span class="line"> </span><br><span class="line">[supervisord]</span><br><span class="line">logfile&#x3D;&#x2F;tmp&#x2F;supervisord.log ; 日志文件，默认是 $CWD&#x2F;supervisord.log</span><br><span class="line">logfile_maxbytes&#x3D;50MB        ; 日志文件大小，超出会 rotate，默认 50MB</span><br><span class="line">logfile_backups&#x3D;10           ; 日志文件保留备份数量默认 10</span><br><span class="line">loglevel&#x3D;info                ; 日志级别，默认 info，其它: debug,warn,trace</span><br><span class="line">pidfile&#x3D;&#x2F;tmp&#x2F;supervisord.pid ; pid 文件</span><br><span class="line">nodaemon&#x3D;false               ; 是否在前台启动，默认是 false，即以 daemon 的方式启动</span><br><span class="line">minfds&#x3D;1024                  ; 可以打开的文件描述符的最小值，默认 1024</span><br><span class="line">minprocs&#x3D;200                 ; 可以打开的进程数的最小值，默认 200</span><br><span class="line"> </span><br><span class="line">; the below section must remain in the config file for RPC</span><br><span class="line">; (supervisorctl&#x2F;web interface) to work, additional interfaces may be</span><br><span class="line">; added by defining them in separate rpcinterface: sections</span><br><span class="line">[rpcinterface:supervisor]</span><br><span class="line">supervisor.rpcinterface_factory &#x3D; supervisor.rpcinterface:make_main_rpcinterface</span><br><span class="line"> </span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl&#x3D;unix:&#x2F;&#x2F;&#x2F;tmp&#x2F;supervisor.sock ; 通过 UNIX socket 连接 supervisord，路径与 unix_http_server 部分的 file 一致</span><br><span class="line">;serverurl&#x3D;http:&#x2F;&#x2F;127.0.0.1:9001 ; 通过 HTTP 的方式连接 supervisord</span><br><span class="line"> </span><br><span class="line">; 包含其他的配置文件</span><br><span class="line">[include]</span><br><span class="line">files &#x3D; relative&#x2F;directory&#x2F;*.ini    ; 可以是 *.conf 或 *.ini</span><br></pre></td></tr></table></figure><h5 id="配置-program"><a href="#配置-program" class="headerlink" title="配置 program"></a>配置 program</h5><p>这个配置,就是管理真正要执行和监控的进程,一般不建议 写到 supervisord.conf 中, 而是使用 <code>include</code> 的方式引入其他不同的服务配置  </p><h6 id="配置-include"><a href="#配置-include" class="headerlink" title="配置 include"></a>配置 include</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 创建文件</span><br><span class="line">    mkdir &#x2F;etc&#x2F;supervisor</span><br><span class="line"># 修改 &#x2F;etc&#x2F;supervisord.conf</span><br><span class="line">    [include]</span><br><span class="line">    files &#x3D; &#x2F;etc&#x2F;supervisor&#x2F;*.conf</span><br><span class="line"># 在 &#x2F;etc&#x2F;supervisor&#x2F; 文件夹下创建配置文件, 以 sshd 服务为例 vi &#x2F;etc&#x2F;supervisor&#x2F;sshd.conf</span><br><span class="line">    [program:sshd]</span><br><span class="line">    directory &#x3D; &#x2F;usr&#x2F;sbin ; 程序的启动目录</span><br><span class="line">    command &#x3D; sshd -D ; 启动命令，可以看出与手动在命令行启动的命令是一样的</span><br><span class="line">    autostart &#x3D; true     ; 在 supervisord 启动的时候也自动启动</span><br><span class="line">    startsecs &#x3D; 5        ; 启动 5 秒后没有异常退出，就当作已经正常启动了</span><br><span class="line">    autorestart &#x3D; true   ; 程序异常退出后自动重启</span><br><span class="line">    startretries &#x3D; 3     ; 启动失败自动重试次数，默认是 3</span><br><span class="line">    user &#x3D; root          ; 用哪个用户启动</span><br><span class="line">    redirect_stderr &#x3D; true  ; 把 stderr 重定向到 stdout，默认 false</span><br><span class="line">    stdout_logfile_maxbytes &#x3D; 20MB  ; stdout 日志文件大小，默认 50MB</span><br><span class="line">    stdout_logfile_backups &#x3D; 20     ; stdout 日志文件备份数</span><br><span class="line">    ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）</span><br><span class="line">    stdout_logfile &#x3D; &#x2F;data&#x2F;logs&#x2F;usercenter_stdout.log</span><br><span class="line"> </span><br><span class="line">    ; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH</span><br><span class="line">    ; environment&#x3D;PYTHONPATH&#x3D;$PYTHONPATH:&#x2F;path&#x2F;to&#x2F;somewhere</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>运行 <code>supervisord</code> 命令就可以启动  </p><p>如果配置了 <code>[inet_http_server]</code> 还可以访问 <code>127.0.0.1:9001</code>, 输入自己配置的用户名和密码就可以在web端监控,管理服务了.</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文件描述符（File Descriptor）简介</title>
      <link href="/2017-09-21-file-descriptor.html"/>
      <url>/2017-09-21-file-descriptor.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://zh.wikipedia.org/wiki/%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6" target="_blank" rel="noopener">维基百科</a>:文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开.  </p></blockquote><a id="more"></a><h3 id="文件描述符概念"><a href="#文件描述符概念" class="headerlink" title="文件描述符概念"></a>文件描述符概念</h3><p>　　Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I/O操作的系统调用都会通过文件描述符。</p><h3 id="文件描述符、文件、进程间的关系"><a href="#文件描述符、文件、进程间的关系" class="headerlink" title="文件描述符、文件、进程间的关系"></a>文件描述符、文件、进程间的关系</h3><h4 id="描述："><a href="#描述：" class="headerlink" title="描述："></a>描述：</h4><ul><li>每个文件描述符会与一个打开的文件相对应</li><li>不同的文件描述符也可能指向同一个文件</li><li>相同的文件可以被不同的进程打开，也可以在同一个进程被多次打开  </li></ul><h4 id="系统为维护文件描述符，建立了三个表"><a href="#系统为维护文件描述符，建立了三个表" class="headerlink" title="系统为维护文件描述符，建立了三个表"></a>系统为维护文件描述符，建立了三个表</h4><ul><li><p>进程级的文件描述符表</p></li><li><p>系统级的文件描述符表</p></li><li><p>文件系统的i-node表 (<a href="http://www.ruanyifeng.com/blog/2011/12/inode.html" target="_blank" rel="noopener">转到：阮一峰——理解inode</a>)  </p></li></ul><p><img src="/images/file-descripter/1.png" alt="image"></p><h4 id="通过这三个表，认识文件描述符"><a href="#通过这三个表，认识文件描述符" class="headerlink" title="通过这三个表，认识文件描述符"></a>通过这三个表，认识文件描述符</h4><p><img src="/images/file-descripter/2.png" alt="image"></p><ul><li>在进程A中，文件描述符1和30都指向了同一个打开的文件句柄（#23），这可能是该进程多次对执行<code>打开</code>操作</li><li>进程A中的文件描述符2和进程B的文件描述符2都指向了同一个打开的文件句柄（#73），这种情况有几种可能，1.进程A和进程B可能是父子进程关系;2.进程A和进程B打开了同一个文件，且文件描述符相同（低概率事件=_=）；3.A、B中某个进程通过UNIX域套接字将一个打开的文件描述符传递给另一个进程。</li><li>进程A的描述符0和进程B的描述符3分别指向不同的打开文件句柄，但这些句柄均指向i-node表的相同条目（#1936），换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了打开请求。同一个进程两次打开同一个文件，也会发生类似情况。</li></ul><p>前人的思考，我们的阶梯，这部分参考自网络：<a href="http://blog.csdn.net/cywosp/article/details/38965239" target="_blank" rel="noopener">链接</a></p><h3 id="文件描述符限制"><a href="#文件描述符限制" class="headerlink" title="文件描述符限制"></a>文件描述符限制</h3><p>　　有资源的地方就有战争，“文件描述符”也是一种资源，系统中的每个进程都需要有“文件描述符”才能进行改变世界的宏图霸业。世界需要秩序，于是就有了“文件描述符限制”的规定。  </p><h4 id="如下表："><a href="#如下表：" class="headerlink" title="如下表："></a>如下表：</h4><p><img src="/images/file-descripter/3.png" alt="image"></p><p>永久修改用户级限制时有三种设置类型：</p><ol><li><code>soft</code> 指的是当前系统生效的设置值</li><li><code>hard</code> 指的是系统中所能设定的最大值</li><li><code>-</code>  指的是同时设置了 soft 和 hard 的值  </li></ol><p>命令讲解:</p><ul><li><a href="http://man.linuxde.net/ulimit" target="_blank" rel="noopener">ulimit</a></li><li><a href="http://man.linuxde.net/sysctl" target="_blank" rel="noopener">sysctl</a>  </li></ul><h3 id="检查某个进程的文件描述符相关内容"><a href="#检查某个进程的文件描述符相关内容" class="headerlink" title="检查某个进程的文件描述符相关内容"></a>检查某个进程的文件描述符相关内容</h3><p>步骤(以nginx为例，*注意权限问题，此示例是在本地环境)：</p><ol><li>找到需要检查的进程id</li></ol><p><img src="/images/file-descripter/4.png" alt="image"><br>如图，找到的进程id为 1367  </p><ol start="2"><li>查看该进程的限制</li></ol><p><img src="/images/file-descripter/5.png" alt="image"></p><p>如图，在 Max open files 那一行，可以看到当前设置中最大文件描述符的数量为1024</p><ol start="3"><li>查看该进程占用了多少个文件描述符</li></ol><p><img src="/images/file-descripter/6.png" alt="image"></p><p>如图所示，使用了17个文件描述符</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>　　实际应用过程中，如果出现“Too many open files” , 可以通过增大进程可用的文件描述符数量来解决，但往往故事不会这样结束，很多时候，并不是因为进程可用的文件描述符过少，而是因为程序bug，打开了大量的文件连接（web连接也会占用文件描述符）而没有释放。程序申请的资源在用完后及时释放，才是解决“Too many open files”的根本之道。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 主从复制</title>
      <link href="/2017-09-13-mysql-replication.html"/>
      <url>/2017-09-13-mysql-replication.html</url>
      
        <content type="html"><![CDATA[<p>MySQL 复制原理及操作</p><a id="more"></a><h4 id="数据库复制-replication-实现原理"><a href="#数据库复制-replication-实现原理" class="headerlink" title="数据库复制 replication 实现原理"></a>数据库复制 replication 实现原理</h4><p>mysql 的主从复制功能是基于 binlog 操作日志的，其过程如下：</p><ol><li>从数据库执行 start salve ，开启主从复制开关</li><li>从数据库的 io 线程会使用主数据上授权的用户请求连接主数据库，并请求指定的 binlog 日志。</li><li>主数据接收到来自从数据库的 io 线程的请求后，主数据库上负责复制的 io 线程根据从数据库的请求信息，读取指定的 binlog 文件的指定位置，返回给从数据库的io线程，返回的信息除了本次请求的日志内容外，还是有本次返回的日志内容后在主数据上新的 binlog 文件名称及在 binlog 中的位置（供从数据库下次请求 binlog 使用）。</li><li>从数据的 io 线程获取到来自主数据上的 io 线程发送的 binlog 后，将 binlog 中的内容依次写入到从数据库自身的 relaylog（中继日志）文件（Mysql-info-realy-bin.XXXX) 的最末端，并将新的 binlog 文件名和位置记录到 Master-info 文件中，以便下一次读取主数据库的新 binlog 日志时，能够告诉 master 服务器需要从新 binlog 日志的那个文件那个位置，开始返回给从数据库。</li><li>从数据库的 sql 线程会实时的检测本地 relay log 中新增的日志内容，然后及时把 log 文件中的内容解析成在主数据库曾经执行的 sql 语句，并在自身从数据库上按顺序执行这些 sql 语句。</li><li>至此，正常情况下，主从数据库，就可以实现同步。</li></ol><h4 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h4><p><img src="/images/mysql-replication/1.png" alt="流程图"></p><h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><img src="/images/mysql-replication/2.png" alt="流程图"></p><h4 id="MySql常用相关指令"><a href="#MySql常用相关指令" class="headerlink" title="MySql常用相关指令"></a>MySql常用相关指令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">show master status;     &#x2F;&#x2F;查看master的状态，尤其是当前的日志及位置</span><br><span class="line">show slave status;     &#x2F;&#x2F;查看slave的状态</span><br><span class="line">reset slave;            &#x2F;&#x2F;重置slave状态</span><br><span class="line">start slave;            &#x2F;&#x2F;启动slave状态（开启监听master的变化）</span><br><span class="line">stop slave;             &#x2F;&#x2F;暂停salve状态</span><br></pre></td></tr></table></figure><h4 id="主从复制配置步骤"><a href="#主从复制配置步骤" class="headerlink" title="主从复制配置步骤"></a>主从复制配置步骤</h4><h5 id="服务器说明"><a href="#服务器说明" class="headerlink" title="服务器说明"></a>服务器说明</h5><table><thead><tr><th>master</th><th>172.17.0.2</th></tr></thead><tbody><tr><td>slave1</td><td>172.17.0.3</td></tr><tr><td>slave2</td><td>172.17.0.4</td></tr></tbody></table><h5 id="配置主服务器"><a href="#配置主服务器" class="headerlink" title="配置主服务器"></a>配置主服务器</h5><h6 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; create user &#39;zj-replication&#39;@&#39;172.17.0.%&#39; identified by &#39;000000&#39;;</span><br><span class="line">Query OK, 0 rows affected (0.08 sec)</span><br></pre></td></tr></table></figure><h6 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; grant replication slave on *.* to &#39;zj-replication&#39;@&#39;172.17.0.%&#39; identified by &#39;000000&#39;;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><h6 id="开启-binlog-日志"><a href="#开启-binlog-日志" class="headerlink" title="开启 binlog 日志"></a>开启 binlog 日志</h6><p>修改 mysql 配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 开启二进制日志</span><br><span class="line">log-bin&#x3D;&#x2F;var&#x2F;mysql&#x2F;mysql-bin</span><br><span class="line"># 给服务器起一个唯一的id</span><br><span class="line">server-id&#x3D;1</span><br></pre></td></tr></table></figure><p>修改完成后,重启MySQL服务器,用客户端登录,查看 binlog状态 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000001 |     154 |              |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="配置从服务器"><a href="#配置从服务器" class="headerlink" title="配置从服务器"></a>配置从服务器</h5><h6 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h6><p>修改配置msyql配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># server id 不能和其他mysql服务器重复</span><br><span class="line">server-id&#x3D;2</span><br><span class="line"># 定义relay_log的位置和名称，如果值为空，则默认位置在数据文件的目录，文件名为host_name-relay-bin.nnnnnn</span><br><span class="line">relay-log&#x3D;slave-relay-bin</span><br></pre></td></tr></table></figure><h6 id="连接-master"><a href="#连接-master" class="headerlink" title="连接 master"></a>连接 master</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; change master to </span><br><span class="line">    -&gt; master_host&#x3D;&#39;172.17.0.2&#39;,</span><br><span class="line">    -&gt; master_port&#x3D;3306,</span><br><span class="line">    -&gt; master_user&#x3D;&#39;zj-replication&#39;,</span><br><span class="line">    -&gt; master_password&#x3D;&#39;000000&#39;,</span><br><span class="line">    -&gt; master_log_file&#x3D;&#39;master-bin.000001&#39;,                      </span><br><span class="line">    -&gt; master_log_pos&#x3D;154;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.02 sec)</span><br></pre></td></tr></table></figure><h6 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start slave;</span><br></pre></td></tr></table></figure><h6 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 172.17.0.2</span><br><span class="line">                  Master_User: zj-replication</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000001</span><br><span class="line">          Read_Master_Log_Pos: 154</span><br><span class="line">               Relay_Log_File: slave-relay-bin.000001</span><br><span class="line">                Relay_Log_Pos: 0</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000001</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">              Replicate_Do_DB: </span><br><span class="line">          Replicate_Ignore_DB: </span><br><span class="line">           Replicate_Do_Table: </span><br><span class="line">       Replicate_Ignore_Table: </span><br><span class="line">      Replicate_Wild_Do_Table: </span><br><span class="line">  Replicate_Wild_Ignore_Table: </span><br><span class="line">                   Last_Errno: 0</span><br><span class="line">                   Last_Error: </span><br><span class="line">                 Skip_Counter: 0</span><br><span class="line">          Exec_Master_Log_Pos: 154</span><br><span class="line">              Relay_Log_Space: 527</span><br><span class="line">              Until_Condition: None</span><br><span class="line">               Until_Log_File: </span><br><span class="line">                Until_Log_Pos: 0</span><br><span class="line">           Master_SSL_Allowed: No</span><br><span class="line">           Master_SSL_CA_File: </span><br><span class="line">           Master_SSL_CA_Path: </span><br><span class="line">              Master_SSL_Cert: </span><br><span class="line">            Master_SSL_Cipher: </span><br><span class="line">               Master_SSL_Key: </span><br><span class="line">        Seconds_Behind_Master: 0</span><br><span class="line">Master_SSL_Verify_Server_Cert: No</span><br><span class="line">                Last_IO_Errno: 0</span><br><span class="line">                Last_IO_Error: </span><br><span class="line">               Last_SQL_Errno: 0</span><br><span class="line">               Last_SQL_Error: </span><br><span class="line">  Replicate_Ignore_Server_Ids: </span><br><span class="line">             Master_Server_Id: 1</span><br><span class="line">                  Master_UUID: 034b7e45-4c16-11e8-9386-0242ac110002</span><br><span class="line">             Master_Info_File: &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;master.info</span><br><span class="line">                    SQL_Delay: 0</span><br><span class="line">          SQL_Remaining_Delay: NULL</span><br><span class="line">      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates</span><br><span class="line">           Master_Retry_Count: 86400</span><br><span class="line">                  Master_Bind: </span><br><span class="line">      Last_IO_Error_Timestamp: </span><br><span class="line">     Last_SQL_Error_Timestamp: </span><br><span class="line">               Master_SSL_Crl: </span><br><span class="line">           Master_SSL_Crlpath: </span><br><span class="line">           Retrieved_Gtid_Set: </span><br><span class="line">            Executed_Gtid_Set: </span><br><span class="line">                Auto_Position: 0</span><br><span class="line">         Replicate_Rewrite_DB: </span><br><span class="line">                 Channel_Name: </span><br><span class="line">           Master_TLS_Version: </span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>要保证 Slave_IO_Running 和 Slave_SQL_Running 的值都是 Yes 才行.</p><h4 id="主主复制"><a href="#主主复制" class="headerlink" title="主主复制"></a>主主复制</h4><p>mysql 的主主复制就是两台 mysql 节点互为主从。搭建起来 mysql 主从,再来搭建主主复<br>制就非常简单了。<br>在原来主从的基础上做如下操作即可:</p><ul><li>1.开启原从节点的 binlog 日志</li><li>2.原从节点创建读取副本的用户</li><li>3.在原主节点中让 master 指向从节点</li><li>4.在原主节点执行 start slave 命令</li></ul><p>在MySQL5.7以前,对于主主复制,如果向两个节点中插入数据,一定会导致主键的重复,这里需要一个小的技巧让第一台节点主键采用 1,3,5,7…的方式自增第二台节点采用 2,4,6,8…. 的方式递增  </p><p><strong>修改配置文件:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 服务器172.17.0.2：</span><br><span class="line">auto_increment_increment &#x3D; 2;</span><br><span class="line">auto_increment_offset &#x3D; 1; </span><br><span class="line"></span><br><span class="line"># 服务器172.17.0.3：</span><br><span class="line">auto_increment_increment &#x3D; 2;</span><br><span class="line">auto_increment_offset &#x3D; 2;</span><br></pre></td></tr></table></figure><p>这种方法有一个弊端:当需要加入新的服务器时，这种方法难以扩展。</p><p>也可以在新增行时,计算好id.比如oracle数据库的主键id，可以使用序列sequence，每次新增生成递增的id。 mysql可以使用redis设置一个自增id,每次新增取出id。  </p><p>但这一问题,在5.7版本得到了解决,不需要配置主键自增策略,每个库的操作都会让另外一个库递增.</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySql 表分区</title>
      <link href="/2017-09-05-mysql-partition.html"/>
      <url>/2017-09-05-mysql-partition.html</url>
      
        <content type="html"><![CDATA[<p>MySQL 表分区和分库分表一样,都是为了提高数据库的吞吐量。分区类似于分表,分表是逻辑上将一个大数据量的表分成多个,可以是水平分也可以是垂直分。而分区是将表的一个数据文件拆分成多个,不同的数据拆分到不同的文件中。这样对于一个数据量非常大的表,有多个数据文件来进行存储,这样就提高了数据库的 io 性能</p><a id="more"></a><h4 id="分区的优点"><a href="#分区的优点" class="headerlink" title="分区的优点"></a>分区的优点</h4><ul><li>和单个磁盘或文件系统分区相比，可以存储更多的数据。  </li><li>优化查询。  <ul><li>where 子句中包含分区条件时，可以只扫描必要的分区。</li><li>涉及聚合函数的查询时，可以容易的在每个分区上并行处理，最终只需汇总得到结果。</li></ul></li><li>对于已经过期或者不需要保存的数据，可以通过删除与这些数据有关的分区来快速删除数据。</li><li>跨多个磁盘来分散数据查询，以获得更大的查询吞吐量。</li></ul><h4 id="分区方法"><a href="#分区方法" class="headerlink" title="分区方法"></a>分区方法</h4><p>将某张表的数据，分别存储到不同的区域中。每个分区都是独立的表，都要存储该分区数据的数据、索引等信息。<br>使用mysql的分区功能，可以把一个大的数据表分成多个小份，用户不需要区分不同的表名。<br>表中有主键的时候，分区只能使用主键</p><h5 id="KEY-分区，按照某个字段取余"><a href="#KEY-分区，按照某个字段取余" class="headerlink" title="KEY 分区，按照某个字段取余"></a>KEY 分区，按照某个字段取余</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table post (</span><br><span class="line">    id int unsigned not null AUTO_INCREMENT,</span><br><span class="line">    title varchar(255),</span><br><span class="line">    PRIMARY KEY (id)</span><br><span class="line">) engine &#x3D; innodb</span><br><span class="line">partition by key (id) partitions 5;</span><br></pre></td></tr></table></figure><h5 id="HASH-分区，基于给定的分区个数，把数据分配到不同的分区。"><a href="#HASH-分区，基于给定的分区个数，把数据分配到不同的分区。" class="headerlink" title="HASH 分区，基于给定的分区个数，把数据分配到不同的分区。"></a>HASH 分区，基于给定的分区个数，把数据分配到不同的分区。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table student_hash(</span><br><span class="line">    id int unsigned not null auto_increment,</span><br><span class="line">    birthday date,</span><br><span class="line">    PRIMARY KEY(id,birthday);</span><br><span class="line">) engine&#x3D;myisam</span><br><span class="line">partition by hash (month(birthday)) patitions 12;</span><br></pre></td></tr></table></figure><p>key 和 hash 分区方法可以有效的分散热点数据。</p><h5 id="RANGE-分区，基于一个给定连续区间范围，把数据分配到不同的分区"><a href="#RANGE-分区，基于一个给定连续区间范围，把数据分配到不同的分区" class="headerlink" title="RANGE 分区，基于一个给定连续区间范围，把数据分配到不同的分区"></a>RANGE 分区，基于一个给定连续区间范围，把数据分配到不同的分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table goods (</span><br><span class="line"> id int,</span><br><span class="line"> uname char(10)</span><br><span class="line"> )engine myisam</span><br><span class="line"> partition by range(id) (</span><br><span class="line"> partition p1 values less than (10),</span><br><span class="line"> partition p2 values less than (20),</span><br><span class="line"> partition p3 values less than MAXVALUE</span><br><span class="line"> );</span><br></pre></td></tr></table></figure><h5 id="LIST-分区，类似-RANGE-分区，区别在-LIST-分区是基于枚举出的值列表分区，RANGE-是局域给定的连续区间范围分区。"><a href="#LIST-分区，类似-RANGE-分区，区别在-LIST-分区是基于枚举出的值列表分区，RANGE-是局域给定的连续区间范围分区。" class="headerlink" title="LIST 分区，类似 RANGE 分区，区别在 LIST 分区是基于枚举出的值列表分区，RANGE 是局域给定的连续区间范围分区。"></a>LIST 分区，类似 RANGE 分区，区别在 LIST 分区是基于枚举出的值列表分区，RANGE 是局域给定的连续区间范围分区。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table user (</span><br><span class="line">uid int,</span><br><span class="line">pid int,</span><br><span class="line">uname </span><br><span class="line">)engine myisam</span><br><span class="line">partition by list(pid) (</span><br><span class="line">partition bj values in (1),</span><br><span class="line">partition ah values in (2),</span><br><span class="line">partition xb values in (4,5,6)</span><br><span class="line">);</span><br><span class="line"># 如果试图插入的列值不包含分区值列表中时，那么 insert 操作会失败并报错，要重点注意的是，list 分区不存在类似 values less than maxvalue 这样包含其他值在内的定义方式，将要匹配的任何值都必须在值列表中找得到。</span><br></pre></td></tr></table></figure><h5 id="Clumns-分区"><a href="#Clumns-分区" class="headerlink" title="Clumns 分区"></a>Clumns 分区</h5><p>是在mysql5.5引入的分区类型，解决了之前版本 range 和 list 分区只支持整数分区，从而导致需要额外的函数计算得到整数或通过额外的转换表来转换为整数再分区的问题。<br>Columns 分区可以细分为 range columns 分区和 list columns 分区，这两种分区都支持整数，日期和字符串三大数据类型。<br>columns 分区的另一个亮点是支持多列分区：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table rc3(</span><br><span class="line">    a int,</span><br><span class="line">    b int</span><br><span class="line">)</span><br><span class="line">partition by range columns(a,b)(</span><br><span class="line">    partition p01 values less than (0,10),</span><br><span class="line">    partition p02 values less than (10,10),</span><br><span class="line">    partition p03 values less than (10,20),</span><br><span class="line">    partition p04 values less than (10,35),</span><br><span class="line">    partition p05 values less than (10,maxvalue),</span><br><span class="line">    partition p06 values less than (maxvalue,maxvalue)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h5 id="子分区"><a href="#子分区" class="headerlink" title="子分区"></a>子分区</h5><p>子分区(subpartitioning)是分区表中对每个分区的再次分割，又被称为符合分区（composite partitioning）。mysql 从 mysql 5.1 开始支持对已经通过 range 或者 list 分区了的表再进行子分区，子分区既可以使用 hash 分区，也可以使用 key 分区。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table ts (id int, purchased date)</span><br><span class="line">    partition by range(year(purchased))</span><br><span class="line">    subpartition by hash (to_days(purchased))</span><br><span class="line">    subpartitions 2</span><br><span class="line">    (</span><br><span class="line">        partition p0 values less than (1990),</span><br><span class="line">        partition p1 values less than (2000),</span><br><span class="line">        partition p2 values less than maxvalue,</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><p>表 ts 有3个 range 分区，这 3 个分区中的每个分区又进一步分成 2 个子分区，实际上，整个表被分成了 3*2=6 个分区，由于 partition by range 子句的作用，第一和第二个分区只保存 purchased 列中值小于 1990 的记录。<br>复合分区适用于保存非常大量的数据记录。  </p><h4 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h4><h5 id="取余算法-key-hash"><a href="#取余算法-key-hash" class="headerlink" title="取余算法 key hash"></a>取余算法 key hash</h5><p><strong>采用取余算法的分区数量的修改，不会导致已有分区数据的丢失，需要重新分配数据到新的分区</strong><br>增加分区： add partition N；<br>减少分区：coalesce partition N；</p><h5 id="条件算法-list-range"><a href="#条件算法-list-range" class="headerlink" title="条件算法 list range"></a>条件算法 list range</h5><p>添加分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alert table goods add partition(</span><br><span class="line">partition p4 values less than 40);</span><br></pre></td></tr></table></figure><p>删除分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alert table goods drop partition p1;</span><br><span class="line">注意：删除条件算法的分区，会导致分区数据的丢失</span><br></pre></td></tr></table></figure><h4 id="MySql-分区处理-null-值的方式"><a href="#MySql-分区处理-null-值的方式" class="headerlink" title="MySql 分区处理 null 值的方式"></a>MySql 分区处理 null 值的方式</h4><p>mysql 不禁止在分区键值上使用 null ， 分区键可能是一个字段或者一个用户定义的额表达式。一般情况下，mysql 的分区把 null 当作零值，或者一个最小值进行处理。<br>range 分区中，null 值会被当作最小值来处理；<br>list 分区中，null 值必须出现在枚举列表中，否则不被接受；<br>hash/key 分区中，null 值会被当作零值来处理。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 备份和恢复机制</title>
      <link href="/2017-08-29-mysql-backup-restore.html"/>
      <url>/2017-08-29-mysql-backup-restore.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>备份和恢复对于数据库来说是相当重要和常见的工作。数据库在使用过程中难免会出现一些意外情况，比如系统崩溃、硬件故障或误操作，这时，如果提前进行了数据备份，就能很方便的进行恢复，使损失到最小</p></blockquote><a id="more"></a><h3 id="备份恢复策略"><a href="#备份恢复策略" class="headerlink" title="备份恢复策略"></a>备份恢复策略</h3><p>进行备份或恢复操作时需要考虑一些因素：  </p><ul><li>确定要备份的表的存储引擎是事务型还是非事务型，两种不同的存储引擎备份方式在处理数据一致性方面是不太一样的。  </li><li>确定使用全备份还是增量备份。全备份的优点是备份保持最新备份，恢复的时候可以花费更少的时间；缺点是如果数据量大，将会花费很多的时间，并对系统造成较长时间的压力。增量备份相反，只需要备份每天的增量日志，备份时间少，对负载压力也小；缺点就是恢复的时候需要全备份加上次备份到故障前的所有日志，恢复时间长一些。  </li><li>可以考虑采用复制的方法来做异地备份，但不能代替备份，它对数据库的误操作也无能为力。  </li><li>要定期做备份，备份的周期要充分考虑系统可以承受的恢复时间。备份要在系统负载较小的时候进行  </li><li>确保 MySQL 打开 log-bin 选项，有了 binlog，MySQL 才可以在必要的时候做完整恢复，或基于时间点的恢复，或基于位置的恢复。  </li><li>经常做备份恢复测试，确保备份时有效的，是可以恢复的。  </li></ul><h3 id="逻辑备份和恢复"><a href="#逻辑备份和恢复" class="headerlink" title="逻辑备份和恢复"></a>逻辑备份和恢复</h3><p>在 MySQL 中，逻辑备份的最大优点是对于各种存储引擎都可以用同样的方法来备份；而物理备份则不同，不同的存储引擎有着不同的备份方法，因此，对于不同存储引擎混合的数据库，逻辑备份会简单一点。  </p><h4 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h4><p>MySQL 中的逻辑备份是将数据库中的数据备份为一个文本文件，备份的文件可以被查看和编辑。在 MySQL 中，可以使用 mysqldump 工具来完成逻辑备份：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 备份指定的数据库或者数据库中的某些表  </span><br><span class="line">shell&gt; mysqldump [options] db_name [tables]  </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 备份指定的一个或多个数据库  </span><br><span class="line">shell&gt; mysqldump [options] --database DB1 [DB2,DB3...]  </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 备份所有数据库  </span><br><span class="line">shell&gt; mysqldump [options] --all-database</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 常用参数:</span><br><span class="line">-B: 指定数据库</span><br><span class="line">-F: 刷新日志</span><br><span class="line">-R: 备份存储过程等</span><br><span class="line">-x: 锁表</span><br><span class="line">--master-data: 在备份语句里提价 CHANGE MASTER 语句以及 binlog 文件及位置点信息</span><br></pre></td></tr></table></figure><p>如果没有指定数据库中的任何表，默认导出所有数据库中的所有表。  </p><p>** 注意： ** 为了保证数据备份的一致性，<code>myisam 存储引擎在备份时需要加上 -l 参数</code>,表示将所有表加上读锁，在备份期间，所有表将只能读而不能进行数据更新。但是对于事务存储引擎来说，可以采用更好的选项 –single-transaction，此选项使得 innodb 存储引擎得到一个快照(snapshot)，使得备份的数据能够保证一致性。  </p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><h5 id="备份所有数据库："><a href="#备份所有数据库：" class="headerlink" title="备份所有数据库："></a>备份所有数据库：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqldump -uroot -p --all-database &gt; all.sql</span><br></pre></td></tr></table></figure><h5 id="备份数据库-test"><a href="#备份数据库-test" class="headerlink" title="备份数据库 test"></a>备份数据库 test</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqldump -uroot -p test &gt; test.sql</span><br></pre></td></tr></table></figure><h5 id="备份数据库-test-下的表-emp"><a href="#备份数据库-test-下的表-emp" class="headerlink" title="备份数据库 test 下的表 emp"></a>备份数据库 test 下的表 emp</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqldump -uroot -p test emp &gt; emp.sql</span><br></pre></td></tr></table></figure><h5 id="备份数据库-test-下的表-emp-和-dept"><a href="#备份数据库-test-下的表-emp-和-dept" class="headerlink" title="备份数据库 test 下的表 emp 和 dept"></a>备份数据库 test 下的表 emp 和 dept</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqldump -uroot -p test emp dept &gt; emp_dept.sql</span><br></pre></td></tr></table></figure><h5 id="备份数据库test-下的所有表为逗号分割的文本，备份到-tmp"><a href="#备份数据库test-下的所有表为逗号分割的文本，备份到-tmp" class="headerlink" title="备份数据库test 下的所有表为逗号分割的文本，备份到 /tmp:"></a>备份数据库test 下的所有表为逗号分割的文本，备份到 /tmp:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlddump -uroot -p -T &#x2F;tmp test emp --fields-terminated-by &#39;,&#39;</span><br><span class="line">shell&gt; more emp.txt  </span><br><span class="line"></span><br><span class="line">1,z1</span><br><span class="line">2,z2</span><br><span class="line">3,z3</span><br><span class="line">4,z4</span><br></pre></td></tr></table></figure><h4 id="完全恢复"><a href="#完全恢复" class="headerlink" title="完全恢复"></a>完全恢复</h4><p>mysqldump 的恢复也很简单，将备份作为输入执行即可：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p db_name &lt; backfile</span><br></pre></td></tr></table></figure><p><strong>注意</strong>，将备份恢复后数据并不完整，还需要将备份后执行的日志进行重做：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog binlog-file | mysql -uroot -p</span><br></pre></td></tr></table></figure><h4 id="完整的-mysqldump-备份与恢复示例："><a href="#完整的-mysqldump-备份与恢复示例：" class="headerlink" title="完整的 mysqldump 备份与恢复示例："></a>完整的 mysqldump 备份与恢复示例：</h4><h5 id="凌晨-2-00，备份数据库："><a href="#凌晨-2-00，备份数据库：" class="headerlink" title="凌晨 2:00，备份数据库："></a>凌晨 2:00，备份数据库：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ .&#x2F;mysqldump -uroot -p -l -F t2 &gt; t2.dmp</span><br><span class="line">Enter password:</span><br></pre></td></tr></table></figure><p>其中 <code>-l 参数表示给所有表加读锁，-F 表示生成一个新的日志文件</code>，此时，t2 中 emp 表的数据如下：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 为了便于测试，执行 reset master 删除所有 binlog。</span><br><span class="line">MySQL [(none)]&gt; reset master;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"># 此时只有一个 binlog 日志文件   mysql-bin.000001</span><br><span class="line">MySQL [t2]&gt; select * from test;</span><br><span class="line">+------+------+</span><br><span class="line">| id   | name |</span><br><span class="line">+------+------+</span><br><span class="line">|    1 | a    |</span><br><span class="line">|    2 | b    |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="备份完毕后，插入新的数据："><a href="#备份完毕后，插入新的数据：" class="headerlink" title="备份完毕后，插入新的数据："></a>备份完毕后，插入新的数据：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 因为上一步执行是加入了 -F 选项, 所以接下来的操作会被记录到新的二进制文件，即名为 mysql-bin.000002 的文件</span><br><span class="line">MySQL [t2]&gt; insert into test values (3,&#39;c&#39;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; insert into test values (4,&#39;d&#39;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="数据库突然故障-其实是小伙伴没事儿删库练手玩儿-，数据无法访问。需要恢复备份："><a href="#数据库突然故障-其实是小伙伴没事儿删库练手玩儿-，数据无法访问。需要恢复备份：" class="headerlink" title="数据库突然故障(其实是小伙伴没事儿删库练手玩儿)，数据无法访问。需要恢复备份："></a>数据库突然故障(其实是小伙伴没事儿删库练手玩儿)，数据无法访问。需要恢复备份：</h5><p>删库跑路：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 这里为了便于测试，不把删库操作记入日志，当前 session 设置 sql_log_bin 为 off。</span><br><span class="line"># 删库后，执行 flush logs，让后续的 binlog 到新的文件中，即名为 mysql-bin.000003中</span><br><span class="line">MySQL [t2]&gt; set sql_log_bin &#x3D; 0;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; show variables like &quot;%sql_log_bin%&quot;;</span><br><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| sql_log_bin   | OFF   |</span><br><span class="line">+---------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; drop database t2;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; flush logs;</span><br><span class="line">Query OK, 0 rows affected (0.22 sec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; drop database t2;</span><br><span class="line">Query OK, 3 rows affected (0.23 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; exit;</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><p>数据恢复：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysql -e &quot;create database t2&quot;   </span><br><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysql t2 &lt; t2.dmp </span><br><span class="line"></span><br><span class="line">*******************************************************************</span><br><span class="line">MySQL [t2]&gt; select * from test;</span><br><span class="line">+------+------+</span><br><span class="line">| id   | name |</span><br><span class="line">+------+------+</span><br><span class="line">|    1 | a    |</span><br><span class="line">|    2 | b    |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="使用-mysqlbinlog-恢复自-mysqldump-备份以来的-binglog"><a href="#使用-mysqlbinlog-恢复自-mysqldump-备份以来的-binglog" class="headerlink" title="使用 mysqlbinlog 恢复自 mysqldump 备份以来的 binglog"></a>使用 mysqlbinlog 恢复自 mysqldump 备份以来的 binglog</h5><p>根据前面操作的内容，可知从备份的时间点到删库的时间点之间的操作被记录到了 mysql-bin.000002 文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysqlbinlog --no-defaults &#x2F;data&#x2F;mysql&#x2F;mysql-bin.000002 | .&#x2F;mysql t2</span><br><span class="line"></span><br><span class="line">*******************************************************</span><br><span class="line">MySQL [t2]&gt; select * from test;</span><br><span class="line">+------+------+</span><br><span class="line">| id   | name |</span><br><span class="line">+------+------+</span><br><span class="line">|    1 | a    |</span><br><span class="line">|    2 | b    |</span><br><span class="line">|    3 | c    |</span><br><span class="line">|    4 | d    |</span><br><span class="line">+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>至此，数据恢复成功。  </p><h4 id="基于时间点恢复"><a href="#基于时间点恢复" class="headerlink" title="基于时间点恢复"></a>基于时间点恢复</h4><p>由于误操作，比如误删除了一张表，这时使用完全恢复时没有用的，因为日志里面还存在误操作的语句，我们需要的是恢复到误操作之前的状态，然后跳过误操作语句，再恢复后面执行的语句，完成恢复。这种恢复叫不完全恢复，在 MySQL 中，不完全恢复分为 基于时间点的恢复和基于位置的恢复。<br>基于时间点恢复的操作步骤：  </p><h5 id="如果是上午-10-点发生了误操作，可以用以下语句用备份和-binlog-将数据恢复到故障前："><a href="#如果是上午-10-点发生了误操作，可以用以下语句用备份和-binlog-将数据恢复到故障前：" class="headerlink" title="如果是上午 10 点发生了误操作，可以用以下语句用备份和 binlog 将数据恢复到故障前："></a>如果是上午 10 点发生了误操作，可以用以下语句用备份和 binlog 将数据恢复到故障前：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqlbinlog --stop-date&#x3D;&quot;2017-09-30 9:59:59&quot; &#x2F;data&#x2F;mysql&#x2F;mysql-bin.123456 | mysql -uroot -ppassword</span><br></pre></td></tr></table></figure><h5 id="跳过故障时的时间点，继续执行后面的-binlog，完成恢复。"><a href="#跳过故障时的时间点，继续执行后面的-binlog，完成恢复。" class="headerlink" title="跳过故障时的时间点，继续执行后面的 binlog，完成恢复。"></a>跳过故障时的时间点，继续执行后面的 binlog，完成恢复。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqlbinlog --start-date&#x3D;&quot;2017-09-30 10:01:00&quot; &#x2F;data&#x2F;mysql&#x2F;mysql-bin.123456 | mysql -uroot -ppassword</span><br></pre></td></tr></table></figure><h4 id="基于位置恢复"><a href="#基于位置恢复" class="headerlink" title="基于位置恢复"></a>基于位置恢复</h4><p>和基于时间点的恢复类似，但是更精确，因为同一个时间点可能有很多条 sql 语句同时执行。恢复的操作步骤如下：  </p><h5 id="在-shell-下执行命令："><a href="#在-shell-下执行命令：" class="headerlink" title="在 shell 下执行命令："></a>在 shell 下执行命令：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqlbinlog --start-date&#x3D;&quot;2017-09-30 9:59:59&quot; --stop-date&#x3D;&quot;2017-09-30 10:01:00&quot; &#x2F;data&#x2F;mysql&#x2F;mysql-bin.123456 &gt; &#x2F;tmp&#x2F;mysql_restore.sql</span><br></pre></td></tr></table></figure><p>该命令将在 /tmp 目录创建小的文本文件，编辑此文件，知道出错语句前后的位置号，例如前后位置号分别为 368312 和 368315。  </p><h5 id="恢复了以前的备份文件后，应从命令行输入下面的内容："><a href="#恢复了以前的备份文件后，应从命令行输入下面的内容：" class="headerlink" title="恢复了以前的备份文件后，应从命令行输入下面的内容："></a>恢复了以前的备份文件后，应从命令行输入下面的内容：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;mysqlbinlog --stop-position&#x3D;&quot;368312&quot; &#x2F;data&#x2F;mysql&#x2F;mysql-bin.123456 | mysql -uroot -ppassword  </span><br><span class="line">shell&gt;mysqlbinlog --start-position&#x3D;&quot;368315&quot; &#x2F;data&#x2F;mysql&#x2F;mysql-bin.123456 | mysql -uroot -ppassword</span><br></pre></td></tr></table></figure><p>上面的第一行将恢复到停止位置为止的所有事务。下一行将恢复从给定的起始位置直到二进制日志结束的所有事务。因为 mysqlbinlog 的输出包括每个 sql 语句记录之前的 set timestamp 语句，因此恢复的数据和相关的 mysql 日志将反应事务执行的<code>原时间。</code>  </p><h3 id="物理备份和恢复"><a href="#物理备份和恢复" class="headerlink" title="物理备份和恢复"></a>物理备份和恢复</h3><p>物理备份又分为冷备份和热备份两种，和逻辑备份相比，它的最大优点是备份和恢复的速度更快，因为物理备份的原理都是基于文件的 cp。  </p><h4 id="冷备份"><a href="#冷备份" class="headerlink" title="冷备份"></a>冷备份</h4><p>冷备份其实就是停掉数据库服务，cp 数据文件的方法。(基本不考虑这种方法)  </p><h4 id="热备份"><a href="#热备份" class="headerlink" title="热备份"></a>热备份</h4><p>在 MySQL 中，对于不同的存储引擎热备份的方法也有所不同。  </p><h5 id="myisam-存储引擎"><a href="#myisam-存储引擎" class="headerlink" title="myisam 存储引擎"></a>myisam 存储引擎</h5><p>myisam 存储引擎的热备份有很多方法，本质其实就是将要备份的表加读锁，然后再 cp 数据文件到备份目录。常用的有以下两种方法：  </p><ul><li>使用 mysqlhotcopy 工具  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; mysqlhotcopy 是 MySQL 的一个自带的热备份工具  </span><br><span class="line">shell&gt; mysqlhotcopy db_name [&#x2F;path&#x2F;to&#x2F;new_directory]</span><br></pre></td></tr></table></figure></li><li>手工锁表 copy  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 在 mysqlhotcopy 使用不正常的情况下，可以用手工来做热备份</span><br><span class="line"></span><br><span class="line">mysql&gt;flush tables for read;</span><br><span class="line"></span><br><span class="line">cp 数据文件到备份目录即可，</span><br></pre></td></tr></table></figure></li></ul><h5 id="innodb-存储引擎-另写"><a href="#innodb-存储引擎-另写" class="headerlink" title="innodb 存储引擎(另写)"></a>innodb 存储引擎(另写)</h5><p>使用第三方工具 ibbackup、xtrabackup、innobacupex  </p><h3 id="表的导入导出"><a href="#表的导入导出" class="headerlink" title="表的导入导出"></a>表的导入导出</h3><p>在数据库的日常维护中，表的导入导出时很频繁的一类操作。  </p><h4 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h4><p>在某些情况下，为了一些特定的目的，经常需要将表里的数据导出为某些符号分割的纯数据文本，而不是 sql 语句：  </p><ul><li>用来作为 Excel 显示；</li><li>单纯为了节省备份空间；</li><li>为了快速的加载数据，load data 的加载速度比普通 sql 加载要快 20 倍以上。  </li></ul><h5 id="使用-select-…into-outfile-…-命令来导出数据，具体语法如下："><a href="#使用-select-…into-outfile-…-命令来导出数据，具体语法如下：" class="headerlink" title="使用 select …into outfile … 命令来导出数据，具体语法如下："></a>使用 select …into outfile … 命令来导出数据，具体语法如下：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from tablename into outfile &#39;target_file&#39; [option];</span><br></pre></td></tr></table></figure><p>其中 option 参数可以是以下选项：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fields terminated by &#39;string&#39;                   &#x2F;&#x2F; 字段分隔符，默认为制表符&#39;\t&#39;</span><br><span class="line">fields [optionally] enclosed by &#39;char&#39;          &#x2F;&#x2F; 字段引用符，如果加 optionally 选项则只用在 char、varchar 和 text 等字符型字段上，默认不使用引用符  </span><br><span class="line">fields escaped by ‘char’                        &#x2F;&#x2F; 转移字符、默认为 &#39;\&#39;  </span><br><span class="line">lines starting by &#39;string&#39;                      &#x2F;&#x2F; 每行前都加此字符串，默认&#39;&#39;  </span><br><span class="line">lines terminated by &#39;string&#39;                    &#x2F;&#x2F; 行结束符，默认为&#39;\n&#39;  </span><br><span class="line"></span><br><span class="line"># char 表示此符号只能是单个字符，string表示可以是字符串。</span><br></pre></td></tr></table></figure><p>例如，将 test 表中数据导出为数据文本，其中，字段分隔符为“,”,字段引用符为“””,记录结束符为回车符：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [t2]&gt; select * from test into outfile &#39;&#x2F;data&#x2F;mysql&#x2F;outfile.txt&#39; fields terminated by &quot;,&quot; enclosed by &#39;&quot;&#39;;</span><br><span class="line">Query OK, 4 rows affected (0.02 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ more outfile.txt </span><br><span class="line">&quot;1&quot;,&quot;a&quot;,&quot;helloworld&quot;</span><br><span class="line">&quot;2&quot;,&quot;b&quot;,&quot;helloworld&quot;</span><br><span class="line">&quot;3&quot;,&quot;c&quot;,&quot;helloworld&quot;</span><br><span class="line">&quot;4&quot;,&quot;d&quot;,&quot;helloworld&quot;</span><br></pre></td></tr></table></figure><p>发现第一列是数值型，如果不希望字段两边用引号引起，则语句改为：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MySQL [t2]&gt; select * from test into outfile &#39;&#x2F;data&#x2F;mysql&#x2F;outfile2.txt&#39; fields terminated by &quot;,&quot; optionally  enclosed by &#39;&quot;&#39;;</span><br><span class="line">Query OK, 4 rows affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ more outfile2.txt </span><br><span class="line">1,&quot;a&quot;,&quot;helloworld&quot;</span><br><span class="line">2,&quot;b&quot;,&quot;helloworld&quot;</span><br><span class="line">3,&quot;c&quot;,&quot;helloworld&quot;</span><br><span class="line">4,&quot;d&quot;,&quot;helloworld&quot;</span><br></pre></td></tr></table></figure><p>测试转义字符，MySQL 导出数据中需要转义的字符主要包括以下 3 类：  </p><ul><li>转义字符本身</li><li>字段分隔符</li><li>记录分隔符  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">MySQL [t2]&gt; update test set content &#x3D; &#39;\\&quot;##!aa&#39; where  id&#x3D;1;</span><br><span class="line">Query OK, 1 row affected (0.05 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; select * from test into outfile &#39;&#x2F;data&#x2F;mysql&#x2F;outfile3.txt&#39; fields terminated by &quot;,&quot; optionally enclosed by &#39;&quot;&#39;;</span><br><span class="line">Query OK, 4 rows affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">*******************************************</span><br><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ more outfile3.txt </span><br><span class="line">1,&quot;a&quot;,&quot;\\\&quot;##!aa&quot;</span><br><span class="line">2,&quot;b&quot;,&quot;helloworld&quot;</span><br><span class="line">3,&quot;c&quot;,&quot;helloworld&quot;</span><br><span class="line">4,&quot;d&quot;,&quot;helloworld&quot;</span><br></pre></td></tr></table></figure><ul><li>当导出命令中包含字段引用符时，数据中含有转义字符本身和字段引用符的字符需要被转义；</li><li>当导出命令中不包含字段引用符时，数据中含有转义字符本身和字段分割符的字符需要被转义。  </li></ul><p>** 注意： ** select … into outfile … 产生的输出文件如果在目标目录下有重名文件，将不会被创建成功，源文件不会被自动覆盖。</p><h5 id="使用-mysqldump-导出数据为文本的具体语法如下："><a href="#使用-mysqldump-导出数据为文本的具体语法如下：" class="headerlink" title="使用 mysqldump 导出数据为文本的具体语法如下："></a>使用 mysqldump 导出数据为文本的具体语法如下：</h5><p>mysqldump -u username -T target_dir dbname tablename [option]  </p><p>其中，option 参数可以是以下选项：  </p><ul><li>–fields-terminated-by=name (字段分隔符)；  </li><li>–fields-enclosed-by=name (字段引用符)；</li><li>–fields-optionally-enclosed-by=name (字段引用符，只用在 char、varchar 和 test 等字符型字段上)；  </li><li>–fields-escaped-by=name (转义字符);  </li><li>–lines-terminated-by=name (记录结束符);  </li></ul><p>例子：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin# .&#x2F;mysqldump -uroot -p -T &#x2F;data&#x2F;mysql&#x2F;dump t2 test --fields-terminated-by &#39;,&#39; --fields-optionally-enclosed-by &#39;&quot;&#39;</span><br><span class="line"></span><br><span class="line">**************** test.txt **********************</span><br><span class="line">zj@bogon:&#x2F;data&#x2F;mysql&#x2F;dump$ more test.txt </span><br><span class="line">1,&quot;a&quot;,&quot;\\\&quot;##!aa&quot;</span><br><span class="line">2,&quot;b&quot;,&quot;helloworld&quot;</span><br><span class="line">3,&quot;c&quot;,&quot;helloworld&quot;</span><br><span class="line">4,&quot;d&quot;,&quot;helloworld&quot;</span><br><span class="line"></span><br><span class="line">***************** test.sql *********************</span><br><span class="line">zj@bogon:&#x2F;data&#x2F;mysql&#x2F;dump$ more test.sql </span><br><span class="line">-- MySQL dump 10.13  Distrib 5.7.18, for Linux (x86_64)</span><br><span class="line">--</span><br><span class="line">-- Host: localhost    Database: t2</span><br><span class="line">-- ------------------------------------------------------</span><br><span class="line">-- Server version5.7.18-log</span><br><span class="line"></span><br><span class="line">&#x2F;*!40101 SET @OLD_CHARACTER_SET_CLIENT&#x3D;@@CHARACTER_SET_CLIENT *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_CHARACTER_SET_RESULTS&#x3D;@@CHARACTER_SET_RESULTS *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_COLLATION_CONNECTION&#x3D;@@COLLATION_CONNECTION *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET NAMES utf8mb4 *&#x2F;;</span><br><span class="line">&#x2F;*!40103 SET @OLD_TIME_ZONE&#x3D;@@TIME_ZONE *&#x2F;;</span><br><span class="line">&#x2F;*!40103 SET TIME_ZONE&#x3D;&#39;+00:00&#39; *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET @OLD_SQL_MODE&#x3D;@@SQL_MODE, SQL_MODE&#x3D;&#39;&#39; *&#x2F;;</span><br><span class="line">&#x2F;*!40111 SET @OLD_SQL_NOTES&#x3D;@@SQL_NOTES, SQL_NOTES&#x3D;0 *&#x2F;;</span><br><span class="line"></span><br><span class="line">--</span><br><span class="line">-- Table structure for table &#96;test&#96;</span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">DROP TABLE IF EXISTS &#96;test&#96;;</span><br><span class="line">&#x2F;*!40101 SET @saved_cs_client     &#x3D; @@character_set_client *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; utf8 *&#x2F;;</span><br><span class="line">CREATE TABLE &#96;test&#96; (</span><br><span class="line">  &#96;id&#96; int(11) DEFAULT NULL,</span><br><span class="line">  &#96;name&#96; varchar(10) DEFAULT NULL,</span><br><span class="line">  &#96;content&#96; varchar(100) DEFAULT NULL</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;</span><br><span class="line">&#x2F;*!40101 SET character_set_client &#x3D; @saved_cs_client *&#x2F;;</span><br><span class="line"></span><br><span class="line">&#x2F;*!40103 SET TIME_ZONE&#x3D;@OLD_TIME_ZONE *&#x2F;;</span><br><span class="line"></span><br><span class="line">&#x2F;*!40101 SET SQL_MODE&#x3D;@OLD_SQL_MODE *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET CHARACTER_SET_CLIENT&#x3D;@OLD_CHARACTER_SET_CLIENT *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET CHARACTER_SET_RESULTS&#x3D;@OLD_CHARACTER_SET_RESULTS *&#x2F;;</span><br><span class="line">&#x2F;*!40101 SET COLLATION_CONNECTION&#x3D;@OLD_COLLATION_CONNECTION *&#x2F;;</span><br><span class="line">&#x2F;*!40111 SET SQL_NOTES&#x3D;@OLD_SQL_NOTES *&#x2F;;</span><br><span class="line"></span><br><span class="line">-- Dump completed on 2017-09-25 11:14:06</span><br></pre></td></tr></table></figure><p>可以发现，除多了一个表的创建脚本文件，mysqldump 和 select … into outfile … 的选项和语法非常相似。其实 mysqldump 实际调用的就是后者提供的接口，并在其上面添加了一些新的功能而已。  </p><h4 id="导入-导入用-select-…-into-outfile-或者-mysqldump-导出的纯数据文本"><a href="#导入-导入用-select-…-into-outfile-或者-mysqldump-导出的纯数据文本" class="headerlink" title="导入  (导入用 select … into outfile 或者 mysqldump 导出的纯数据文本)"></a>导入  (导入用 select … into outfile 或者 mysqldump 导出的纯数据文本)</h4><p>和导出类似，导入也有两种不同的方法，分别是 load data infile… 和 mysqlimport，它们的本质是一样的，区别只是在于一个在 MySQL 内部执行，另一个在 MySQL 外部执行。  </p><h5 id="使用-“load-data-infile…”-命令，具体语法如下"><a href="#使用-“load-data-infile…”-命令，具体语法如下" class="headerlink" title="使用 “load data infile…” 命令，具体语法如下"></a>使用 “load data infile…” 命令，具体语法如下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; load data [local]infile &#39;filename&#39; into table tablename [option]</span><br></pre></td></tr></table></figure><p>option 可以是以下选项：  </p><ul><li>fields terminated by ‘string’ (字段分割符，默认为制表符’\t’);</li><li>fields [optionally] enclosed by ‘char’ (字段引用符，如果加 optionally 选项则只用在 char varchar text 等字符型字段上。默认不使用引用符)；</li><li>fields escaped by ‘char’ (转义字符，默认为’&#39;)</li><li>lines starting by ‘string’ (每行前都加此字符串，默认为’’)</li><li>lines terminated by ‘string’ (行结束符，默认为’\n’)</li><li>ignore number lines (忽略输入文件中的前几行数据)  </li><li>(col_name_or_user_var,…) (按照列出的字段顺序和字段数量加载数据)；</li><li>set col_name = expr,…将列做一定的数值转换后再加载。  </li></ul><p>fields 、lines 和前面 select…into outfile…的含义完全相同，不同的是多了几个不同的选项，下面的例子将文件’test.txt’中的数据加载到表 test 中:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 清空表 test  </span><br><span class="line">MySQL [t2]&gt; truncate table test;</span><br><span class="line">Query OK, 0 rows affected (0.07 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; load data infile &#39;&#x2F;data&#x2F;mysql&#x2F;outfile.txt&#39; into table test fields terminated by &#39;,&#39; enclosed by &#39;&quot;&#39;;</span><br><span class="line">Query OK, 4 rows affected (0.10 sec)</span><br><span class="line">Records: 4  Deleted: 0  Skipped: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; select * from test;</span><br><span class="line">+------+------+------------+</span><br><span class="line">| id   | name | content    |</span><br><span class="line">+------+------+------------+</span><br><span class="line">|    1 | a    | helloworld |</span><br><span class="line">|    2 | b    | helloworld |</span><br><span class="line">|    3 | c    | helloworld |</span><br><span class="line">|    4 | d    | helloworld |</span><br><span class="line">+------+------+------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>如果不希望加载文件中的前两行，可以进行如下操作： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [t2]&gt; truncate table test;</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; load data infile &#39;&#x2F;data&#x2F;mysql&#x2F;outfile.txt&#39; into table test fields terminated by &#39;,&#39; enclosed by &#39;&quot;&#39; ignore 2 lines;</span><br><span class="line">Query OK, 2 rows affected (0.00 sec)</span><br><span class="line">Records: 2  Deleted: 0  Skipped: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [t2]&gt; select * from test;</span><br><span class="line">+------+------+------------+</span><br><span class="line">| id   | name | content    |</span><br><span class="line">+------+------+------------+</span><br><span class="line">|    3 | c    | helloworld |</span><br><span class="line">|    4 | d    | helloworld |</span><br><span class="line">+------+------+------------+</span><br><span class="line">2 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure><h4 id="使用-mysqldump-实现"><a href="#使用-mysqldump-实现" class="headerlink" title="使用 mysqldump 实现"></a>使用 mysqldump 实现</h4><p>语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqlimport -uroot -p [--local] dbname order_tab.txt [option]</span><br></pre></td></tr></table></figure><p>其中，option 参数可以是以下选项：  </p><ul><li>–fields-terminated-by=name (字段分隔符)</li><li>–fields-enclosed-by=name (字段引用符)  </li><li>–fields-optionally-enclosed-by=name (字段引用符，只用在 char、varchar、text等字符型字段上)</li><li>–fields-escaped-by=name (转义字符)  </li><li>–lines-terminated-by=name (记录结束符)</li><li>–ignore-lines=number (忽略前几行)  </li></ul><p>** 注意： **<br>如果导入和导出时跨平台操作的（windows 和 linux），那么要注意设置参数 line-terminated-by，windows 上设置为 line-terminated-by=’\r\n’, linux 上设置为 line-terminated-by=’\n’。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 查询优化</title>
      <link href="/2017-08-22-mysql-select-optimization.html"/>
      <url>/2017-08-22-mysql-select-optimization.html</url>
      
        <content type="html"><![CDATA[<p>整理总结一些sql语句的优化技巧</p><a id="more"></a><h4 id="开启缓存"><a href="#开启缓存" class="headerlink" title="开启缓存"></a>开启缓存</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like &#39;query_cache%&#39;;</span><br><span class="line">mysql&gt; set global query_cache_type&#x3D;1;</span><br><span class="line">mysql&gt; set global query_cache_size&#x3D;1024*1024*32</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>查询缓存存在判断是严格依据select语句本身的：严格保证sql一致。</li><li>如果查询时包含动态数据，则不能被缓存。</li><li>如果不想使用缓存，可以使用 SQL_NO_CACHE 语法提示。</li></ol><h4 id="in型子查询"><a href="#in型子查询" class="headerlink" title="in型子查询"></a>in型子查询</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select goods_id,cat_id,goods_name from good where cat_id in(select cat_id form category where parent_id&#x3D;6);</span><br><span class="line">这条语句执行会非常慢，因为它会扫描goods全表，逐行与category表对照</span><br><span class="line">原因：mysql的查询优化器，针对in型做了优化，优化成了exists的执行效果。</span><br><span class="line">改进：用连接查询代替子查询</span><br><span class="line">select goods_id,g.cat_id,g.goods_name from goods as g inner join (select cat_id from category where parent_id&#x3D;6) as t;</span><br></pre></td></tr></table></figure><h4 id="from-子查询"><a href="#from-子查询" class="headerlink" title="from 子查询"></a>from 子查询</h4><p>内层 from 语句查到的临时表，没有索引，所以from返回的内容要尽量少 </p><h4 id="count-优化"><a href="#count-优化" class="headerlink" title="count()优化"></a>count()优化</h4><p>没有查询条件时count(*)非常快，不需要查表。但当有查询条件时，速度将减慢。<br>可以使用缩小范围的方法优化查询。<br>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">需要统计good_id&gt;100的总数时一般会写为：</span><br><span class="line">select count(*) form goods where good_id&gt;100;</span><br><span class="line">优化为：</span><br><span class="line">slect (select count(*) from goods)-(select count(*) from goods where id&lt;100);</span><br><span class="line">这样，就把范围由无限大缩小到了100</span><br></pre></td></tr></table></figure><h4 id="group-by-优化"><a href="#group-by-优化" class="headerlink" title="group by 优化"></a>group by 优化</h4><ul><li>分组用于统计，而不用于筛选数据。 </li><li>用索引避免产生临时表和文件排序</li><li>A,B表连接查询，group by和order by 的列尽量相同，而且列应该为A的列    </li></ul><p>默认情况下，MySQL 对所有 group by col1, col2, …… 的字段进行排序。这与查询中指定 order by col1,col2,…… 类似。因此，如果显示包括一个包含相同列的 order by 子句，则对 MySQL 的实际执行性能没有什么影响。  </p><p>如果查询包括 group by 但用户想要避免排序结果的消耗，则可以指定 order by null 禁止排序。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select col1 from table group by col2 order by null;</span><br></pre></td></tr></table></figure><h4 id="union-优化"><a href="#union-优化" class="headerlink" title="union 优化"></a>union 优化</h4><p>union all 不过滤 效率提高,如非必须,请用union all<br>因为 union去重的代价非常高, 放在程序里去重.</p><h4 id="limit-amp-分页优化"><a href="#limit-amp-分页优化" class="headerlink" title="limit &amp; 分页优化"></a>limit &amp; 分页优化</h4><p>limit offset,n   当offset非常大时，效率极低。mysql并不是跳过offset行，然后单取n行，而是取offset+n行，返回放弃前offset行，返回n行。<br>优化：  </p><ol><li>从业务上解决<br>不允许翻过100页（百度也是如此）</li><li>利用索引<br>select id,name from goods inner join (select id from goods limit 5000000,10) as tmp using(id);  </li><li>记录上一次取出的最后一条数据，把 limit m, n 语句转化为 limit n。 </li></ol><h4 id="消除msyql内部临时表"><a href="#消除msyql内部临时表" class="headerlink" title="消除msyql内部临时表"></a>消除msyql内部临时表</h4><p>在一些sql请求中，mysql会创建临时表，可能创建到内存中，也可能由内存中转存到磁盘。<br>会创建临时表的查询：</p><ol><li>group by 的列没有索引，必创建临时表</li><li>order by 与 group by 为不同列时，或多表联查时order by,group by 包含的列不是第一张表的列，必产生临时表。 </li><li>distinct 与 order by 一起使用可能会产生临时表  </li><li>union合并查询时会用到临时表 </li></ol><h4 id="大批量插入数据"><a href="#大批量插入数据" class="headerlink" title="大批量插入数据"></a>大批量插入数据</h4><h5 id="对于-myisam-引擎"><a href="#对于-myisam-引擎" class="headerlink" title="对于 myisam 引擎"></a>对于 myisam 引擎</h5><p>如果是空的 myisam 表，默认就是先导入数据才创建索引的，不存在优化问题。<br>对于非空的 myisam 表，在一次性插入大量数据时，可以通过设置 disable keys 和 enable keys 来提高导入的效率。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 假设给 test 表一次性插入大量数据  </span><br><span class="line">alert table test disable keys;</span><br><span class="line"></span><br><span class="line">loading the data ……</span><br><span class="line"></span><br><span class="line">alert table test enable keys;</span><br></pre></td></tr></table></figure><h5 id="对于-innodb-引擎"><a href="#对于-innodb-引擎" class="headerlink" title="对于 innodb 引擎"></a>对于 innodb 引擎</h5><p>disable keys 的方式适用于 myisam 引擎，但不适用于 innodb 引擎。  </p><ol><li>因为 innodb 类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率。  </li><li>在导入数据前执行 set unique_checks=0 , 关闭唯一性校验，在导入结束后执行 set unique_checks=1,恢复唯一性校验，可以提高导入的效率。  </li><li>如果应用使用自动提交的方式，建议在导入前执行 set autocommit=0，关闭自动提交，导入结束后再执行 set autocommit=1，打开自动提交，也可以提高导入的效率。  </li></ol><h4 id="优化-insert-语句"><a href="#优化-insert-语句" class="headerlink" title="优化 insert 语句"></a>优化 insert 语句</h4><h5 id="同一客户端一次插入多行"><a href="#同一客户端一次插入多行" class="headerlink" title="同一客户端一次插入多行"></a>同一客户端一次插入多行</h5><p>使用多个值表的 insert 语句,可以减少客户端与数据库之间的连接、关闭等资源消耗</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into test values (1,1),(2,2),(3,3)……</span><br></pre></td></tr></table></figure><h5 id="从不同客户插入很多行，可以使用-insert-delayed-语句得到更高的素的。"><a href="#从不同客户插入很多行，可以使用-insert-delayed-语句得到更高的素的。" class="headerlink" title="从不同客户插入很多行，可以使用 insert delayed 语句得到更高的素的。"></a>从不同客户插入很多行，可以使用 insert delayed 语句得到更高的素的。</h5><p>delayed 的含义是让 insert 语句马上执行，其实数据都被放在内存的队列中，并没有真正的写入磁盘，这比每条土局分别插入要快的多；<br>low_priority 刚好相反，在所有其他用户对表的读写完成后才进行插入（比如记录日志的场景）  </p><h5 id="将索引文件和数据文件分在不同的磁盘上存放（利用建表中的选项）"><a href="#将索引文件和数据文件分在不同的磁盘上存放（利用建表中的选项）" class="headerlink" title="将索引文件和数据文件分在不同的磁盘上存放（利用建表中的选项）"></a>将索引文件和数据文件分在不同的磁盘上存放（利用建表中的选项）</h5><h5 id="如果进行批量插入，可以通过增加-bulk-insert-buffer-size-变量值的方法来提高速度，但是，这只能对-myisam-表使用。"><a href="#如果进行批量插入，可以通过增加-bulk-insert-buffer-size-变量值的方法来提高速度，但是，这只能对-myisam-表使用。" class="headerlink" title="如果进行批量插入，可以通过增加 bulk_insert_buffer_size 变量值的方法来提高速度，但是，这只能对 myisam 表使用。"></a>如果进行批量插入，可以通过增加 bulk_insert_buffer_size 变量值的方法来提高速度，但是，这只能对 myisam 表使用。</h5><h5 id="当从一个文本文件装载一个表时，使用-load-data-infile-。这通常比使用很多-insert-语句快-20-倍。"><a href="#当从一个文本文件装载一个表时，使用-load-data-infile-。这通常比使用很多-insert-语句快-20-倍。" class="headerlink" title="当从一个文本文件装载一个表时，使用 load data infile 。这通常比使用很多 insert 语句快 20 倍。"></a>当从一个文本文件装载一个表时，使用 load data infile 。这通常比使用很多 insert 语句快 20 倍。</h5><h4 id="优化-order-by-语句"><a href="#优化-order-by-语句" class="headerlink" title="优化 order by 语句"></a>优化 order by 语句</h4><h5 id="mysql-的两种排序方式"><a href="#mysql-的两种排序方式" class="headerlink" title="mysql 的两种排序方式"></a>mysql 的两种排序方式</h5><ol><li>通过有序索引顺序扫描直接返回有序数据，这种方式在使用 explain 分析查询时显示为 using index ，不需要额外的排序，操作效率极高。  </li><li>通过对返回的数据行进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫做 filesort 排序。filefort 并不代表通过磁盘文件进行排序，而只是说明进行了一个排序操作，至于排序操作是否使用了磁盘文件或临时表等，则取决于 MySQL 服务器对排序参数的设置和需要排序数据的大小。  </li></ol><ul><li>filesort 是通过相应的排序算法，将取得的数据在 sort_buffer_size 系统变量设置的内存排序区中进行排序，如果内存装载不下，它就会将磁盘上的数据进行分块，再对各个数据块进行排序，然后将各个块合并成有序的结果集。sort_buffer_size 设置的排序区是每个线程独占的，所以同一个时刻，mysql 中存在多个sort buffer 排序区。  </li></ul><h5 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h5><p>尽量减少额外的排序，通过索引直接返回有序数据。<br>where 条件和 order_by 使用相同的索引，并且 order_by 的顺序和索引的顺序相同，并且 order by 的字段都是升序或者降序。否则肯定需要额外的排序操作，这样就会出现 filesort 。  </p><h5 id="filesort-的优化"><a href="#filesort-的优化" class="headerlink" title="filesort 的优化"></a>filesort 的优化</h5><p>在某些不得不使用 filesort 的场景中，需要想办法加快 filesort 的操作。对于 filesort ，MySQL 有两种排序算法。  </p><ul><li>两次扫描算法：  </li></ul><p>首先根据条件取出排序字段和行指针信息，之后在排序区 sort_buffer 中排序。如果排序区 sort buffer 不够，则在临时表 temporary table 汇总存储排序结果，完成排序后根据行指针回表读取记录。这种算法需要两次访问数据，第一次获取排序字段和行指针信息，第二次根据行指针获取记录，尤其是第二次读取操作可能导致大量随机 I/O 操作；优点是排序的时候<code>内存开销较少</code>。  </p><ul><li>一次扫描算法：  </li></ul><p>一次性取出满足条件的行的所有字段，然后在排序区 sort buffer 中排序后直接输出结果集，排序的时候内存开销较大，但是排序效率比两次扫描算法要高。  </p><p>mysql 通过比较系统变量 max_length_for_sort_data 的大小和 query 语句取出的字段总大小来判断使用哪种排序算法。如果 max_length_for_sort_data 更大，那么使用第二种优化之后的算法，否则使用第一种算法。<br>适当加大系统变量 max_length_for_sort_data 的值，能够让 MySQL 选择更优化的 filesort 的排序算法，当然，设置过大，会造成cpu利用率过低和磁盘 I/O 过高  </p><p>适当加大 sort_buffer_size 排序区，尽量让排序在内存中完成，而不是通过创建临时表放在文件中进行；当然也不能不限制加大 sort_buffer_size 排序区，因为 sort_buffer_size 参数时每个线程独占的，所以要考虑数据库活动连接数和服务器内存的大小来适当设置排序区。  </p><p>尽量只使用必要的字段，select 具体的字段名称，而不是 select * 选择所有字段，这样可以减少排序区使用，提高 sql 性能。  </p><h4 id="使用-sql-提示"><a href="#使用-sql-提示" class="headerlink" title="使用 sql 提示"></a>使用 sql 提示</h4><p>sql 提示（sql hint） 是优化数据库的一个重要手段，简单来说就是在 sql 语句中加入一些人为的提示来达到优化操作的目的。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sql_buffer_results * from ……</span><br></pre></td></tr></table></figure><p>这个语句将强制 MySQL 生成一个临时结果集。只要临时结果集生成后，所有表上的锁定均被释放。<br><code>这能再遇到表锁定问题时或要花很长时间将结果传给客户端时有所帮助，因为可以尽快释放锁资源。</code>   </p><p>常用的 sql 提示：  </p><ul><li>use index 提供希望 MySQL 去参考的索引列表，就可以让 MySQL 不再考虑其他可用的索引。  </li><li>ignore index  忽略一个或者多个索引  </li><li>force index 强制 MySQL 使用一个特定的索引。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 内存优化</title>
      <link href="/2017-08-16-mysql-memory-optimization.html"/>
      <url>/2017-08-16-mysql-memory-optimization.html</url>
      
        <content type="html"><![CDATA[<h3 id="内存优化原则"><a href="#内存优化原则" class="headerlink" title="内存优化原则"></a>内存优化原则</h3><ul><li>将尽量多的内存分配给 MySQL 做缓存，但要给操作系统和其他程序的运行预留足够的内存。  </li><li>myisam 的数据文件读取依赖于操作系统自身的 I/O 缓存，因此，如果有 myisam 表，要预留更多的内存给操作系统做 IO 缓存。  </li><li>排序区，连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置过大，不但浪费内存资源，而且在并发连接较高时会导致物理内存耗尽。  </li></ul><h3 id="myisam-内存优化"><a href="#myisam-内存优化" class="headerlink" title="myisam 内存优化"></a>myisam 内存优化</h3><p>myisam 存储引擎使用 key buffer 缓存索引块，以加速 myisam 索引的读写速度，对于 myisam 表的数据块，MySQL 没有特别的患处机制，完全依赖操作系统的 IO 缓存。  </p><h4 id="key-buffer-size-设置"><a href="#key-buffer-size-设置" class="headerlink" title="key_buffer_size 设置"></a>key_buffer_size 设置</h4><p>key_buffer_size 决定 myisam <code>索引块</code>缓存区的大小，它直接影响 myisam 表的存取效率。可以在 MySQL 的参数文件中设置该值。<br>对弈一般数据库服务器，建议至少将 1/4 可用内存分配给 key_buffer_size。  </p><p>可以通过检查 key_read_requests、key_write_requests 和 key_writes 等 MySQL 状态变量来评估索引缓存的效率。<br>一般来说，索引块物理读比率：<br><code>key_reads / key_read_requests</code> 应小于0.01<br>索引块写比率也应尽可能小，但这与应用特点有关，对于更新和删除操作特别多的应用，key_writes / key_write_requests 可能会接近 1，而对于每次更新很多行记录的应用，key_writes / key_write_requests 就会比较小。  </p><p>除通过索引块的物理读写比率衡量 key buffer 外，也可以通过评估 key buffer 的使用率来判断索引缓存设置是否合理。<br>key buffer 使用率的计算公式如下：<br><code>1 - ((key_blocks_unused * key_cache_block_size) / key_buffer_size)</code><br>一般来说，使用率在 80% 左右比较合适，大于 80% 可能因索引缓存不足而导致性能下降；小于 80% 会导致内存浪费。  </p><h4 id="使用多个索引缓存"><a href="#使用多个索引缓存" class="headerlink" title="使用多个索引缓存"></a>使用多个索引缓存</h4><p>MySQL 通过各 session 共享的 key buffer 提高了 myisam 索引存取的性能，但它并不能消除 session 间对 key buffer 的竞争。比如，一个 session 如果对某个很大的索引进行扫描，就可能将其他的索引数据块挤出索引缓存区，而这些索引块可能是其他 session 要用的热数据。<br>为减少 session 间对 key buffer 的竞争，MySQL 从 5.1 版本开始引入了多缓存的机制，从而可以将不同表的索引缓存到不同的key buffer 中：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># hot_cache 是新建索引缓存的命名，global 关键字表示新建缓存对每一个新的连接都有效。</span><br><span class="line">mysql&gt; set global hot_cache.key_buffer_size &#x3D; 128*1024;</span><br><span class="line"></span><br><span class="line"># 删除刚刚创建的索引缓存</span><br><span class="line">mysql&gt; set global hot_cache.key_buffer_size &#x3D; 0;</span><br></pre></td></tr></table></figure><p>默认情况下，MySQL 将使用默认的key buffer 缓存 myisam 表的索引，可以用 cache index 命令指定表的索引缓存：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table t2 (id int , name varchar(30)) engine myisam;</span><br><span class="line">Query OK, 0 rows affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; create table t1 (id int , name varchar(30)) engine myisam;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; cache index t1,t2 in hot_cache;</span><br><span class="line">+-----------+--------------------+----------+----------+</span><br><span class="line">| Table     | Op                 | Msg_type | Msg_text |</span><br><span class="line">+-----------+--------------------+----------+----------+</span><br><span class="line">| sakila.t1 | assign_to_keycache | status   | OK       |</span><br><span class="line">| sakila.t2 | assign_to_keycache | status   | OK       |</span><br><span class="line">+-----------+--------------------+----------+----------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>更常见的做法是通过配置文件在 MySQL 启动时自动创建并加在索引缓存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">key_buffer_size &#x3D; 4G</span><br><span class="line">hot_cache.key_buffer_size &#x3D; 2G</span><br><span class="line">cold_cache.key_buffer_size &#x3D; 1G</span><br><span class="line">init_file&#x3D;&#x2F;path&#x2F;mysqld_init.sql</span><br></pre></td></tr></table></figure><p>在 mysql_init.sql 文件中，可以通过 cache index 命令分配索引缓存，并用 load index into cache 命令来进行索引预加载：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cache index sales in hot_cache;</span><br><span class="line">cache index sales2 in cold_cache;</span><br><span class="line">load index into cache sales,sales2</span><br></pre></td></tr></table></figure><h4 id="调整“中点插入策略”"><a href="#调整“中点插入策略”" class="headerlink" title="调整“中点插入策略”"></a>调整“中点插入策略”</h4><p><a href="https://dev.mysql.com/doc/refman/5.7/en/midpoint-insertion.html" target="_blank" rel="noopener">中点插入策略</a>（midpoint insertion strategy）是对简单 lru 淘汰算法的改进，它将 lru 链分成两部分：hot 子表和 warm 子表，当一个索引块读入内存时，先放到 lru 链表的“中点”，即 warm 子表的尾部，当达到一定的命中次数后，该索引块会被晋升到 hot 子表的尾部；此后，该数据块在 hot 子表流转，如果其到达 hot 子表的头部并超过一定时间，它将由 hot 子表的头部降级到 warm 子表的头部；当需要淘汰索引块时，缓存管理程序会优先淘汰 warm 表头部的内存块。这种算法能够避免偶尔被访问的索引块将访问频繁的热块淘汰。  </p><p>可以通过调节 key_cache_division_limit 来控制多大比例的缓存用做 warm 子表，其默认值是 100，意思是全部缓存块都放在 warm 子表，其实也就是不启用“中点插入策略”。如果我们希望将大致 30% 的缓存用来 cache 最热的索引块，可以对做如下设置  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set global key_cache_division_limit &#x3D; 70  </span><br><span class="line">set global hot_cache key_cache_division_limit &#x3D; 70</span><br></pre></td></tr></table></figure><p>除了调节 warm 子表的比例外，还可以通过 key_cache_age_threshold 控制数据块由 hot 子表向 warm 子表降级的时间，值越小，数据块就越快被降级。对于有 N 个块的索引缓存来说，如果一个在 hot 子表头部的索引块，在最后 N * key_cache_age_threshold / 100 次缓存命中内未被访问过，就会被降级到 warm 子表。 </p><h4 id="调整-read-buffer-size-和-read-rnd-buffer-size"><a href="#调整-read-buffer-size-和-read-rnd-buffer-size" class="headerlink" title="调整 read_buffer_size 和 read_rnd_buffer_size"></a>调整 read_buffer_size 和 read_rnd_buffer_size</h4><p>如果需要经常顺序扫描 myisam 表，可以通过增大 read_buffer_size 的值来改善性能，但需要注意的是：read_buffer_size 是每个 session 独占的，如果默认值太大，就会造成内存浪费，甚至导致物理内存耗尽。<br>对于需要做排序的 myisam 表查询，如带有 order by 子句的 sql， 适当增大 read_rnd_buffer_size 的值，也可以改善此类 sql 的性能。read_rnd_buffer_size 也是按 session 分配的，默认值不能太大。  </p><h3 id="innodb-内存优化"><a href="#innodb-内存优化" class="headerlink" title="innodb 内存优化"></a>innodb 内存优化</h3><h4 id="innodb-缓存机制"><a href="#innodb-缓存机制" class="headerlink" title="innodb 缓存机制"></a>innodb 缓存机制</h4><p>innodb 用一块内存区做 io 缓存池，该缓存池不仅用来缓存 innodb 的索引块，也用来缓存 innodb 的数据块，这一点与 myisam 不同。  </p><p>在内部，innodb 缓存池逻辑上由 free list、flush list 和 lru list 组成：  </p><ul><li>free list ： 空闲缓存块列表  </li><li>flush list ： 是需要刷新到此磁盘的缓存块列表  </li><li>lru list ： 是 innodb 正在使用的缓存块，它是 innodb buffer pool 的核心。 </li></ul><p>innodb 使用的 lru 算法与 myisam 的“中点插入策略”lru算法很类似，大致原理是：将 lru list 分为 young sublist 和 old sublist，数据从磁盘读入时，会将该缓存块插入到 lru list 的“中点”，即 old sublist 的头部；经过一定时间的访问（由 innodb_old_blocks_time 系统参数决定），该数据块将会由 old sublist 转移到 young sublist 的头部，也就是整个lru list 的头部；随着时间推移，young sublist 和 old sublist 中较少被访问的缓存块将从各自链表的头部逐渐向尾部移动；需要淘汰数据块时，优先从链表尾部淘汰。这种设计同样是为了防止偶尔被访问的索引块将访问频繁的热块淘汰。  </p><h4 id="innodb-buffer-pool-size-的设置"><a href="#innodb-buffer-pool-size-的设置" class="headerlink" title="innodb_buffer_pool_size 的设置"></a>innodb_buffer_pool_size 的设置</h4><p>innodb_buffer_size 决定 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问 innodb 表需要的磁盘 io 就越少，性能也就越高。在一个专用的数据库服务器上，可以将 80% 的物理内存分配给 innodb buffer pool。  </p><p>通过以下命令查看 buffer pool 的使用情况：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql$ mysqladmin -u root -p -S &#x2F;tmp&#x2F;mysql.sock ext | grep -i innodb_buffer_pool</span><br><span class="line">Enter password: </span><br><span class="line">| Innodb_buffer_pool_dump_status                | Dumping of buffer pool not started               |</span><br><span class="line">| Innodb_buffer_pool_load_status                | Buffer pool(s) load completed at 170918 15:07:09 |</span><br><span class="line">| Innodb_buffer_pool_resize_status              |                                                  |</span><br><span class="line">| Innodb_buffer_pool_pages_data                 | 456                                              |</span><br><span class="line">| Innodb_buffer_pool_bytes_data                 | 7471104                                          |</span><br><span class="line">| Innodb_buffer_pool_pages_dirty                | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_bytes_dirty                | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_pages_flushed              | 39                                               |</span><br><span class="line">| Innodb_buffer_pool_pages_free                 | 32312                                            |</span><br><span class="line">| Innodb_buffer_pool_pages_misc                 | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_pages_total                | 32768                                            |</span><br><span class="line">| Innodb_buffer_pool_read_ahead_rnd             | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_read_ahead                 | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_read_ahead_evicted         | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_read_requests              | 3329                                             |</span><br><span class="line">| Innodb_buffer_pool_reads                      | 422                                              |</span><br><span class="line">| Innodb_buffer_pool_wait_free                  | 0                                                |</span><br><span class="line">| Innodb_buffer_pool_write_requests             | 515                                              |</span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql$</span><br></pre></td></tr></table></figure><p>可用以下公式计算 innodb 缓存池的命中率：  </p><p>(1 - innodb_buffer_pool_read / innodb_buffer_pool_read_request) * 100  </p><p>如果命中率太低，则应考虑扩充内存，增加 innodb_buffer_pool_size 的值。  </p><h4 id="调整-old-sublist-大小"><a href="#调整-old-sublist-大小" class="headerlink" title="调整 old sublist 大小"></a>调整 old sublist 大小</h4><p>调整 old_sublist 的比例由系统参数 innodb_old_blocks_pct 决定，其取值范围是 5 ~ 95, 默认值是 37。<br>通过以下命令可以查看其当前设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show global variables like &#39;%innodb_old_blocks_pct%&#39;;</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| Variable_name         | Value |</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| innodb_old_blocks_pct | 37    |</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="调整-innodb-old-blocks-time-的设置"><a href="#调整-innodb-old-blocks-time-的设置" class="headerlink" title="调整 innodb_old_blocks_time 的设置"></a>调整 innodb_old_blocks_time 的设置</h4><p>innodb_old_blocks_time 参数决定了缓存数据块由 old sublist 转移到 young sublist 的快慢，当一个缓存数据块被插入到 midpoint（old sublist）后,至少要在 old sublist 停留超过 innodb_old_blocks_time(ms)后，才有可能被转移到 young list。  </p><h4 id="调整缓存池数量，减少内部对缓存池数据源结构的争用"><a href="#调整缓存池数量，减少内部对缓存池数据源结构的争用" class="headerlink" title="调整缓存池数量，减少内部对缓存池数据源结构的争用"></a>调整缓存池数量，减少内部对缓存池数据源结构的争用</h4><p>MySQL 内部不同线程对 innodb 缓存池的访问在某些阶段是互斥的，<code>这种内部竞争也会产生性能问题</code>, 尤其在高并发和 buffer pool 较大的情况下。为解决这个问题，innodb 的缓存系统引入了 innodb_buffer_poolinstances 配置参数，对于较大的缓存池，适当增大此参数的值，可以降低并发导致的内部缓存访问冲突，改善性能。innodb 缓存系统会将参数 innodb_buffer_pool_size 指定大小的缓存平分为 innodb_buffer_pool_instances 个 buffer pool。  </p><h4 id="控制-innodb-buffer-刷新，延长数据缓存事件，减缓磁盘-I-O"><a href="#控制-innodb-buffer-刷新，延长数据缓存事件，减缓磁盘-I-O" class="headerlink" title="控制 innodb buffer 刷新，延长数据缓存事件，减缓磁盘 I/O"></a>控制 innodb buffer 刷新，延长数据缓存事件，减缓磁盘 I/O</h4><p>在 innodb 找不到干净可用缓存页或检查点被触发等情况下，innodb 的后台线程就会开始把“脏的缓存页”回写到磁盘文件中，这个过程叫<code>缓存刷新</code>。<br>·<br>通常都希望 buffer pool 中的数据在缓存中停留的时间尽可能长，以备重用，从而减少磁盘 IO 的次数。<code>磁盘 IO 慢，是数据库系统最主要的性能瓶颈</code>，可以通过延迟缓存刷新来减轻 IO 压力。  </p><p>innodb buffer pool 的刷新快慢主要取决于两个参数。  </p><h5 id="innodb-max-dirty-pages-pct"><a href="#innodb-max-dirty-pages-pct" class="headerlink" title="innodb_max_dirty_pages_pct"></a>innodb_max_dirty_pages_pct</h5><p>它控制缓存池中脏页的最大比例，默认是 75% ，如果脏页的数量达到或超过该值，innodb 的后台线程将开始缓存刷新。  </p><h5 id="innodb-io-capacity"><a href="#innodb-io-capacity" class="headerlink" title="innodb_io_capacity"></a>innodb_io_capacity</h5><p>它代表磁盘系统的 IO 能力，其值在一定程度上代表磁盘每秒可完成 IO 的次数。innodb_io_capacity 默认值是 200，对于低转速的磁盘，如 7200RPM 的磁盘，可将该值降低到 100，而对于固态硬盘和由多个磁盘组成的盘阵，它的值可以适当增大。  </p><p>innodb_io_capacity 决定一批刷新脏页的数量，当缓存池脏页的比例达到 innodb_max_dirty_pages_pct 时，innodb 大约将 innodb_io_capacity 个已改变的缓存页刷新到磁盘。在合并插入缓存时，innodb 每次合并的页数是 0.05 * innodb_io_capacity。<br>若 innodb_buffer_pool_wait_free 的值增长较快，则说明 innodb 经常在等待空闲缓存页，如果无法增大缓存池，那么应将 innodb_max_dirty_pages_pct 的值调小，或将innodb_io_capacity 的值提高，以加快脏页的刷新。  </p><h4 id="innodb-doublewrite"><a href="#innodb-doublewrite" class="headerlink" title="innodb doublewrite"></a>innodb doublewrite</h4><p>当进行脏页刷新时，innodb 采用了双写（double write）策略，这么做的原因是：MySQL 的数据页大小（一般是 16KB）与操作系统的 IO 数据页大小（一般是 4KB）不一致，无法保证 innodb 缓存页被完整、一致的刷新到磁盘，而innodb 的 redo 日志只记录了数据页改变的部分，并未记录数据页的完整前像，当发生部分写或断裂写时（比如讲缓存页的第一个 4KB 写入磁盘后，服务器断电），就会出现页面无法恢复的问题，为解决这个问题，innodb 引入了 doublewrite 技术。  </p><p>doublewrite 机制的实现原理：  </p><p>用系统表空间的一块连续磁盘空间（100个连续数据页，大小为 2MB）作为 doublewrite buffer，当进行脏页刷新时，首先将脏页的副本写到系统表空间的 doublewrite buffer 中，然后调用 fsync() 刷新操作系统 IO 缓存，确保副本被真正写入磁盘，最后 innodb 后台 IO 线程将脏页刷新到磁盘数据文件。  </p><p>在做恢复时，若㘝发现不一致的页，innodb 会用系统表空间 doublewrite buffer 区的相应副本来恢复数据页。  </p><h3 id="调整用户服务线程排序缓冲区"><a href="#调整用户服务线程排序缓冲区" class="headerlink" title="调整用户服务线程排序缓冲区"></a>调整用户服务线程排序缓冲区</h3><p>如果通过 show global status 看到 sort_merge_passes 的值很大，可以考虑通过调整参数 sort_buffer_size 的值来增大排序缓存区，以改善带有 order by 子句或 group 子句 sql 的性能。  </p><p>对于无法通过索引进行连接操作的查询，可以尝试通过增大 join_buffer_size 的值来改善性能。  </p><p>不过需要注意的是，sort buffer 和 join buffer 都是面向客户线程分配的，如果设置过大可能造成内存浪费。  </p><p>最好的策略是：设置较小的全局 join_buffer_size,而对需要做复杂连接操作的 session 单独设置较大的 join_buffer_size。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySql 索引优化</title>
      <link href="/2017-08-07-mysql-index-optimization.html"/>
      <url>/2017-08-07-mysql-index-optimization.html</url>
      
        <content type="html"><![CDATA[<h4 id="索引的数据结构-B-Tree（mysql主要使用-B-tree-平衡树）"><a href="#索引的数据结构-B-Tree（mysql主要使用-B-tree-平衡树）" class="headerlink" title="索引的数据结构 B-Tree（mysql主要使用 B-tree 平衡树）"></a>索引的数据结构 B-Tree（mysql主要使用 B-tree 平衡树）</h4><h5 id="聚簇索引与非聚簇索引"><a href="#聚簇索引与非聚簇索引" class="headerlink" title="聚簇索引与非聚簇索引"></a>聚簇索引与非聚簇索引</h5><blockquote><p>聚簇索引：索引的叶节点指向数据<br>非聚簇索引：索引的叶节点指向数据的引用  </p></blockquote><table><thead><tr><th>索引类型</th><th>优</th><th>劣</th></tr></thead><tbody><tr><td>聚簇索引</td><td>查询数据少时，无须回行</td><td>不规则插入数据，频繁的页分裂</td></tr></tbody></table><blockquote><p>myisam使用非聚簇索引，innodb使用聚簇索引  </p></blockquote><p>对于innodb引擎：  </p><ol><li>主键索引既存储索引值，又在叶中存储行数据</li><li>如果没有主键，则会使用 unique key 做主键  </li><li>如果没有unique，则mysql会生成一个rowid做主键　　</li></ol><h4 id="索引类型"><a href="#索引类型" class="headerlink" title="索引类型"></a>索引类型</h4><h5 id="主键索引"><a href="#主键索引" class="headerlink" title="主键索引"></a>主键索引</h5><p>primary key() 要求关键字不能重复，也不能为null,同时增加主键约束<br>主键索引定义时，不能命名</p><h5 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h5><p>unique index() 要求关键字不能重复，同时增加唯一约束</p><h5 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h5><p>index() 对关键字没有要求</p><h5 id="全文索引"><a href="#全文索引" class="headerlink" title="全文索引"></a>全文索引</h5><p>fulltext key() 关键字的来源不是所有字段的数据，而是字段中提取的特别关键字</p><p><strong>关键字：可以是某个字段或多个字段，多个字段称为复合索引</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">建表：</span><br><span class="line">creat table student(</span><br><span class="line">    stu_id int unsigned not null auto_increment,</span><br><span class="line">    name varchar(32) not null default &#39;&#39;,</span><br><span class="line">    phone char(11) not null default &#39;&#39;,</span><br><span class="line">    stu_code varchar(32) not null default &#39;&#39;,</span><br><span class="line">    stu_desc text,</span><br><span class="line">    primary key (&#39;stu_id&#39;),     &#x2F;&#x2F;主键索引</span><br><span class="line">    unique index &#39;stu_code&#39; (&#39;stu_code&#39;), &#x2F;&#x2F;唯一索引</span><br><span class="line">    index &#39;name_phone&#39; (&#39;name&#39;,&#39;phone&#39;),  &#x2F;&#x2F;普通索引，复合索引</span><br><span class="line">    fulltext index &#39;stu_desc&#39; (&#39;stu_desc&#39;), &#x2F;&#x2F;全文索引</span><br><span class="line">) engine&#x3D;myisam charset&#x3D;utf8;</span><br><span class="line"></span><br><span class="line">更新：</span><br><span class="line">alert table student</span><br><span class="line">    add primary key (&#39;stu_id&#39;),     &#x2F;&#x2F;主键索引</span><br><span class="line">    add unique index &#39;stu_code&#39; (&#39;stu_code&#39;), &#x2F;&#x2F;唯一索引</span><br><span class="line">    add index &#39;name_phone&#39; (&#39;name&#39;,&#39;phone&#39;),  &#x2F;&#x2F;普通索引，复合索引</span><br><span class="line">    add fulltext index &#39;stu_desc&#39; (&#39;stu_desc&#39;); &#x2F;&#x2F;全文索引</span><br><span class="line"></span><br><span class="line">删除：</span><br><span class="line">alert table sutdent</span><br><span class="line">    drop primary key,</span><br><span class="line">    drop index &#39;stu_code&#39;,</span><br><span class="line">    drop index &#39;name_phone&#39;,</span><br><span class="line">    drop index &#39;stu_desc&#39;;</span><br></pre></td></tr></table></figure><h4 id="索引使用原则"><a href="#索引使用原则" class="headerlink" title="索引使用原则"></a>索引使用原则</h4><h5 id="列独立"><a href="#列独立" class="headerlink" title="列独立"></a>列独立</h5><p>保证索引包含的字段独立在查询语句中，不能是在表达式中</p><h5 id="左前缀"><a href="#左前缀" class="headerlink" title="左前缀"></a>左前缀</h5><p>like:匹配模式左边不能以通配符开始，才能使用索引<br>注意：前缀索引在排序 order by 和分组 group by 操作的时候无法使用。</p><h5 id="复合索引由左到右生效"><a href="#复合索引由左到右生效" class="headerlink" title="复合索引由左到右生效"></a>复合索引由左到右生效</h5><p>建立联合索引，要同时考虑列查询的频率和列的区分度。  </p><p>eg. index(a,b,c)  </p><table><thead><tr><th>语句</th><th>索引是否发挥作用</th></tr></thead><tbody><tr><td>where a=3</td><td>是，只使用了a</td></tr><tr><td>where a=3 and b=5</td><td>是，使用了a,b</td></tr><tr><td>where a=3 and b=5 and c=4</td><td>是，使用了a,b,c</td></tr><tr><td>where b=3 or where c=4</td><td>否</td></tr><tr><td>where a=3 and c=4</td><td>是，仅使用了a</td></tr><tr><td>where a=3 and b&gt;10 and c=7</td><td>是，使用了a,b</td></tr><tr><td>where a=3 and b like ‘%xx%’ and c=7</td><td>使用了a,b</td></tr><tr><td>or的两边都有存在可用的索引，该语句才能用索引。</td><td></td></tr></tbody></table><h5 id="不要滥用索引，多余的索引会降低读写性能"><a href="#不要滥用索引，多余的索引会降低读写性能" class="headerlink" title="不要滥用索引，多余的索引会降低读写性能"></a>不要滥用索引，多余的索引会降低读写性能</h5><p><strong>即使满足了上述原则，mysql还是可能会弃用索引，因为有些查询即使使用索引，也会出现大量的随机io，相对于从数据记录中的顺序io开销更大。</strong></p><h4 id="MySql-中能够使用索引的典型应用"><a href="#MySql-中能够使用索引的典型应用" class="headerlink" title="MySql 中能够使用索引的典型应用"></a>MySql 中能够使用索引的典型应用</h4><blockquote><p>测试库下载地址：<a href="https://downloads.mysql.com/docs/sakila-db.zip" target="_blank" rel="noopener">https://downloads.mysql.com/docs/sakila-db.zip</a>  </p></blockquote><h5 id="匹配全值（match-the-full-value）"><a href="#匹配全值（match-the-full-value）" class="headerlink" title="匹配全值（match the full value）"></a>匹配全值（match the full value）</h5><p>对索引中所有列都指定具体值，即是对索引中的所有列都有等值匹配的条件。<br>例如，租赁表 rental 中通过指定出租日期 rental_date + 库存编号 inventory_id + 客户编号 customer_id 的组合条件进行查询，熊执行计划的 key he extra 两字段的值看到优化器选择了复合索引 idx_rental_date:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select * from rental where rental_date&#x3D;&#39;2005-05-25 17:22:10&#39; and inventory_id&#x3D;373 and customer_id&#x3D;343 \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: rental</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: const</span><br><span class="line">possible_keys: rental_date,idx_fk_inventory_id,idx_fk_customer_id</span><br><span class="line">          key: rental_date</span><br><span class="line">      key_len: 10</span><br><span class="line">          ref: const,const,const</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><p>explain 输出结果中字段 type 的值为 const，表示是常量；字段 key 的值为 rental_date, 表示优化器选择索引 rental_date 进行扫描。  </p><h5 id="匹配值的范围查询（match-a-range-of-values）"><a href="#匹配值的范围查询（match-a-range-of-values）" class="headerlink" title="匹配值的范围查询（match a range of values）"></a>匹配值的范围查询（match a range of values）</h5><p>对索引的值能够进行范围查找。<br>例如，检索租赁表 rental 中客户编号 customer_id 在指定范围内的记录：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select * from rental where customer_id &gt;&#x3D; 373 and customer_id &lt; 400 \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: rental</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: range</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 718</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using index condition</span><br><span class="line"> 1 row in set, 1 warning (0.05 sec)</span><br></pre></td></tr></table></figure><p>类型 type 为  range 说明优化器选择范围查询，索引 key 为 idx_fk_customer_id 说明优化器选择索引 idx_fk_customer_id 来加速访问，注意到这个列子中 extra 列为 using index codition ,表示 mysql 使用了 ICP（using index condition） 来进一步优化查询。  </p><h5 id="匹配最左前缀（match-a-leftmost-prefix）"><a href="#匹配最左前缀（match-a-leftmost-prefix）" class="headerlink" title="匹配最左前缀（match a leftmost prefix）"></a>匹配最左前缀（match a leftmost prefix）</h5><p>仅仅使用索引中的最左边列进行查询，比如在 col1 + col2 + col3 字段上的联合索引能够被包含 col1、(col1 + col2)、（col1 + col2 + col3）的等值查询利用到，可是不能够被 col2、（col2、col3）的等值查询利用到。<br>最左匹配原则可以算是 MySQL 中 B-Tree 索引使用的首要原则。</p><h5 id="仅仅对索引进行查询（index-only-query）"><a href="#仅仅对索引进行查询（index-only-query）" class="headerlink" title="仅仅对索引进行查询（index only query）"></a>仅仅对索引进行查询（index only query）</h5><p>当查询的列都在索引的字段中时，查询的效率更高，所以应该尽量避免使用 select *，需要哪些字段，就只查哪些字段。  </p><h5 id="匹配列前缀（match-a-column-prefix）"><a href="#匹配列前缀（match-a-column-prefix）" class="headerlink" title="匹配列前缀（match a column prefix）"></a>匹配列前缀（match a column prefix）</h5><p>仅仅使用索引中的第一列，并且只包含索引第一列的开头一部分进行查找。<br>例如，现在需要查询出标题 title 是以 AFRICAN 开头的电影信息，从执行计划能够清楚看到，idx_title_desc_part 索引被利用上了：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create index idx_title_desc_part on film_text(title (10), description(20));</span><br><span class="line">Query OK, 0 rows affected (0.07 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; explain select title from film_text where title like &#39;AFRICAN%&#39;\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: film_text</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: range</span><br><span class="line">possible_keys: idx_title_desc_part,idx_title_description</span><br><span class="line">          key: idx_title_desc_part</span><br><span class="line">      key_len: 32</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using where</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><p>extra 值为 using where 表示优化器需要通过索引回表查询数据。  </p><h6 id="能够实现索引匹配部分精确而其他部分进行范围匹配（match-one-part-exactly-and-match-a-range-on-another-part）"><a href="#能够实现索引匹配部分精确而其他部分进行范围匹配（match-one-part-exactly-and-match-a-range-on-another-part）" class="headerlink" title="能够实现索引匹配部分精确而其他部分进行范围匹配（match one part exactly and match a range on another part）"></a>能够实现索引匹配部分精确而其他部分进行范围匹配（match one part exactly and match a range on another part）</h6><p>例如，需要查询出租日期 rental_date 为指定日期且客户编号 customer_id 为指定范围的库存：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; MySQL [sakila]&gt; explain select inventory_id from rental where rental_date&#x3D;&#39;2006-02-14 15:16:03&#39; and customer_id &gt;&#x3D; 300 and customer_id &lt;&#x3D;400\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: rental</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: rental_date,idx_fk_customer_id</span><br><span class="line">          key: rental_date</span><br><span class="line">      key_len: 5</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 182</span><br><span class="line">     filtered: 16.85</span><br><span class="line">        Extra: Using where; Using index</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="如果列名是索引，那么使用-column-name-is-null-就会使用索引。"><a href="#如果列名是索引，那么使用-column-name-is-null-就会使用索引。" class="headerlink" title="如果列名是索引，那么使用 column_name is null 就会使用索引。"></a>如果列名是索引，那么使用 column_name is null 就会使用索引。</h5><p>例如，查询支付表 payment 的租赁编号 rental_id 字段为空的记录就用到了索引：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select * from payment where rental_id is null \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: payment</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: fk_payment_rental</span><br><span class="line">          key: fk_payment_rental</span><br><span class="line">      key_len: 5</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 5</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using index condition</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="存在索引但不能使用索引的典型场景"><a href="#存在索引但不能使用索引的典型场景" class="headerlink" title="存在索引但不能使用索引的典型场景"></a>存在索引但不能使用索引的典型场景</h4><p>有些时候虽然有索引，但是并不被优化器选择使用，下面举例几个不能使用索引的场景。</p><h5 id="以-开头的-like-查询不能利用-B-Tree-索引，执行计划中-key-的值为-null-表示没有使用索引"><a href="#以-开头的-like-查询不能利用-B-Tree-索引，执行计划中-key-的值为-null-表示没有使用索引" class="headerlink" title="以%开头的 like 查询不能利用 B-Tree 索引，执行计划中 key 的值为 null 表示没有使用索引"></a>以%开头的 like 查询不能利用 B-Tree 索引，执行计划中 key 的值为 null 表示没有使用索引</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select * from actor where last_name like &quot;%NI%&quot;\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: actor</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: NULL</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 200</span><br><span class="line">     filtered: 11.11</span><br><span class="line">        Extra: Using where</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><p>因为 B-Tree 索引的结构，所以以%开头的插叙很自然就没法利用索引了。一般推荐使用全文索引（Fulltext）来解决类似的全文检索的问题。或者考虑利用 innodb 的表都是聚簇表的特点，采取一种轻量级别的解决方式：一般情况下，索引都会比表小，扫描索引要比扫描表更快，而Innodb 表上二级索引 idx_last_name 实际上存储字段 last_name 还有主键 actot_id,那么理想的访问应该是<code>首先扫描二级索引 idx_last_name 获得满足条件的last_name like &#39;%NI%&#39; 的主键 actor_id 列表，之后根据主键回表去检索记录，这样访问避开了全表扫描演员表 actor 产生的大量 IO 请求。</code>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">ySQL [sakila]&gt; explain select * from (select actor_id from actor where last_name like &#39;%NI%&#39;) a , actor b where a.actor_id &#x3D; b.actor_id \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: actor</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: index</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: idx_actor_last_name</span><br><span class="line">      key_len: 137</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 200</span><br><span class="line">     filtered: 11.11</span><br><span class="line">        Extra: Using where; Using index</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: b</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: eq_ref</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: PRIMARY</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: sakila.actor.actor_id</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br></pre></td></tr></table></figure><p>从执行计划中能够看出，extra 字段 using wehre；using index。理论上比全表扫描更快一下。  </p><h5 id="数据类型出现隐式转换的时候也不会使用索引"><a href="#数据类型出现隐式转换的时候也不会使用索引" class="headerlink" title="数据类型出现隐式转换的时候也不会使用索引"></a>数据类型出现隐式转换的时候也不会使用索引</h5><p>当列的类型是字符串，那么一定记得在 where 条件中<code>把字符常量值用引号引起来</code>，否则即便这个列上有索引，mysql 也不会用到，因为 MySQL 默认把输入的常量值进行转换以后才进行检索。  </p><p>例如，演员表 actor 中的姓氏字段 last_name 是字符型的，但是 sql 语句中的条件值 1 是一个数值型值，因此即便存在索引 idx_last_name, mysql 也不能正确的用上索引，而是继续进行全表扫描：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select * from actor where last_name &#x3D; 1 \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: actor</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: idx_actor_last_name</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 200</span><br><span class="line">     filtered: 10.00</span><br><span class="line">        Extra: Using where</span><br><span class="line"> 1 row in set, 3 warnings (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; explain select * from actor where last_name &#x3D; &#39;1&#39;\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: actor</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: idx_actor_last_name</span><br><span class="line">          key: idx_actor_last_name</span><br><span class="line">      key_len: 137</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左原则-leftmost，是不会使用复合索引的。"><a href="#复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左原则-leftmost，是不会使用复合索引的。" class="headerlink" title="复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左原则 leftmost，是不会使用复合索引的。"></a>复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左原则 leftmost，是不会使用复合索引的。</h5><h5 id="如果-MySQL-估计使用索引比全表扫描更慢，则不使用索引。"><a href="#如果-MySQL-估计使用索引比全表扫描更慢，则不使用索引。" class="headerlink" title="如果 MySQL 估计使用索引比全表扫描更慢，则不使用索引。"></a>如果 MySQL 估计使用索引比全表扫描更慢，则不使用索引。</h5><h5 id="用-or-分割开的条件，如果-or-前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。"><a href="#用-or-分割开的条件，如果-or-前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。" class="headerlink" title="用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。"></a>用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。</h5><h4 id="索引的维护和优化"><a href="#索引的维护和优化" class="headerlink" title="索引的维护和优化"></a>索引的维护和优化</h4><h5 id="查看索引使用情况"><a href="#查看索引使用情况" class="headerlink" title="查看索引使用情况"></a>查看索引使用情况</h5><p>如果索引正在工作， Handler_read_key 的值将很高，这个值代表了一个行被索引值读的次数，很低的值表名增加索引得到的性能改善不高，因为索引并不经常使用。<br>Handler_read_rnd_next 的值高则意味着查询运行低效，并且应该建立索引补救。这个值的含义是在数据文件中读下一行的请求数。如果正在进行大量的表扫描，Handler_read_rnd_next 的值较高，则通常说明表索引不正确或写入的查询没有利用索引，具体如下。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; show status like &#39;Handler_read%&#39;;</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| Variable_name         | Value |</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| Handler_read_first    | 1     |</span><br><span class="line">| Handler_read_key      | 5     |</span><br><span class="line">| Handler_read_last     | 0     |</span><br><span class="line">| Handler_read_next     | 200   |</span><br><span class="line">| Handler_read_prev     | 0     |</span><br><span class="line">| Handler_read_rnd      | 0     |</span><br><span class="line">| Handler_read_rnd_next | 0     |</span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure><h5 id="删除冗余的索引"><a href="#删除冗余的索引" class="headerlink" title="删除冗余的索引"></a>删除冗余的索引</h5><p>使用percona-toolkit工具包中的 <code>pt-duplicate-key-checker</code> 可以删除重复和冗余的索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;pt-duplicate-key-checker --host&#x3D;172.16.16.35 --port&#x3D;3306 --user&#x3D;root --password&#x3D;123456 --database&#x3D;test --tables&#x3D;new_orders;</span><br></pre></td></tr></table></figure><h5 id="查找未被使用的索引"><a href="#查找未被使用的索引" class="headerlink" title="查找未被使用的索引"></a>查找未被使用的索引</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SELECT object_schema, object_name, index_name, b.&#96;TABLE_ROWS&#96; </span><br><span class="line">FROM performance_schema.table_io_waits_summary_by_index_usage a </span><br><span class="line">JOIN information_schema.tables b </span><br><span class="line">ON a.&#96;OBJECT_SCHEMA&#96;&#x3D;b.&#96;TABLE_SCHEMA&#96; AND a.&#96;OBJECT_NAME&#96;&#x3D;b.&#96;TABLE_NAME&#96; </span><br><span class="line">WHERE index_name IS NOT NULL </span><br><span class="line">AND count_star&#x3D;0 </span><br><span class="line">ORDER BY object_schema, object_name;</span><br></pre></td></tr></table></figure><h5 id="更新索引统计信息及减少索引碎片"><a href="#更新索引统计信息及减少索引碎片" class="headerlink" title="更新索引统计信息及减少索引碎片"></a>更新索引统计信息及减少索引碎片</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">analyze table table_name</span><br></pre></td></tr></table></figure><h4 id="使用索引的小技巧"><a href="#使用索引的小技巧" class="headerlink" title="使用索引的小技巧"></a>使用索引的小技巧</h4><h5 id="字符串字段权衡区分度与长度的技巧"><a href="#字符串字段权衡区分度与长度的技巧" class="headerlink" title="字符串字段权衡区分度与长度的技巧"></a>字符串字段权衡区分度与长度的技巧</h5><p>截取不同长度，测试区分度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 这里假设截取6个字符长度计算区别度，直到区别度达到0.1，就可以把这个字段的这个长度作为索引了</span><br><span class="line">mysql&gt; select count(distinct left([varchar]],6))&#x2F;count(*) from table;</span><br><span class="line"></span><br><span class="line">#注意：设置前缀索引时指定的长度表示字节数，而对于非二进制类型(CHAR, VARCHAR, TEXT)字段而言的字段长度表示字符数，所</span><br><span class="line">#      以，在设置前缀索引前需要把计算好的字符数转化为字节数，常用字符集与字节的关系如下：</span><br><span class="line"># latin      单字节：1B</span><br><span class="line"># GBK    双字节：2B</span><br><span class="line"># UTF8       三字节：3B</span><br><span class="line"># UTF8mb4四字节：4B     </span><br><span class="line"># myisam 表的索引大小默认为 1000字节，innodb 表的索引大小默认为 767 字节，可以在配置文件中修改 innodb_large_prefix </span><br><span class="line"># 项的值增大 innodb 索引的大小，最大 3072 字节。</span><br></pre></td></tr></table></figure><p>区别度能达到0.1，就可以。  </p><h5 id="左前缀不易区分的字段索引建立方法"><a href="#左前缀不易区分的字段索引建立方法" class="headerlink" title="左前缀不易区分的字段索引建立方法"></a>左前缀不易区分的字段索引建立方法</h5><p>这样的字段，左边有大量重复字符，比如url字段汇总的http://</p><ol><li>倒过来存储并建立索引</li><li>新增伪hash字段 把字符串转化为整型</li></ol><h5 id="索引覆盖"><a href="#索引覆盖" class="headerlink" title="索引覆盖"></a>索引覆盖</h5><p>概念：如果查询的列恰好是索引的一部分，那么查询只需要在索引文件上进行，不需要回行到磁盘，这种查询，速度极快，江湖人称——索引覆盖  </p><h5 id="延迟关联"><a href="#延迟关联" class="headerlink" title="延迟关联"></a>延迟关联</h5><p>在根据条件查询数据时，如果查询条件不能用的索引，可以先查出数据行的id，再根据id去取数据行。<br>eg.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;普通查询 没有用到索引</span><br><span class="line">select * from post where content like &quot;%新闻%&quot;;</span><br><span class="line">&#x2F;&#x2F;延迟关联优化后 内层查询走content索引，取出id,在用join查所有行</span><br><span class="line">select a.* from post as a inner join (select id from post where content like &quot;%新闻%&quot;) as b on a.id&#x3D;b.id;</span><br></pre></td></tr></table></figure><h5 id="索引排序"><a href="#索引排序" class="headerlink" title="索引排序　"></a>索引排序　</h5><p>排序的字段上加入索引，可以提高速度。</p><h5 id="重复索引和冗余索引"><a href="#重复索引和冗余索引" class="headerlink" title="重复索引和冗余索引"></a>重复索引和冗余索引</h5><p>重复索引：在同一列或者相同顺序的几个列建立了多个索引，成为重复索引，没有任何意义，删掉<br>冗余索引：两个或多个索引所覆盖的列有重叠，比如对于列m,n ，加索引index m(m),indexmn(m,n),称为冗余索引。  </p><h5 id="索引碎片与维护"><a href="#索引碎片与维护" class="headerlink" title="索引碎片与维护"></a>索引碎片与维护</h5><p>在数据表长期的更改过程中，索引文件和数据文件都会产生空洞，形成碎片。修复表的过程十分耗费资源，可以用比较长的周期修复表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;清理方法</span><br><span class="line">alert table xxx engine innodb; </span><br><span class="line">&#x2F;&#x2F;或</span><br><span class="line">optimize table xxx;</span><br></pre></td></tr></table></figure><h5 id="innodb引擎的索引注意事项"><a href="#innodb引擎的索引注意事项" class="headerlink" title="innodb引擎的索引注意事项"></a>innodb引擎的索引注意事项</h5><p>Innodb 表要尽量自己指定主键，如果有几个列都是唯一的，要选择最常作为访问条件的列作为主键，另外，Innodb 表的普通索引都会保存主键的键值，所以主键要尽可能选择较短的数据类型，可以有效的减少索引的磁盘占用，提高索引的缓存效果。  </p><h5 id="使用索引扫描优化排序"><a href="#使用索引扫描优化排序" class="headerlink" title="使用索引扫描优化排序"></a>使用索引扫描优化排序</h5><ul><li>索引的列顺序和 Order By 子句的顺序完全一致</li><li>索引中所有列的方向(升序, 降序) 和 order by 子句完全一致</li><li>order by 中的字段全部在关联表中的第一张表表中</li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 权限管理</title>
      <link href="/2017-07-30-mysql-authority-management.html"/>
      <url>/2017-07-30-mysql-authority-management.html</url>
      
        <content type="html"><![CDATA[<p>MySQL 的权限表在数据库启动的时候就载入内存，当用户通过身份认证后，就在内存中进行相应权限的存取，这样，此用户就可以在数据库中做权限范围内的各种操作了。  </p><a id="more"></a><h3 id="权限表的存取"><a href="#权限表的存取" class="headerlink" title="权限表的存取"></a>权限表的存取</h3><p>在权限存取的两个过程中，系统会用到 “mysql” 数据库(安装 MySQL 时被创建，数据库名称叫“mysql”) 中 user、host 和 db 这3个最重要的权限表。  </p><p>在这 3 个表中，最重要的表示 user 表，其次是 db 表，host 表在大多数情况下并不使用。  </p><p>user 中的列主要分为 4 个部分：用户列、权限列、安全列和资源控制列。  </p><p>通常用的最多的是用户列和权限列，其中权限列又分为普通权限和管理权限。普通权限用于数据库的操作，比如 <code>select_priv</code>、<code>super_priv</code> 等。  </p><p>当用户进行连接时，权限表的存取过程有以下两个过程：  </p><ul><li>先从 user 表中的 host、user 和 password 这 3 个字段中判断连接的 IP、用户名、和密码是否存在于表中，如果存在，则通过身份验证，否则拒绝连接。</li><li>如果通过身份验证、则按照以下权限表的顺序得到数据库权限：user -&gt; db -&gt; tables_priv -&gt; columns_priv。  </li></ul><p>在这几个权限表中，权限范围依次递减，全局权限覆盖局部权限。上面的第一阶段好理解，下面以一个例子来详细解释一下第二阶段。<br><strong>为了方便测试，需要修改变量 sql_mode</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; sql_mode 默认值中有 NO_AUTO_CREATE_USER (防止GRANT自动创建新用户，除非还指定了密码)</span><br><span class="line">SET SESSION sql_mode&#x3D;&#39;STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION&#39;;</span><br></pre></td></tr></table></figure><h4 id="创建用户-zj-localhost，并赋予所有数据库上的所有表的-select-权限"><a href="#创建用户-zj-localhost，并赋予所有数据库上的所有表的-select-权限" class="headerlink" title="创建用户 zj@localhost，并赋予所有数据库上的所有表的 select 权限"></a>创建用户 zj@localhost，并赋予所有数据库上的所有表的 select 权限</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant select on *.* to zj@localhost;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&quot;zj&quot; and host&#x3D;&#39;localhost&#39; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: localhost</span><br><span class="line">                  User: zj</span><br><span class="line">           Select_priv: Y</span><br><span class="line">           Insert_priv: N</span><br><span class="line">           Update_priv: N</span><br><span class="line">           Delete_priv: N</span><br><span class="line">           Create_priv: N</span><br><span class="line">             Drop_priv: N</span><br><span class="line">           Reload_priv: N</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="查看-db-表"><a href="#查看-db-表" class="headerlink" title="查看 db 表"></a>查看 db 表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; select * from db where user&#x3D;&#39;zj&#39; \G ;</span><br><span class="line">Empty set (0.00 sec)</span><br></pre></td></tr></table></figure><p>可以发现，user 表的 select_priv 列是 “Y”，而 db 表中并没有记录，也就是说，对所有数据库都具有相同的权限的用户并不需要记录到 db 表，而仅仅需要将 user 表中的 select_priv 改为 “Y” 即可。换句话说，user 表中的每个权限都代表了对所有数据库都有权限。  </p><h4 id="将-zj-localhost-上的权限改为只对-t2-数据库上所有表的-select-权限。"><a href="#将-zj-localhost-上的权限改为只对-t2-数据库上所有表的-select-权限。" class="headerlink" title="将 zj@localhost 上的权限改为只对 t2 数据库上所有表的 select 权限。"></a>将 zj@localhost 上的权限改为只对 t2 数据库上所有表的 select 权限。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; revoke select on *.* from zj@localhost;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; grant select on t2.* to zj@localhost;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.04 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&#39;zj&#39; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: localhost</span><br><span class="line">                  User: zj</span><br><span class="line">           Select_priv: N</span><br><span class="line">           Insert_priv: N</span><br><span class="line">           Update_priv: N</span><br><span class="line">           Delete_priv: N</span><br><span class="line">           Create_priv: N</span><br><span class="line">             Drop_priv: N</span><br><span class="line">           Reload_priv: N</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from db where user&#x3D;&#39;zj&#39; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                 Host: localhost</span><br><span class="line">                   Db: t2</span><br><span class="line">                 User: zj</span><br><span class="line">          Select_priv: Y</span><br><span class="line">          Insert_priv: N</span><br><span class="line">          Update_priv: N</span><br><span class="line">          Delete_priv: N</span><br><span class="line">          Create_priv: N</span><br><span class="line">            Drop_priv: N</span><br><span class="line">           Grant_priv: N</span><br></pre></td></tr></table></figure><p>这时候发现，user 表中的 select_priv 变为 “N” ，而 db 表中增加了 db 为 t2 的一条记录。也就是说，当只授予部分数据库某些权限时，user 表中的相应权限列保持 “N”，而将具体的数据库权限写入 db 表。table 和 column 的权限机制和 db 类似。  </p><p>从上例可以看出，当用户通过权限认证，进行权限分配时，将按照 user -&gt; db -&gt; tables_priv -&gt; columns_priv 的顺序进行权限分配，即先检查全局权限表 user，如果 user 中对应 权限为 “Y”，则此用户对所有数据库的权限都为“Y”，将不再检查 db、tables_priv 和 columns_priv；如果为“N”，则到 db 表中检查此用户对应的具体数据库，并得到 db 中为 “Y”的权限；如果 db 中相应权限为 “N”，则再依次检查tables_priv 和 columns_priv 中的权限，如果所有的都为“N”，则判断为不具备权限。  </p><h3 id="账号管理"><a href="#账号管理" class="headerlink" title="账号管理"></a>账号管理</h3><p>主要包括账号的创建，权限的更改和账号的删除。  </p><h4 id="创建账号"><a href="#创建账号" class="headerlink" title="创建账号"></a>创建账号</h4><p>使用 grant 语法创建，示例：  </p><h5 id="创建用户-zj-，权限为可以在所有数据库上执行所有权限，只能从本地进行连接。"><a href="#创建用户-zj-，权限为可以在所有数据库上执行所有权限，只能从本地进行连接。" class="headerlink" title="创建用户 zj ，权限为可以在所有数据库上执行所有权限，只能从本地进行连接。"></a>创建用户 zj ，权限为可以在所有数据库上执行所有权限，只能从本地进行连接。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant all privileges on *.* to zj@localhost;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&quot;zj&quot; and host&#x3D;&quot;localhost&quot; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: localhost</span><br><span class="line">                  User: zj</span><br><span class="line">           Select_priv: Y</span><br><span class="line">           Insert_priv: Y</span><br><span class="line">           Update_priv: Y</span><br><span class="line">           Delete_priv: Y</span><br><span class="line">           Create_priv: Y</span><br><span class="line">             Drop_priv: Y</span><br><span class="line">           Reload_priv: Y</span><br><span class="line">         Shutdown_priv: Y</span><br></pre></td></tr></table></figure><p>可以发现，除了 grant_priv 权限外，所有权限在 user 表里面都是 “Y”。  </p><h5 id="增加对-zj-的-grant-权限"><a href="#增加对-zj-的-grant-权限" class="headerlink" title="增加对 zj 的 grant 权限"></a>增加对 zj 的 grant 权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; grant all privileges on *.* to zj@localhost with grant option;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&quot;zj&quot; and host&#x3D;&#39;localhost&#39; \G ;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: localhost</span><br><span class="line">                  User: zj</span><br><span class="line">           Select_priv: Y</span><br><span class="line">           Insert_priv: Y</span><br><span class="line">           Update_priv: Y</span><br><span class="line">           Delete_priv: Y</span><br><span class="line">           Create_priv: Y</span><br><span class="line">             Drop_priv: Y</span><br><span class="line">           Reload_priv: Y</span><br><span class="line">         Shutdown_priv: Y</span><br><span class="line">          Process_priv: Y</span><br><span class="line">             File_priv: Y</span><br><span class="line">            Grant_priv: Y</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h5 id="设置密码为-“123”"><a href="#设置密码为-“123”" class="headerlink" title="设置密码为 “123”"></a>设置密码为 “123”</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant all  privileges on *.* to zj@localhost identified by &#39;123&#39; with grant option;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&quot;zj&quot; and host&#x3D;&quot;localhost&quot; \G ;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: localhost</span><br><span class="line">                  User: zj</span><br><span class="line">           Select_priv: Y</span><br><span class="line">           Insert_priv: Y</span><br><span class="line">           Update_priv: Y</span><br><span class="line">           Delete_priv: Y</span><br><span class="line">           Create_priv: Y</span><br><span class="line">             Drop_priv: Y</span><br><span class="line">           Reload_priv: Y</span><br><span class="line">......  </span><br><span class="line"> authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257</span><br><span class="line">      password_expired: N</span><br><span class="line"> password_last_changed: 2017-09-25 20:29:42</span><br><span class="line">     password_lifetime: NULL</span><br></pre></td></tr></table></figure><p>可以发现，密码变成了一堆加密后的字符串。  </p><h5 id="创建新用户-zj2，可以从任何-IP-连接，权限为对-t2-数据库里的所有表进行-select-、update、insert-和-delete-操作，初始密码为“123”"><a href="#创建新用户-zj2，可以从任何-IP-连接，权限为对-t2-数据库里的所有表进行-select-、update、insert-和-delete-操作，初始密码为“123”" class="headerlink" title="创建新用户 zj2，可以从任何 IP 连接，权限为对 t2 数据库里的所有表进行 select 、update、insert 和 delete 操作，初始密码为“123”"></a>创建新用户 zj2，可以从任何 IP 连接，权限为对 t2 数据库里的所有表进行 select 、update、insert 和 delete 操作，初始密码为“123”</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant select ,insert, update,delete on t2.* to &#39;zj2&#39;@&#39;%&#39; identified by &#39;123&#39;;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from user where user&#x3D;&#39;zj2&#39; and host&#x3D;&quot;%&quot; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                  Host: %</span><br><span class="line">                  User: zj2</span><br><span class="line">           Select_priv: N</span><br><span class="line">           Insert_priv: N</span><br><span class="line">           Update_priv: N</span><br><span class="line">           Delete_priv: N</span><br><span class="line">           Create_priv: N</span><br><span class="line">             Drop_priv: N</span><br><span class="line">......</span><br><span class="line"> authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257</span><br><span class="line">      password_expired: N</span><br><span class="line"> password_last_changed: 2017-09-25 20:37:49</span><br><span class="line">     password_lifetime: NULL</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; select * from db where user&#x3D;&quot;zj2&quot; and host&#x3D;&#39;%&#39; \G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                 Host: %</span><br><span class="line">                   Db: t2</span><br><span class="line">                 User: zj2</span><br><span class="line">          Select_priv: Y</span><br><span class="line">          Insert_priv: Y</span><br><span class="line">          Update_priv: Y</span><br><span class="line">          Delete_priv: Y</span><br><span class="line">          Create_priv: N</span><br><span class="line">            Drop_priv: N</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>user 表中的权限都是“N”，db 表中增加的记录权限则都是“Y”。一般的，只授予用户适当的权限，而不会授予过多的权限。  </p><p>本例中的 IP 限制为所有 IP 都可以连接，因此设置为 “*”，mysql 数据库中是通过 user 表的 host 字段来进行控制，host 可以是以下类型的赋值。  </p><ul><li>Host 值可以是主机名或IP号，或 “localhost” 指出本地主机。  </li><li>可以在 Host 列值使用通配符字符 “%” 和 “_”</li><li>Host 值 “%” 匹配任何主机名，空 Host 值等价于 “%”，它们的含义与 like 操作符的模式匹配操作相同。  </li></ul><p><strong>注意:</strong> mysql 数据库的 user 表中 host 的值为 “*” 或者空，表示所有外部 IP 都可以连接，但是不包括本地服务器 localhost，因此，如果要包括本地服务器，必须单独为 localhost 赋予权限。  </p><h5 id="授予-super、process、file-权限给用户-zj3"><a href="#授予-super、process、file-权限给用户-zj3" class="headerlink" title="授予 super、process、file 权限给用户 zj3@%"></a>授予 super、process、file 权限给用户 zj3@%</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant super,process,file on *.* to &#39;zj3&#39;@&#39;%&#39;;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><p>因为这几个权限都是属于管理权限，因此不能够指定某个数据库，on 后面必须跟 “<em>.</em>”,下面语法将提示错误  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant super,process,file on t2.* to &#39;zj3&#39;@&#39;%&#39;;</span><br><span class="line">ERROR 1221 (HY000): Incorrect usage of DB GRANT and GLOBAL PRIVILEGES</span><br></pre></td></tr></table></figure><h5 id="只授予登录权限给-zj4-localhost"><a href="#只授予登录权限给-zj4-localhost" class="headerlink" title="只授予登录权限给 zj4@localhost"></a>只授予登录权限给 zj4@localhost</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">MySQL [mysql]&gt; grant usage on *.* to &#39;zj4&#39;@&#39;localhost&#39;;</span><br><span class="line">Query OK, 0 rows affected, 2 warnings (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; exit</span><br><span class="line">Bye</span><br><span class="line"></span><br><span class="line">zj@bogon:~$ mysql -uzj4 -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 78</span><br><span class="line">Server version: 5.7.18-log Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2017, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">+--------------------+</span><br><span class="line">1 row in set (0.02 sec)</span><br></pre></td></tr></table></figure><p>usage 权限只能用于数据库登录，不能执行任何操作  </p><h4 id="查看账号权限"><a href="#查看账号权限" class="headerlink" title="查看账号权限"></a>查看账号权限</h4><p>账号创建好后，可以通过如下命令查看权限:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show grants for user@host;</span><br></pre></td></tr></table></figure><p>示例：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show grants for zj@localhost;</span><br><span class="line">+-------------------------------------------------------------------+</span><br><span class="line">| Grants for zj@localhost                                           |</span><br><span class="line">+-------------------------------------------------------------------+</span><br><span class="line">| GRANT ALL PRIVILEGES ON *.* TO &#39;zj&#39;@&#39;localhost&#39; WITH GRANT OPTION |</span><br><span class="line">+-------------------------------------------------------------------+</span><br><span class="line">1 row in set (0.01 sec)</span><br></pre></td></tr></table></figure><h4 id="更改账号权限"><a href="#更改账号权限" class="headerlink" title="更改账号权限"></a>更改账号权限</h4><p>可以进行权限的新增和回收。和创建账号一样，权限变更也有两种方法：使用 grant(新增) 和 revoke (回收) 语句，或者更改权限表。  </p><h5 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h5><h5 id="zj4-localhost-目前只有登录权限"><a href="#zj4-localhost-目前只有登录权限" class="headerlink" title="zj4@localhost 目前只有登录权限"></a>zj4@localhost 目前只有登录权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show grants for zj4@localhost;</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">| Grants for zj4@localhost                |</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">| GRANT USAGE ON *.* TO &#39;zj4&#39;@&#39;localhost&#39; |</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="赋予-zj4-localhost-所有数据库上的所有表的-select-权限"><a href="#赋予-zj4-localhost-所有数据库上的所有表的-select-权限" class="headerlink" title="赋予 zj4@localhost 所有数据库上的所有表的 select 权限"></a>赋予 zj4@localhost 所有数据库上的所有表的 select 权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; grant select on *.* to &#39;zj4&#39;@&#39;localhost&#39;;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show grants for zj4@localhost;</span><br><span class="line">+------------------------------------------+</span><br><span class="line">| Grants for zj4@localhost                 |</span><br><span class="line">+------------------------------------------+</span><br><span class="line">| GRANT SELECT ON *.* TO &#39;zj4&#39;@&#39;localhost&#39; |</span><br><span class="line">+------------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="继续给-zj4-localhost-赋予-select-和-insert-权限，和已有的-select-权限进行合并"><a href="#继续给-zj4-localhost-赋予-select-和-insert-权限，和已有的-select-权限进行合并" class="headerlink" title="继续给 zj4@localhost 赋予 select 和 insert 权限，和已有的 select 权限进行合并"></a>继续给 zj4@localhost 赋予 select 和 insert 权限，和已有的 select 权限进行合并</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show grants for &#39;zj4&#39;@&#39;localhost&#39;;</span><br><span class="line">+--------------------------------------------------+</span><br><span class="line">| Grants for zj4@localhost                         |</span><br><span class="line">+--------------------------------------------------+</span><br><span class="line">| GRANT SELECT, INSERT ON *.* TO &#39;zj4&#39;@&#39;localhost&#39; |</span><br><span class="line">+--------------------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>revoke 语句可以回收已经赋予的权限，对于上面的例子，这里决定要收回 zj4@localhost 上的 insert 和 select 权限：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; revoke select,insert on *.* from zj4@localhost;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show grants for zj4@localhost;</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">| Grants for zj4@localhost                |</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">| GRANT USAGE ON *.* TO &#39;zj4&#39;@&#39;localhost&#39; |</span><br><span class="line">+-----------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>usage 权限不能被回收，也就是说，revoke 用户并不能删除用户。  </p><h4 id="修改账号密码"><a href="#修改账号密码" class="headerlink" title="修改账号密码"></a>修改账号密码</h4><h5 id="可以用-mysqladmin-命令在命令行指定密码。"><a href="#可以用-mysqladmin-命令在命令行指定密码。" class="headerlink" title="可以用 mysqladmin 命令在命令行指定密码。"></a>可以用 mysqladmin 命令在命令行指定密码。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysqladmin -u user_name -h host_name password &quot;123456&quot;</span><br></pre></td></tr></table></figure><h5 id="执行-set-password-语句。"><a href="#执行-set-password-语句。" class="headerlink" title="执行 set password 语句。"></a>执行 set password 语句。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; set password for &#39;username&#39;@&#39;%&#39; &#x3D; password(&#39;pwd&#39;);</span><br></pre></td></tr></table></figure><p>如果是更改自己的密码，可以省略 for 语句  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; set password&#x3D;password(&#39;pwd&#39;);</span><br></pre></td></tr></table></figure><h5 id="可以在全局级别使用-grant-usage-语句-在“-”-来指定某个账户的密码而不影响账户当前的权限。"><a href="#可以在全局级别使用-grant-usage-语句-在“-”-来指定某个账户的密码而不影响账户当前的权限。" class="headerlink" title="可以在全局级别使用 grant usage 语句(在“.”)来指定某个账户的密码而不影响账户当前的权限。"></a>可以在全局级别使用 grant usage 语句(在“<em>.</em>”)来指定某个账户的密码而不影响账户当前的权限。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant usage on *.* to &#39;username&#39;@&#39;%&#39; identified by &#39;pwd&#39;;</span><br></pre></td></tr></table></figure><h4 id="删除账号"><a href="#删除账号" class="headerlink" title="删除账号"></a>删除账号</h4><p>要彻底的删除账号，可以使用 drop user ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop user zj@localhost;</span><br></pre></td></tr></table></figure><h4 id="账号资源限制"><a href="#账号资源限制" class="headerlink" title="账号资源限制"></a>账号资源限制</h4><p>创建 MySQL 账号时，还有一类选项称为<code>账号资源限制</code>，这类选项的作用是限制每个账号实际具有的资源限制，这里的“资源”主要包括：  </p><ul><li>max_queries_per_hour count : 单个账号每小时执行的查询次数</li><li>max_upodates_per_hour count : 单个账号每小时执行的更新次数</li><li>max_connections_per_hour count : 单个账号每小时连接服务器的次数</li><li>max_user_connections count : 单个账号并发连接服务器的次数  </li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调整 MySQL 并发相关的参数</title>
      <link href="/2017-07-23-mysql-concurrent-params.html"/>
      <url>/2017-07-23-mysql-concurrent-params.html</url>
      
        <content type="html"><![CDATA[<p>MySQL server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在 MySQL 中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size 以及 table_open_cache 等。  </p><a id="more"></a><h3 id="调整-max-connections，提高并发连接"><a href="#调整-max-connections，提高并发连接" class="headerlink" title="调整 max_connections，提高并发连接"></a>调整 max_connections，提高并发连接</h3><p>参数 max_connections 控制允许连接到 MySQL 数据库的最大数量。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，就说明不断有连接请求因数据库连接数已达到最大允许的值而失败，应考虑增大max_connections 的值。  </p><p>MySQL 最大可支持的数据库连接取决于很多因素，包括给定操作系统平台线程库的质量、内存大小、每个连接的符合以及期望的响应时间等。在 Linux 平台下，MySQL 支持 500~1000 个连接不是难事，如果内存足够，不考虑响应时间，甚至能达到上万个连接。而在 windows 平台下，受其所用线程库的影响，最大连接数有以下限制：  </p><p>(open tables * 2 + open connections) &lt; 2048  </p><p>每个session 操作 MySQL 数据库表都需要占用文件描述符，数据库连接本身也要占用文件描述符，因此，在增大 max_connections 时，也要注意评估 open-files-limit (文件描述符)的设置是否够用。</p><h3 id="调整-back-log"><a href="#调整-back-log" class="headerlink" title="调整 back_log"></a>调整 back_log</h3><p>back_log 参数控制 MySQL 监听 tcp 端口时设置的积压请求栈大小，5.6.6版本以前的默认值是 50,5.6.6 版本以后的默认值是 50 + （max_connections/5）,但最大不能超过 900。  </p><p>如果需要数据库在较短时间内处理大量连接请求，可以考虑适当增大 back_log 的值。  </p><h3 id="调整-table-open-cache"><a href="#调整-table-open-cache" class="headerlink" title="调整 table_open_cache"></a>调整 table_open_cache</h3><p>每一个 sql 执行线程至少都要打开一个表缓存，参数 table_open_cache 控制所有 sql 执行线程可打开表缓存的数量。这个参数的值应根据最大连接数 max_connections 以及每个连接执行关联查询中所涉及表的最大个数（用 N 表示）来设定：  </p><p>max_connection * N  </p><p>在未执行 flush tables 命令的情况下，如果 MySQL 状态值 opened_tables 的值较大，就说明 table_open_cache 设置的太小，应适当增大。增大 table_open_cache 的值，会增加 MySQL 对文件描述符的使用量，因此，也要注意评估 open-files-limit 的设置是或否够用。  </p><h3 id="调整-thread-cache-size"><a href="#调整-thread-cache-size" class="headerlink" title="调整 thread_cache_size"></a>调整 thread_cache_size</h3><p>为加快连接数据库的速度，MySQL  会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户端线程的数量。<br>可以通过计算线程 cache 的失效率 threads_created / connections 来衡量 thread_cahce_size 的设置是否合适，该值越接近 1，说明线程 cache 命中率越低，应考虑适当增加 thread_cahce_size 的值。  </p><h3 id="innodb-lock-wait-timeout-的设置"><a href="#innodb-lock-wait-timeout-的设置" class="headerlink" title="innodb_lock_wait_timeout 的设置"></a>innodb_lock_wait_timeout 的设置</h3><p>参数 innodb_lock_wait_timeout 可以控制 innodb 事务等待行锁的时间，默认值是 50ms，可以根据需要动态设置。对于需要快速反馈的交互式应用，可以将行锁等待超时时间调大，以避免发生大的回滚操作。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 常见问题和应用技巧</title>
      <link href="/2017-07-09-mysql-common-skill.html"/>
      <url>/2017-07-09-mysql-common-skill.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>记录 MySQL 的一些使用技巧 </p></blockquote><a id="more"></a><h3 id="忘记-MySQL-的-root-密码"><a href="#忘记-MySQL-的-root-密码" class="headerlink" title="忘记 MySQL 的 root 密码"></a>忘记 MySQL 的 root 密码</h3><h4 id="登录到数据库所在的服务器，手工-kill-掉-MySQL-进程"><a href="#登录到数据库所在的服务器，手工-kill-掉-MySQL-进程" class="headerlink" title="登录到数据库所在的服务器，手工 kill 掉 MySQL 进程"></a>登录到数据库所在的服务器，手工 kill 掉 MySQL 进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@bogon:&#x2F;data&#x2F;mysql# kill &#96;cat .&#x2F;mysql.pid&#96;</span><br></pre></td></tr></table></figure><p>其中，mysql.pid 指的是 MySQL 数据目录下的 pid 文件，它记录了 MySQL 服务的进程号。  </p><h4 id="使用-–skip-grant-tables-选项重启-MySQL-服务："><a href="#使用-–skip-grant-tables-选项重启-MySQL-服务：" class="headerlink" title="使用 –skip-grant-tables 选项重启 MySQL 服务："></a>使用 –skip-grant-tables 选项重启 MySQL 服务：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;data&#x2F;mysql$ sudo &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin&#x2F;mysqld --skip-grant-tables --user&#x3D;root &amp;</span><br></pre></td></tr></table></figure><p>–skip-grant-tables 选项意思是启动 MySQL 服务时跳过权限表认证。启动后，连接到 MySQL 的 root 将不需要口令。  </p><h4 id="用空密码的-root-用户连接到-mysql-，并且更改-root-口令"><a href="#用空密码的-root-用户连接到-mysql-，并且更改-root-口令" class="headerlink" title="用空密码的 root 用户连接到 mysql ，并且更改 root 口令"></a>用空密码的 root 用户连接到 mysql ，并且更改 root 口令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ mysql -uroot</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.7.18-log Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2017, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; set password &#x3D; password(&#39;123456&#39;);</span><br><span class="line">ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement</span><br><span class="line">MySQL [(none)]&gt; use mysql</span><br><span class="line">Database changed</span><br><span class="line">MySQL [mysql]&gt; update user set authentication_string&#x3D;password(&#39;123456&#39;) where user&#x3D;&quot;root&quot; and host&#x3D;&quot;localhost&quot;;</span><br><span class="line">Query OK, 1 row affected, 1 warning (0.02 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 1</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [mysql]&gt; exit;</span><br><span class="line">Bye</span><br><span class="line"></span><br><span class="line">****************************************************************</span><br><span class="line"></span><br><span class="line">zj@bogon:&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin$ mysql -uroot -p123456</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 7</span><br><span class="line">Server version: 5.7.18-log Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2017, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt;</span><br></pre></td></tr></table></figure><p>由于使用了 –skip-grant-tables 选项启动，使用 “set password” 命令更改密码失败，直接更新 user 表的 authentication_string(测试版本为5.7.18,有的版本密码字段是 ‘password’) 字段后，更改密码成功。刷新权限表，使权限认证重新生效。重新用 root 登录时，就可以使用刚刚修改后的口令了。  </p><h3 id="如何处理-myisam-存储引擎的表损坏"><a href="#如何处理-myisam-存储引擎的表损坏" class="headerlink" title="如何处理 myisam 存储引擎的表损坏"></a>如何处理 myisam 存储引擎的表损坏</h3><p>有的时候可能会遇到 myisam 表损坏的情况。一张损坏的表的症状通常是查询意外中断，并且能看到下述错误：  </p><ul><li>‘table_name.frm’ 被锁定不能更改  </li><li>不能找到文件 ‘tbl_name.MYYI’ （errcode:nnn）  </li><li>文件意外结束</li><li>记录文件被毁坏</li><li>从表处理器得到错误 nnn。  </li></ul><p>通常有以下两种解决方法：  </p><h4 id="使用-myisamchk-工具"><a href="#使用-myisamchk-工具" class="headerlink" title="使用 myisamchk 工具"></a>使用 myisamchk 工具</h4><p>使用 MySQL 自带的 myisamchk 工具进行修复：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; myisamchk -r tablename</span><br></pre></td></tr></table></figure><p>其中 -r 参数的含义是 recover，上面的方法几乎能解决所有问题，如果不行，则使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysiamchk -o tablename</span><br></pre></td></tr></table></figure><p>其中 -o 参数的含义是 –safe-recover，可以进行更安全的修复。  </p><h4 id="使用-sql-命令"><a href="#使用-sql-命令" class="headerlink" title="使用 sql 命令"></a>使用 sql 命令</h4><p>使用 MySQL 的 check table 和 repair table 命令一起进行修复，check table 用来检查表是否有损坏；repair table 用来对坏表进行修复。  </p><h3 id="数据目录磁盘空间不足的问题"><a href="#数据目录磁盘空间不足的问题" class="headerlink" title="数据目录磁盘空间不足的问题"></a>数据目录磁盘空间不足的问题</h3><p>系统上线后，随着数据量的不断增加，会发现数据目录下的可用空间越来越小，从而给应用造成了安全隐患。  </p><h4 id="对于-myisam-存储引擎的表"><a href="#对于-myisam-存储引擎的表" class="headerlink" title="对于 myisam 存储引擎的表"></a>对于 myisam 存储引擎的表</h4><p>对于 myisam 存储引擎的表，在建表时可以用如下选项分别制定数据目录和索引目录存储到不同的磁盘空间，而默认会同时放在数据目录下：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data directory &#x3D; &#39;absolute path to directory&#39;</span><br><span class="line">index directory &#x3D; &#39;absolute path to directory&#39;</span><br></pre></td></tr></table></figure><p>如果表已经创建，只能先停机或者将表锁定，防止表的更改，然后将表的数据文件和索引文件 mv 到磁盘充足的分区上，然后在原文件处创建符号链接即可。</p><h4 id="对于-innodb-存储引擎的表"><a href="#对于-innodb-存储引擎的表" class="headerlink" title="对于 innodb 存储引擎的表"></a>对于 innodb 存储引擎的表</h4><p>因为数据文件和索引文件是存放在一起的，所以无法将它们分离。当磁盘空间出现不足时，可以增加一个新的数据文件，这个文件放在充足空间的磁盘上。<br>具体实现方法是在参数 innodb_data_file_path 中增加此文件，路径写为新磁盘的绝对路径。<br>例如，如果 /home 下空间不足，希望在 /home1 下新增加一个可自动扩充数据的文件，那么参数可以这么写：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">innodb_data_file_path &#x3D; &#x2F;home&#x2F;ibdata1:2000M;&#x2F;home1&#x2F;ibdata2:2000M:autoextend</span><br></pre></td></tr></table></figure><p>参数修改后，必须重启数据库才可以生效。  </p><h3 id="DNS反向解析的问题-5-0-以后的版本默认跳过域名逆向解析"><a href="#DNS反向解析的问题-5-0-以后的版本默认跳过域名逆向解析" class="headerlink" title="DNS反向解析的问题  (5.0 以后的版本默认跳过域名逆向解析)"></a>DNS反向解析的问题  (5.0 以后的版本默认跳过域名逆向解析)</h3><p>在客户端执行 show processlist 命令，有时会出现很多进程，类似于：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unauthenticated user | 192.168.10.10:55644 | null | connect | null | login | null</span><br></pre></td></tr></table></figure><p>这些进程会累计的越来越多，并且不会消失，应用无法正常相应，导致系统瘫痪。  </p><p>MySQL 在默认情况下对于远程连接过来的 IP 地址会进行域名的逆向解析，如果系统的 hosts 文件中没有与之对应的域名，MySQL 就会将此连接认为是无效用户，所以下进程中出现 unauthenticated user 并导致进程阻塞。  </p><p>解决的方法很简单，在启动时加上 –skip-name-resolve 选项，则 MySQL 就可以跳过域名解析过程，避免上述问题。  </p><h3 id="mysql-sock-丢失后如何连接数据库"><a href="#mysql-sock-丢失后如何连接数据库" class="headerlink" title="mysql.sock 丢失后如何连接数据库"></a>mysql.sock 丢失后如何连接数据库</h3><p>在 MySQL 服务器本机上连接数据库时，经常会出现 mysql.sock 不存在，导致无法连接的问题。这是因为如果指定 localhost 作为一个主机名，则 mysqladmin 默认使用 Unix 套接字文件连接，而不是 tcp/ip。而这个套接字文件(一般命名为 mysql.sock)经常会因为各种原因而被删除。通过 –protocol=TCP|SOCKET|PIPE|MEMORY 选项，用户可以显式地指定连接协议，下面演示使用了 Unix 套接字失败后使用 tcp 协议连接成功的例子。  </p><h4 id="Unix-套接字连接："><a href="#Unix-套接字连接：" class="headerlink" title="Unix 套接字连接："></a>Unix 套接字连接：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:~$ mysql</span><br><span class="line">ERROR 2002 (HY000): Can&#39;t connect to local MySQL server through socket &#39;&#x2F;tmp&#x2F;mysql.sock&#39; (2)</span><br></pre></td></tr></table></figure><h4 id="tcp-连接"><a href="#tcp-连接" class="headerlink" title="tcp 连接"></a>tcp 连接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zj@bogon:~$ mysql --protocol&#x3D;TCP</span><br></pre></td></tr></table></figure><h3 id="MySQL-查看生效的配置文件"><a href="#MySQL-查看生效的配置文件" class="headerlink" title="MySQL 查看生效的配置文件"></a>MySQL 查看生效的配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld --help --verbose | grep -A 1 &#39;Default options&#39;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 锁机制</title>
      <link href="/2017-07-02-mysql-lock-intro.html"/>
      <url>/2017-07-02-mysql-lock-intro.html</url>
      
        <content type="html"><![CDATA[<p>锁是计算机协调多个进程或线程并发访问某一资源的机制</p><a id="more"></a><h4 id="MySQL-锁概述"><a href="#MySQL-锁概述" class="headerlink" title="MySQL 锁概述"></a>MySQL 锁概述</h4><p>MySQL 两种锁特性归纳 ：  </p><ul><li>表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。  </li><li>行级锁：开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率最低，并发度也最高。   </li></ul><p>myisam 和 memory 存储引擎采用的是 表级锁；<br>innodb 存储引擎既支持行级锁，也支持表级锁，但默认情况下采用行级锁。  </p><p>表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如 web 应用；<br>行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用。 </p><h4 id="myisam-表锁"><a href="#myisam-表锁" class="headerlink" title="myisam 表锁"></a>myisam 表锁</h4><h5 id="MySQL-表级锁的锁模式"><a href="#MySQL-表级锁的锁模式" class="headerlink" title="MySQL 表级锁的锁模式"></a>MySQL 表级锁的锁模式</h5><p>MySQL 的表级锁有两种模式，表共享读锁(table read lock)和表独占写锁（table write lock）。  </p><p><code>对 myisam 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；对 myisam 表的写操作，则会阻塞其他用户对同一表的读和写操作</code>  </p><p>当一个线程获得对一个表的写锁时，只有持有锁的线程可以对表进行更新操作，其他线程的读写操作都会等待，直到锁被释放。  </p><h5 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h5><p>myisam 在执行查询语句（select）前，会自动给涉及到的所有表加读锁；在执行更新操作（update、delete、insert等）前，会自动给涉及的表加写锁，这个过程并不需要直接用 <code>lock table</code> 命令给 myisam 表显示加锁。<br>给 myisam 表显式加锁，一般是为了在一定程度模拟事务操作。<br>myisam 总是自动给sql语句涉及到的所有表加锁，所以显示锁表的时候，必须同时取得所有涉及表的锁，这也正是 myisam 表不会出现死锁（deadlock）的原因。  </p><p>注意：在使用 <code>lock tables</code> 时，不仅需要一次锁定用到的所有表，而且同一个表在 sql 语句中出现多少次，就要通过与 sql 语句中相同的别名锁定多少次，否则会报错。</p><h5 id="并发插入（concurrent-inserts）"><a href="#并发插入（concurrent-inserts）" class="headerlink" title="并发插入（concurrent inserts）"></a>并发插入（concurrent inserts）</h5><p>myisam 表的读和写是串行的，但这是就总体而言的。在一定条件下，myisam 表也支持查询和插入操作的并发进行。<br>myisam 存储引擎有一个系统变量 <code>concurrent_insert</code> , 专门用以控制其并发插入的行为，其值分别可以为0,1,2。  </p><ul><li>为 <code>0</code> 时，不允许并发插入。  </li><li>为 <code>1</code> 时，如果 myisam 表中没有空洞（即表的中间没有被删除的行），myisam 允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是 MySQL 的默认设置。  </li><li>为 <code>2</code> 时，无论 myisam 表中有没有空洞，都允许在表尾并发插入记录。  </li></ul><h5 id="myisam-的锁调度"><a href="#myisam-的锁调度" class="headerlink" title="myisam 的锁调度"></a>myisam 的锁调度</h5><p>myisam 存储引擎的读锁和写锁是互斥的，读写操作时串行的。<br>当一个进程请求某个 myisam 表的读锁，同时另一个进程也请求同一表的写锁时，写进程会先获得锁。<br>不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前，这是因为 mysql 认为写请求一般比读请求重要。<br>这也正是<code>myisam 表不太适合有大量更新操作和查询操作应用</code>的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。  </p><p>通过一些参数设置可以调节 MySQL 的默认调度行为：  </p><ul><li>通过指定启动参数 <code>low-priority-updates</code>, 使 myisam 引擎默认给予读请求以优先的权利。  </li><li>通过执行命令 <code>set low_priority_updates = 1</code>, 使该连接发出的更新请求优先级降低。  </li><li>通过指定 insert、update、delete 语句的 low_priority 属性，降低该语句的优先级。  </li></ul><p>上述方式都是要么更新优先，要么查询优先，MySQL 也提供了一种折中的办法调节读写冲突：  </p><p>给系统参数 <code>max_write_lock_count</code> 设置一个合适的值，当一个表的读锁达到这个值后，MySQL 就暂时将写请求的优先级降低，给读进程一定获得锁的机会。  </p><h5 id="查询表级锁争用情况"><a href="#查询表级锁争用情况" class="headerlink" title="查询表级锁争用情况"></a>查询表级锁争用情况</h5><p>可以通过检查 <code>table_locks_waited</code> 和 <code>table_locks_immediate</code> 状态变量来分析系统上的表锁定争夺：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; show status like &#39;table_locks%&#39;;</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| Variable_name         | Value |</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">| Table_locks_immediate | 100   |</span><br><span class="line">| Table_locks_waited    | 0     |</span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure><p>如果 <code>table_locks_waited</code> 的值比较高，则说明存在着较严重的表级锁争用情况。  </p><h4 id="InnoDB-锁"><a href="#InnoDB-锁" class="headerlink" title="InnoDB 锁"></a>InnoDB 锁</h4><p>innodb 与 myisam 的最大不同有两点，一是支持事务（transaction），二是采用了行级锁。  </p><h5 id="并发事务处理存在的问题"><a href="#并发事务处理存在的问题" class="headerlink" title="并发事务处理存在的问题"></a>并发事务处理存在的问题</h5><p>相对于串行处理来说，并发事务处理能力大大增加数据库资源的利用率，提高数据库系统事务吞吐量，从而可以支持更多的用户，但并发事务处理也会带来一些问题：</p><table><thead><tr><th>\</th><th>描述</th></tr></thead><tbody><tr><td>更新丢失(lost update)</td><td>多个事务更新某行时，每个事务都不知道其他事务的存在，最后的更新覆盖了其他事务所做的更新</td></tr><tr><td>脏读(dirty reads)</td><td>事务A正在修改id为1的记录，在事务A提交前，事务B也来读取同一行，如果事务B读出了事务A做的修改，就叫脏读</td></tr><tr><td>不可重复读(non-repeatable reads)</td><td>事务A做一个查询(M)后未提交，此时事务B修改查询(M)涉及到的记录并提交，事务A做查询(M),发现结果集发生变化，这种情况叫做不可重复读</td></tr><tr><td>幻读(phantom reads)</td><td>事务A查询id为1的记录，发现没有，此时事务B插入id为1的记录，因为事务A并没有找到id为1的记录，所以随后插入id为1的记录，但会提示<code>duplicate key</code>拒绝插入，这种情况叫做幻读</td></tr></tbody></table><h5 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h5><p><code>更新丢失:</code>通常可以加必要的锁来解决。<br><code>脏读/不可重复读/幻读:</code>其实都是数据库读一致性问题，须由数据库提供一定的事务隔离机制。  </p><p>数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上<code>串行化</code>进行，这显然与<code>并发</code>是矛盾的。<br>为了解决<code>隔离</code>与<code>并发</code>的矛盾，<code>ISO/ANSI SQL92</code> 定义了 4 个事务隔离级别，MySQL 实现了这四种级别，应用可以选择适合自己的隔离级别</p><p><img src="/images/mysql-lock/1.png" alt="image"></p><h5 id="innodb-的行锁模式及加锁方法"><a href="#innodb-的行锁模式及加锁方法" class="headerlink" title="innodb 的行锁模式及加锁方法"></a>innodb 的行锁模式及加锁方法</h5><p>Innodb 实现了两种类型的行锁：  </p><ul><li>共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。</li><li>排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务获取相同数据集的共享读锁和排他写锁。  </li></ul><p>另外，为了允许行锁和表锁共存，实现多粒度锁机制，innodb 还有两种内部使用的意向锁，这两种意向锁都是表锁：  </p><ul><li>意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。  </li><li>意向排它锁（IX）: 事务打算给数据行加行排它锁，事务在给一个数据行加排它锁前必须先取得该表的 IX 锁。  </li></ul><p><img src="/images/mysql-lock/2.png" alt="image"><br>如果一个事务请求的锁模式与当前的锁兼容，innodb 就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。<br>意向锁是 innodb 自动加的，不需要用户干预。对于 update、delete 和 insert 语句，innodb 会自动给涉及数据集加排它锁（X）；对于普通 select 语句，innodb 不会加任何锁。  </p><p>事务可以通过以下语句显式给记录集加共享锁或排它锁。  </p><ul><li>共享锁（S）：select * from table_name where … lock in share mode.</li><li>排它锁（X）: select * from table_name where … for update.    </li></ul><p>用 select… in share mode 获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行 update 或者 delete 操作。但是如果<code>当前事务也需要对该记录进行更新操作</code>,则有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用 select… for update 方式获得排他锁。</p><h6 id="innodb-存储引擎共享锁例子-更新时死锁"><a href="#innodb-存储引擎共享锁例子-更新时死锁" class="headerlink" title="innodb 存储引擎共享锁例子(更新时死锁)"></a>innodb 存储引擎<code>共享锁</code>例子(更新时死锁)</h6><p><img src="/images/mysql-lock/3.png" alt="image"></p><h6 id="innodb-存储引擎排它锁例子"><a href="#innodb-存储引擎排它锁例子" class="headerlink" title="innodb 存储引擎排它锁例子"></a>innodb 存储引擎<code>排它锁</code>例子</h6><p><img src="/images/mysql-lock/4.png" alt="image"></p><h5 id="innodb-行锁实现方式"><a href="#innodb-行锁实现方式" class="headerlink" title="innodb 行锁实现方式"></a>innodb 行锁实现方式</h5><p>innodb 行锁是通过给索引项加锁来实现的，如果么有索引，innodb 将通过隐藏的聚簇索引来对记录加锁。innodb 行锁分为 3 种情形：  </p><ul><li>record lock： 对索引项加锁</li><li>gap lock： 对索引项之间的“间隙”、第一条记录前的“间隙”或最后一条记录的“间隙”加锁。  </li><li>next-key lock： 前两种的结合，对记录及其前面的间隙加锁。</li></ul><p><code>innodb 这种行锁实现特点意味着：如果不通过索引条件检索数据，那么 innodb 将对表中的所有记录加锁，实际效果和表锁一样！</code>  </p><p>在实际应用中，要特别注意 innodb 行锁的这一特性，否则可能导致大量的锁冲突，从而影响并发性能。  </p><h5 id="查看-Innodb-行锁争用情况"><a href="#查看-Innodb-行锁争用情况" class="headerlink" title="查看 Innodb 行锁争用情况"></a>查看 Innodb 行锁争用情况</h5><p>可以通过检查 <code>innodb_row_lock</code> 状态变量来分析系统上的行锁的争夺情况：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; show status like &#39;innodb_row_lock%&#39;;</span><br><span class="line">+-------------------------------+-------+</span><br><span class="line">| Variable_name                 | Value |</span><br><span class="line">+-------------------------------+-------+</span><br><span class="line">| Innodb_row_lock_current_waits | 0     |</span><br><span class="line">| Innodb_row_lock_time          | 0     |</span><br><span class="line">| Innodb_row_lock_time_avg      | 0     |</span><br><span class="line">| Innodb_row_lock_time_max      | 0     |</span><br><span class="line">| Innodb_row_lock_waits         | 0     |</span><br><span class="line">+-------------------------------+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>如果发现锁争用比较严重，如 <code>Innodb_row_lock_waits</code> 和 <code>Innodb_row_lock_time_avg</code> 的值比较高，可以通过查询 <code>information_schema</code> 数据库中相关的表来查看锁情况，或者通过设置 <code>innodb monitors</code> 来进一步观察。  </p><h6 id="查询-information-schema-数据库中的表了解锁等待"><a href="#查询-information-schema-数据库中的表了解锁等待" class="headerlink" title="查询 information_schema 数据库中的表了解锁等待"></a>查询 information_schema 数据库中的表了解锁等待</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; use information_schema</span><br><span class="line">Database changed</span><br><span class="line">MySQL [information_schema]&gt; select * from innodb_locks \G;</span><br></pre></td></tr></table></figure><h6 id="通过设置-innodb-monitors-观察锁冲突情况"><a href="#通过设置-innodb-monitors-观察锁冲突情况" class="headerlink" title="通过设置 innodb monitors 观察锁冲突情况"></a>通过设置 innodb monitors 观察锁冲突情况</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table innodb_monitor (a int) engine&#x3D;innodb;</span><br><span class="line">Query OK, 0 rows affected (0.05 sec)</span><br><span class="line"></span><br><span class="line">show engine innodb status \G;</span><br></pre></td></tr></table></figure><h6 id="在不通过索引条件查询时，innodb-会锁定表中的所有记录。"><a href="#在不通过索引条件查询时，innodb-会锁定表中的所有记录。" class="headerlink" title="在不通过索引条件查询时，innodb 会锁定表中的所有记录。"></a>在不通过索引条件查询时，innodb 会锁定表中的所有记录。</h6><p>创建测试表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table tab_no_index (id int, name varchar(10)) engine&#x3D;innodb;</span><br><span class="line">Query OK, 0 rows affected (0.04 sec)</span><br><span class="line">MySQL [sakila]&gt; insert into tab_no_index values (1,&#39;1&#39;),(2,&#39;2&#39;),(3,&#39;3&#39;),(4,&#39;4&#39;);</span><br><span class="line">Query OK, 4 rows affected (0.01 sec)</span><br><span class="line">Records: 4  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p><img src="/images/mysql-lock/5.png" alt="image"></p><p><code>看起来 session_1 只给一行加了排他锁，但 session_2 在请求其他行的排他锁时，却出现了锁等待！原因就是在没有索引的情况下，Innodb 会对所有记录都加锁。当给其增加一个索引后，innodb 就只锁定了符合条件的行</code> </p><p>创建测试表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table tab_with_index (id int , name varchar(10)) engine &#x3D; innodb;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; alter table tab_with_index add index id(id);</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; insert into tab_with_index values (1,&#39;1&#39;),(2,&#39;2&#39;),(3,&#39;3&#39;),(4,&#39;4&#39;);</span><br><span class="line">Query OK, 4 rows affected (0.00 sec)</span><br><span class="line">Records: 4  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p><img src="/images/mysql-lock/6.png" alt="image"></p><h6 id="由于-MySQL-的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。"><a href="#由于-MySQL-的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。" class="headerlink" title="由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。"></a>由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。</h6><p>创建测试表,id字段有索引，name字段没有索引：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table tab_with_index (id int , name varchar(10)) engine &#x3D; innodb;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; alter table tab_with_index add index id(id);</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; insert into tab_with_index values (1,&#39;1&#39;),(1,&#39;4&#39;);</span><br><span class="line">Query OK, 4 rows affected (0.00 sec)</span><br><span class="line">Records: 4  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p><img src="/images/mysql-lock/7.png" alt="image"></p><h6 id="当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，innodb-都会使用行锁对数据加锁。"><a href="#当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，innodb-都会使用行锁对数据加锁。" class="headerlink" title="当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，innodb 都会使用行锁对数据加锁。"></a>当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，不论是使用主键索引、唯一索引或普通索引，innodb 都会使用行锁对数据加锁。</h6><p>创建测试表，id 字段和 name 字段都有索引：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; create table tab_with_index (id int , name varchar(10),index id (id),index name (name)) engine &#x3D; innodb;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; insert into tab_with_index values (1,&#39;1&#39;),(1,&#39;4&#39;),(2,&#39;2&#39;);</span><br><span class="line">Query OK, 3 rows affected (0.00 sec)</span><br><span class="line">Records: 3  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p><img src="/images/mysql-lock/8.png" alt="image"></p><h6 id="即便在条件中使用了索引字段，但是否使用索引来检索数据是由-MySQL-通过判断不同执行计划的代价来决定的，如果-MySQL-认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下-innodb-也会对所有记录加锁。因此，在分析锁冲突时，别忘了检查-sql-的执行计划，以确认是否真正使用了索引。"><a href="#即便在条件中使用了索引字段，但是否使用索引来检索数据是由-MySQL-通过判断不同执行计划的代价来决定的，如果-MySQL-认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下-innodb-也会对所有记录加锁。因此，在分析锁冲突时，别忘了检查-sql-的执行计划，以确认是否真正使用了索引。" class="headerlink" title="即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 innodb 也会对所有记录加锁。因此，在分析锁冲突时，别忘了检查 sql 的执行计划，以确认是否真正使用了索引。"></a>即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，<code>如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 innodb 也会对所有记录加锁。</code>因此，在分析锁冲突时，别忘了检查 sql 的执行计划，以确认是否真正使用了索引。</h6><h5 id="next-key-锁"><a href="#next-key-锁" class="headerlink" title="next-key 锁"></a>next-key 锁</h5><p>当我们用<code>范围条件</code>而不是相等条件检索数据，并请求共享或排他锁时,innodb 会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（gap）”，innodb 也会对这个“间隙”加锁，这种锁机制就是所谓的 next-key 锁。  </p><p>举例来说，假如 emp 表中只有 101 条记录，其 id 的值分别是1、2、…、100、101，下面的 sql:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 这是一个范围条件的检索，innodb 不仅会对符合条件的 id 值为 101 的记录加锁，也会对 id 大于 101（这些记录并不存在）的“间隙”加锁。</span><br><span class="line">select * from emp where id &gt; 100 for update;</span><br></pre></td></tr></table></figure><p>innodb 使用 next-key 锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，对于上面的例子，要是不使用间隙锁，如果其他事务插入了 id 大于 100 的任何记录，那么本事务如果再次执行上述语句，就会发生幻读；另一方面，是为了满足其恢复和复制的需要。  </p><p>在使用范围条件检索并锁定记录时，innodb 这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，<code>在实际开发中，尤其是并发插入比较多的应用，应该尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。</code>  </p><p>innodb 除了通过范围条件加锁时使用 next-key 锁外，如果使用相等条件请求给一个不存在的记录加锁，innodb 也会使用 next-key 锁！  </p><h5 id="innodb-使用表锁"><a href="#innodb-使用表锁" class="headerlink" title="innodb 使用表锁"></a>innodb 使用表锁</h5><p>innodb 表在个别特殊任务中，也考虑使用表级锁：  </p><ol><li>事务需要更新大部分或全部数据，表又比较大</li><li>事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定多个表，从而避免死锁，减少数据库因事务回滚带来的开销。<br>当然，应用中这两种事务不能太多，否则，就应该考虑使用 myisam 表了。  </li></ol><p>在 innodb 下，使用表锁要注意以下两点：  </p><ol><li>使用 <code>lock tables</code> 虽然可以给 innodb 加表级锁，但必须说明的是，表锁不是由 innodb 存储引擎管理的，而是由其上一层———— MySQL server 负责的，<code>仅当 autocommit=0、innodb_table_locks=1(默认设置)时，innodb 层才知道 MySQL 加的表锁，MySQL server 也才能够感知 innodb 加的行锁，这种情况下，innodb 才能自动识别涉及到的锁</code>。  </li><li>在用 <code>lock_tables</code> 对 innodb 表加锁时要注意，要将 autocommit 设为 0，否则 MySQL 不会给表加锁；事务结束前，不要用 unlock tables 释放表锁，因为 unlock tables 会隐含的提交事务；commit 或 rollback 并不能释放用 lock tables 加的表锁，必须用 unlock tables 释放表锁  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set autocommit &#x3D; 0;</span><br><span class="line">lock tables ti write, t2 read, ...;</span><br><span class="line">[do something with tables t1 and t2 here];</span><br><span class="line">commit;</span><br><span class="line">unlock tables;</span><br></pre></td></tr></table></figure><h4 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h4><p>myisam 表锁是 deadlock free 的，这是因为 myisam 总是一次获取所需的全部锁，要么全部满足，要么等待，因此不会出现死锁。但在 innodb 中，除单个 sql 组成的事务外，锁是逐步获得的，这就决定了在 innodb 中发生死锁是可能的。  </p><p><img src="/images/mysql-lock/9.png" alt="image"></p><p>上面的例子中，<code>两个事务都需要获得对方持有的排他锁才能继续完成事务，这种循环锁等待就是典型的死锁。</code>  </p><p>发生死锁后，innodb 一般都能自动检测到，并使一个事务释放锁回退，另一个事务获得锁，继续完成事务。但在涉及外部锁或表锁的情况下，innodb 并不能完全自动检测到死锁，只需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占用大量计算机资源，造成严重的性能问题，甚至拖垮数据库。  </p><p><code>通常来说，死锁都是应用设计的问题，通过调整业务流程，数据库对象设计、事务大小、以及访问数据库的 sql 语句，绝大部分死锁都可以避免。</code>  </p><h5 id="避免死锁的方法"><a href="#避免死锁的方法" class="headerlink" title="避免死锁的方法"></a>避免死锁的方法</h5><h6 id="程序中并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会"><a href="#程序中并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会" class="headerlink" title="程序中并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会"></a>程序中并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会</h6><p>下面的例子中，由于两个 session 访问两个表的顺序不同，发生死锁的机会就非常高！但如果以相同的顺序来访问，死锁就可以避免。<br><img src="/images/mysql-lock/10.png" alt="image"> </p><p>如果 session_2 以相同的顺序执行 sql 语句，会造成锁等待，但不会死锁。</p><h6 id="在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。"><a href="#在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。" class="headerlink" title="在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。"></a>在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。</h6><p><img src="/images/mysql-lock/11.png" alt="image"> </p><h6 id="在事务中，如果要更新记录，应该申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。"><a href="#在事务中，如果要更新记录，应该申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。" class="headerlink" title="在事务中，如果要更新记录，应该申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。"></a>在事务中，如果要更新记录，应该申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。</h6><h6 id="在-repeatable-read-隔离级别下，如果两个线程同时对相同条件记录用-select-…-for-update-加排他锁，在没有符合该条件记录情况下，两个线程过会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成-read-committed-，就可避免问题。"><a href="#在-repeatable-read-隔离级别下，如果两个线程同时对相同条件记录用-select-…-for-update-加排他锁，在没有符合该条件记录情况下，两个线程过会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成-read-committed-，就可避免问题。" class="headerlink" title="在 repeatable-read 隔离级别下，如果两个线程同时对相同条件记录用 select … for update 加排他锁，在没有符合该条件记录情况下，两个线程过会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成 read committed ，就可避免问题。"></a>在 repeatable-read 隔离级别下，如果两个线程同时对相同条件记录用 select … for update 加排他锁，在没有符合该条件记录情况下，两个线程过会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成 read committed ，就可避免问题。</h6><h6 id="当隔离级别为-read-committed-时，如果两个线程都先执行-select-…-for-update-判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第-1-个线程提交后，第-2-个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁，如果有第-3-个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行-rollback-释放获得的排他锁"><a href="#当隔离级别为-read-committed-时，如果两个线程都先执行-select-…-for-update-判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第-1-个线程提交后，第-2-个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁，如果有第-3-个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行-rollback-释放获得的排他锁" class="headerlink" title="当隔离级别为 read committed 时，如果两个线程都先执行 select … for update, 判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第 1 个线程提交后，第 2 个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁，如果有第 3 个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行 rollback 释放获得的排他锁"></a>当隔离级别为 read committed 时，如果两个线程都先执行 select … for update, 判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第 1 个线程提交后，第 2 个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁，如果有第 3 个线程又来申请排他锁，也会出现死锁。<code>对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行 rollback 释放获得的排他锁</code></h6><p>** 尽管通过上面介绍的设计和 sql 优化等措施，可以大大减少死锁，但死锁很难完全避免。因此，在程序设计中总是捕获并处理死锁异常是一个很好的编程习惯 **  </p><p>如果出现死锁，可以用 show innodb status 命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 sql 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。可以据此分析产生死锁的原因。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 日志介绍</title>
      <link href="/2017-06-24-mysql-log.html"/>
      <url>/2017-06-24-mysql-log.html</url>
      
        <content type="html"><![CDATA[<p>MySQL 中有 4 中不同的日志，分别是错误日志、二进制日志（binlog 日志）、查询日志和，慢查询日志，应该充分利用这些日志对数据库进行各种维护和调优。  </p><a id="more"></a><h3 id="错误日志"><a href="#错误日志" class="headerlink" title="错误日志"></a>错误日志</h3><p>错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。  </p><p>配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log-error&#x3D;mysql_error.log</span><br></pre></td></tr></table></figure><h3 id="二进制日志"><a href="#二进制日志" class="headerlink" title="二进制日志"></a>二进制日志</h3><p><a href="http://waterandair.top/mysql-binlog.html" target="_blank" rel="noopener">二进制日志</a>  </p><h3 id="查询日志"><a href="#查询日志" class="headerlink" title="查询日志"></a>查询日志</h3><p>查询日志记录了客户端的所有语句，而二进制日志不包含查询语句。  </p><h4 id="日志的位置和格式"><a href="#日志的位置和格式" class="headerlink" title="日志的位置和格式"></a>日志的位置和格式</h4><h5 id="开启查询日志"><a href="#开启查询日志" class="headerlink" title="开启查询日志"></a>开启查询日志</h5><p>可以通过参数 <code>--general_log[={0|1}]</code> 和 <code>--general_log_file=file_name</code> 来配置查询日志。<br><code>--general_log</code> 设置为 1 或者不带值都可以启用查询日志；设置为 0 或者不指定次参数表示关闭查询日志。<br>如果没有指定 <code>--general_log_file=file_name</code> 的值，且没有设置 <code>--log-output</code> 参数，那么日志将写入参数 datadir(数据目录)指定的路径下，默认文件名为 host_name.log。  </p><p>这两个参数都是 global 类型，可以在系统启动时或者系统运行时进行动态修改，如果想在 session 级别控制，则通过在 session 中设置参数 <code>sql_log_off</code> 为 on 或者 off。  </p><h5 id="查询日志的存储位置"><a href="#查询日志的存储位置" class="headerlink" title="查询日志的存储位置"></a>查询日志的存储位置</h5><p>查询日志和慢查询日志都可以选择保存在文件或者表中，使用参数 <code>--log-output[=value]</code> 来进行控制，value 值可以是 table、file、none 的一个或者多个组合，中间用逗号进行分割，分别表示日志保存在表、文件、不保存在表和文件中，这里的表指的是 mysql 库中的 <code>general_log</code> (慢查询日志是 <code>slow_log</code>) 表。<br>其中 none 的优先级最高，比如：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log-output &#x3D; table, file  # 表示日志可以同时输出到表和文件中  </span><br><span class="line">log-output &#x3D; table, none  #由于 none 的优先级高，表示日志不保存在表和文件中。</span><br></pre></td></tr></table></figure><p>如果不设置此参数，则默认输出到文件。  </p><h4 id="日志的读取"><a href="#日志的读取" class="headerlink" title="日志的读取"></a>日志的读取</h4><p>查询日志记录的格式是纯文本，所以可以直接进行读取。  </p><p>** 注意：** log 日志中记录了所有数据库的操作，对于访问频繁的系统，此日志对系统性能的影响较大，建议一般情况下关闭。  </p><h3 id="慢查询日志"><a href="#慢查询日志" class="headerlink" title="慢查询日志"></a>慢查询日志</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>慢查询日志记录了所有执行时间超过参数 <code>long_query_time(单位:秒)</code> 设置值且扫描记录数不小于 <code>min_examined_row_limit</code> 的 sql 语句(注意：获得表锁定的时间不算作执行时间)。<code>long_query_time</code> 默认为 10 秒，最小为 0，精度可以到微秒。  </p><p>在默认情况下，有两类常见语句不会记录到慢查询日志：<code>管理语句和不使用索引进行查询的语句</code>。管理语句包括 alter table、analyze table、check table、create index、drop index、optimize table、repair table。如果要监控这两类 sql语句，可以分别通过设置参数<br><code>--log-slow-admin-statements</code> 和 <code>log_queries_not_using_indexes</code> 进行控制。  </p><h4 id="开启慢查询日志"><a href="#开启慢查询日志" class="headerlink" title="开启慢查询日志"></a>开启慢查询日志</h4><p><strong>修改配置文件:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 开启慢查询日志</span><br><span class="line">slow_query_log&#x3D;1</span><br><span class="line"># 指定慢查询日志的路径</span><br><span class="line">slow_query_log_file&#x3D;&quot;mysql-slow.log&quot;</span><br><span class="line"># 指定查询时间大于多少的才进行记录,单位是秒,也就是操作大于1s 的操作都会被记录。</span><br><span class="line">long_query_time&#x3D;1</span><br></pre></td></tr></table></figure><h4 id="慢日志的读取"><a href="#慢日志的读取" class="headerlink" title="慢日志的读取"></a>慢日志的读取</h4><p>和错误日志、查询日志一样，慢查询日志记录的格式也是纯文本，可以被直接读取。<br>如果要设置微秒级的慢查询，可以 <code>set global long_query_time=0.01</code>  </p><p>如果慢查询日志中记录内容很多，可以使用 <code>mysqldumpslow</code> 工具对慢查询日志进行分类汇总。对于 sql 文本完全一致，只是变量不同的语句，mysqldumpslow 将会自动视为同一个语句进行统计，变量值用 N 来代替。这个统计结果将大大增加用户阅读慢查询日志的效率，迅速定位系统的 sql 瓶颈。  </p><p>** 注意： ** 慢查询日志对于发现应用中有性能问题的 sql 很有帮助，应该经常查看分析。  </p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制日志 binlog 详细介绍</title>
      <link href="/2017-06-20-mysql-binlog.html"/>
      <url>/2017-06-20-mysql-binlog.html</url>
      
        <content type="html"><![CDATA[<p>二进制日志（binlog） 记录了所有 DDL 语句和 DML 语句，但是不包括数据查询语句。语句以“事件”的形式保存，它描述了数据的更改过程。通过binlog可以做数据恢复,做主从复制,此日志对于灾难时的数据恢复起着极其重要的作用。  </p><a id="more"></a><h4 id="开启-binlog日志"><a href="#开启-binlog日志" class="headerlink" title="开启 binlog日志"></a>开启 binlog日志</h4><p>mysql5.7 默认是不开启 binlog 的, 需要在配置文件或MySQL客户端配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 添加三个参数</span><br><span class="line">log_bin&#x3D;ON  # 打开 binlog 日志</span><br><span class="line">log_bin_basename&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin  # binlog 日志基本文件名,后面会追加标识来表示每一个文件</span><br><span class="line">log_bin_index&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin.index  # binlog 文件的索引文件,这个文件管理了所有的 binlog 文件的目录</span><br><span class="line"></span><br><span class="line"># 上面三个参数的配置也可以简化为一个,下面这个配置的作用等同于上面三个</span><br><span class="line">log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin</span><br><span class="line"></span><br><span class="line"># 上面的配置修改好后,对于 5.7版本以下的MySQL,就算是完成了,</span><br><span class="line"># 但是对于 5.7 及以上的版本,还需要配置一个 server-id, 且不能和集群中的其他MySQL服务器相同</span><br><span class="line"></span><br><span class="line">server-id&#x3D;1</span><br></pre></td></tr></table></figure><h4 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h4><h5 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h5><p>重启 MySQL 后, 用 mysql 客户端登录  </p><p>查看是否开启binlog: <code>show variables like &#39;%log_bin%&#39;</code><br>查看当前 mysql 的 binlog 情况: <code>show master status</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; show variables like &quot;%log_bin%&quot;</span><br><span class="line">    -&gt; ;</span><br><span class="line">+---------------------------------+----------------------------+</span><br><span class="line">| Variable_name                   | Value                      |</span><br><span class="line">+---------------------------------+----------------------------+</span><br><span class="line">| log_bin                         | ON                         |</span><br><span class="line">| log_bin_basename                | &#x2F;var&#x2F;mysql&#x2F;mysql-bin       |</span><br><span class="line">| log_bin_index                   | &#x2F;var&#x2F;mysql&#x2F;mysql-bin.index |</span><br><span class="line">| log_bin_trust_function_creators | OFF                        |</span><br><span class="line">| log_bin_use_v1_row_events       | OFF                        |</span><br><span class="line">| sql_log_bin                     | ON                         |</span><br><span class="line">+---------------------------------+----------------------------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000001 |      154 |              |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.04 sec)</span><br></pre></td></tr></table></figure><h5 id="手动刷新-binlog"><a href="#手动刷新-binlog" class="headerlink" title="手动刷新 binlog"></a>手动刷新 binlog</h5><p>通过 flush logs, 会创建一个新的 binlog 文件, 注意下面的 File 变为了 mysql-bin.000002</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; flush logs;</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000002 |      154 |              |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h5 id="清空-binlog"><a href="#清空-binlog" class="headerlink" title="清空 binlog"></a>清空 binlog</h5><h5 id="日志的删除"><a href="#日志的删除" class="headerlink" title="日志的删除"></a>日志的删除</h5><p>对于比较繁忙的系统，每天产生大量日志，这些日志如果长时间不清除，将会对磁盘空间带来极大的浪费，因此，需要定期删除日志。  </p><h6 id="删除所有日志"><a href="#删除所有日志" class="headerlink" title="删除所有日志"></a>删除所有日志</h6><p>执行 <code>reset master</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MySQL [(none)]&gt; reset master;</span><br><span class="line">Query OK, 0 rows affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000001 |      154 |              |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>命令将删除所有 binlog 日志，新日志编号从“000001”开始  </p><h6 id="删除指定序号之前的日志文件"><a href="#删除指定序号之前的日志文件" class="headerlink" title="删除指定序号之前的日志文件"></a>删除指定序号之前的日志文件</h6><p>执行 “purge master logs to ‘mysql-bin.<strong>**</strong>‘”命令，将删除编号之前的所有日志(不删除命令中指定的文件)  </p><h6 id="删除指定日期前的日志"><a href="#删除指定日期前的日志" class="headerlink" title="删除指定日期前的日志"></a>删除指定日期前的日志</h6><p>执行 “purge master log before ‘yyyy-mm-dd hh24:mi:ss’” 命令将删除指定日期前的所有日志  </p><h6 id="修改配置文件，自动删除"><a href="#修改配置文件，自动删除" class="headerlink" title="修改配置文件，自动删除"></a>修改配置文件，自动删除</h6><p>在配置文件的[mysqld]中设置参数 <code>expire_logs_days=#</code>, 此参数的含义是设置日志的过期天数，过了指定的天数后日志将会被自动删除。  </p><h5 id="查看-binlog-文件"><a href="#查看-binlog-文件" class="headerlink" title="查看 binlog 文件"></a>查看 binlog 文件</h5><p>binlog 文件是二进制文件,mysql 提供了 <code>mysqlbinlog</code> 工具用于查看 binlog 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@c5da76d1ecdc:&#x2F;var&#x2F;mysql# mysqlbinlog mysql-bin.000001 </span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;1*&#x2F;;</span><br><span class="line">&#x2F;*!50003 SET @OLD_COMPLETION_TYPE&#x3D;@@COMPLETION_TYPE,COMPLETION_TYPE&#x3D;0*&#x2F;;</span><br><span class="line">DELIMITER &#x2F;*!*&#x2F;;</span><br><span class="line"># at 4</span><br><span class="line">#180621 11:33:12 server id 1  end_log_pos 123 CRC32 0xb2899866 Start: binlog v 4, server v 5.7.22-0ubuntu0.16.04.1-log created 180621 11:33:12 at startup</span><br><span class="line"># Warning: this binlog is either in use or was not closed properly.</span><br><span class="line">ROLLBACK&#x2F;*!*&#x2F;;</span><br><span class="line">BINLOG &#39;</span><br><span class="line">eBwrWw8BAAAAdwAAAHsAAAABAAQANS43LjIyLTB1YnVudHUwLjE2LjA0LjEtbG9nAAAAAAAAAAAA</span><br><span class="line">AAAAAAAAAAAAAAAAAAB4HCtbEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQA</span><br><span class="line">AWaYibI&#x3D;</span><br><span class="line">&#39;&#x2F;*!*&#x2F;;</span><br><span class="line"># at 123</span><br><span class="line">#180621 11:33:12 server id 1  end_log_pos 154 CRC32 0x5d42e0ee Previous-GTIDs</span><br><span class="line"># [empty]</span><br><span class="line">SET @@SESSION.GTID_NEXT&#x3D; &#39;AUTOMATIC&#39; &#x2F;* added by mysqlbinlog *&#x2F; &#x2F;*!*&#x2F;;</span><br><span class="line">DELIMITER ;</span><br><span class="line"># End of log file</span><br><span class="line">&#x2F;*!50003 SET COMPLETION_TYPE&#x3D;@OLD_COMPLETION_TYPE*&#x2F;;</span><br><span class="line">&#x2F;*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE&#x3D;0*&#x2F;;</span><br></pre></td></tr></table></figure><p>这里主要关注 <code>end_log_pos 154</code>, 表示 binlog 当前的位置,主从复制中,从数据库就是根据这个位置完成增量复制  </p><h4 id="用-binlog-完成数据恢复"><a href="#用-binlog-完成数据恢复" class="headerlink" title="用 binlog 完成数据恢复"></a>用 binlog 完成数据恢复</h4><p><a href="http://waterandair.top/mysql-backup-restore.html#%E5%AE%8C%E5%85%A8%E6%81%A2%E5%A4%8D" target="_blank" rel="noopener">使用 binlog 日志完成数据恢复 </a></p><h4 id="用-binlog-完成主从复制"><a href="#用-binlog-完成主从复制" class="headerlink" title="用 binlog 完成主从复制"></a>用 binlog 完成主从复制</h4><p><a href="http://waterandair.top/mysql-replication.html" target="_blank" rel="noopener">用 binlog 完成主从复制</a></p><h4 id="日志的格式"><a href="#日志的格式" class="headerlink" title="日志的格式"></a>日志的格式</h4><p>二进制日志的格式分为 3 种：statement、row、mixed，可以在启动时通过参数 –binlog_format 进行设置，这 3 种格式的区别如下：  </p><h5 id="STATEMENT"><a href="#STATEMENT" class="headerlink" title="STATEMENT"></a>STATEMENT</h5><p>mysql 5.1 之前的版本都采用这种方式，日志中记录的都是语句（statement），每一条对数据造成修改的sql语句都会记录到日志中，通过 mysqlbinlog 工具，可以清晰的看到每条语句的文本。主从复制时，从库(slave)会将日志解析为原文本，并在从库中重新执行一次。这种格式的优点的日志记录清晰易读，日志量少，对 I/O 影响较小。缺点是在某些情况下 slave 的日志复制会出错。  </p><h5 id="ROW"><a href="#ROW" class="headerlink" title="ROW"></a>ROW</h5><p>mysql 5.1.11 之后，出现了这种新的日志格式，它将每一行的变更记录到日志中，而不是记录sql语句，比如一个简单的跟新sql：<br>update emp set name=’abc’<br>如果是 statement 格式，日志中会记录一行 sql 文本；<br>如果是 row 格式，由于是对全表进行更新，也就是每一行记录都会发生改变，如果是一个 100 万行的大表，则日志中会记录 100万条记录的变化情况。日志量大大增加。  </p><p>这种格式的优点是会记录每一行数据变化的细节，不会出现某些情况下无法复制的情况，缺点是日志量大，对 I/O 影响较大。  </p><h5 id="MIXED"><a href="#MIXED" class="headerlink" title="MIXED"></a>MIXED</h5><p>这是目前 MySQL 默认的日志格式，即混合了 statement 和 row 两种日志。默认情况下采用 statement，但在一些特殊情况下采用 row 来进行记录，比如<br>采用 NDB 存储引擎，此时对表的 DML 语句全部采用 row；<br>客户端使用了临时表；<br>客户端采用了不确定函数，比如 current_user() 等；  </p><p>因为这种不确定函数在主从中得到的值可能不同，导致主从数据产生不一致。mixed 格式能尽量利用两种模式的优点，而避开他们的缺点。  </p><p><strong>注意：</strong>可以在 global 和 session 级别对 binlog_format 进行日志格式设置，但一定要谨慎操作，确保从库的复制能够正常进行。  </p><h4 id="其他选项"><a href="#其他选项" class="headerlink" title="其他选项"></a>其他选项</h4><p>二进制日志记录了数据的变化过程，对于数据的完整性和安全性起着非常重要的作用。因此，MySQL 还提供了一些其他参数选项来进行更小粒度的管理  </p><h5 id="–binlog-db-db-name"><a href="#–binlog-db-db-name" class="headerlink" title="–binlog-db=db_name"></a>–binlog-db=db_name</h5><p>该选项告诉主服务器，如果当前的数据库（即 use 选定的数据库）是 db_name, 应将更新记录到二进制文件中，其他所有么有显式指定的数据库更新将被忽略，不记录在日志中。  </p><h5 id="–binlog-ignore-db-db-name"><a href="#–binlog-ignore-db-db-name" class="headerlink" title="–binlog-ignore-db=db_name"></a>–binlog-ignore-db=db_name</h5><p>该选项告诉主服务器，如果当前的数据库(即 use 选定的数据库) 是 db_name,不应将更新保存到二进制日志中，其他没有显式忽略的数据库都将进行记录。<br>如果想记录或忽略多个数据库，可以对上面两个选项分别使用多次。  </p><h5 id="–innodb-safe-binlog"><a href="#–innodb-safe-binlog" class="headerlink" title="–innodb-safe-binlog"></a>–innodb-safe-binlog</h5><p>此选项经常和 –sync-binlog = N （每写 N 次日志同步磁盘）一起配合使用，使得事务在日志中的记录更加安全。  </p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL Innodb 日志机制及优化</title>
      <link href="/2017-06-16-mysql-innodb-log.html"/>
      <url>/2017-06-16-mysql-innodb-log.html</url>
      
        <content type="html"><![CDATA[<h3 id="innodb-重做日志"><a href="#innodb-重做日志" class="headerlink" title="innodb 重做日志"></a>innodb 重做日志</h3><p>当更新数据时，innodb 内部的操作流程大致是：  </p><ol><li>将数据读入 innodb buffer pool，并对相关记录加独占锁；</li><li>将 undo 信息写入 undo 表空间的回滚段中；  </li><li>更改缓存页中的数据，并将更新记录写入 redo buffer中；</li><li>提交时，根据 innodb_flush_log_at_trx_commit 的设置，用不同的方式将 redo buffer 中的更新记录刷新到 innodb redo log file 中，然后释放独占锁；</li><li>最后，后台 IO 线程根据需要择机将缓存中更新过的数据刷新到磁盘文件中。  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">LOG</span><br><span class="line">---</span><br><span class="line">Log sequence number 1708635750      &#x2F;&#x2F; 上次数据页的修改，还没有刷新到日志文件的lsn号</span><br><span class="line">Log flushed up to   1708635750      &#x2F;&#x2F; 上次成功操作，已经刷新到日志文件中的lsn号</span><br><span class="line">Pages flushed up to 1708635750</span><br><span class="line">Last checkpoint at  1708635741      &#x2F;&#x2F; 上次检查点成功完成时的lsn号，以为着恢复的起点</span><br></pre></td></tr></table></figure><p>其中 lsn(log sequence number) 称为<code>日志序列号</code>,它实际上对应日志文件的偏移量，其生成公式是：<br>新的 lsn = 旧的 lsn + 写入的日志大小  </p><p>例如，日志文件大小为 600 MB,目前的 lsn 是 1GB，现在要将 512 字节的更新记录写入 redo log，则实际写入过程如下：  </p><ol><li>求出偏移量：由于 lsn 数值远大于日志文件大小，因此通过取余方式，得到偏移量为 400MB   </li><li>写入日志：找到偏移 400MB 的位置，写入 512 字节日志内容，下一个事务的 lsn 就是 1000000512。  </li></ol><p>除 innodb buffer pool，innodb log buffer 的大小，redo 日志文件的大小以及 innodb_flush_log_at_trx_commit 参数的设置等，都会影响 innodb 的性能。  </p><h3 id="innodb-flush-log-at-trx-commit-的设置"><a href="#innodb-flush-log-at-trx-commit-的设置" class="headerlink" title="innodb_flush_log_at_trx_commit 的设置"></a>innodb_flush_log_at_trx_commit 的设置</h3><p>innodb_flush_log_at_trx_commit 参数可以控制将 redo buffer 中的更新记录写入到日志文件以及将日志文件数据刷新到磁盘的操作时机。通过调整这个参数，可以在性能和数据安全之间做取舍。  </p><ul><li>0：在事务提交时，innodb 不会立即触发将缓存日志写到磁盘文件的操作，而是每秒触发一次缓存日志回写磁盘操作，并调用系统函数 fsync 刷新 IO 缓存。这种方式效率最高，也最不安全。</li><li>1：在每个事务提交时，innodb 立即将缓存中的 redo 日志回写到日志文件，并调用 fsync 刷新 IO 缓存。  </li><li>2：在每个事务提交时，innodb 立即将缓存中的 redo 日志回写到日志文件，但并不马上调用 fsync 来刷新 IO 缓存，而是每秒只做一次磁盘IO 缓存刷新操作。只要操作系统不发生崩溃，数据就不会丢失，这种方式是对性能和数据安全的折中，其性能和数据安全性介于其他两种方式之间。  </li></ul><p>innodb_flush_log_at_trx_commit 参数的默认值是 1，即每个事务提交时都会从 log buffer 写更新记录到日志文件，而且会实际刷新磁盘缓存，显然，这完全能满足事务的持久化要求，是最安全的，但这样会有较大的性能损失。  </p><p>在某些需要尽量提高性能，并且可以容忍在数据库崩溃时丢失小部分数据，那么通过将参数 innodb_flush_log_at_trx_commit 设置成 0 或 2 都能明显减少日志同步 IO，加快事务提交，从而改善性能。</p><h3 id="设置-log-file-size-，控制检查点"><a href="#设置-log-file-size-，控制检查点" class="headerlink" title="设置 log file size ，控制检查点"></a>设置 log file size ，控制检查点</h3><p>当一个日志文件写满后，innodb 会自动切换到另一个日志文件，但切换时会触发数据库<code>检查点(checkpoint)</code>,这将导致 innodb 缓存脏页的小批量刷新，会明显降低 innodb 的性能。  </p><p>可以通过增大 log file size 避免一个日志文件过快的被写满，但如果日志文件设置的过大，恢复时将需要更长的时间，同时也不便于管理，一般来说，<code>平均每半个小时写满一个日志文件比较合适</code>。  </p><p>可以通过下面的方式来计算 innodb 每小时产生的日志量并估算合适的 innodb_log_file_size 的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 1. 计算 innodb 每分钟产生的日志量  </span><br><span class="line">MySQL [(none)]&gt; pager grep -i &quot;log sequence number&quot;</span><br><span class="line">PAGER set to &#39;grep -i &quot;log sequence number&quot;&#39;</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; show engine innodb status\G select sleep(60);show engine innodb status\G</span><br><span class="line">Log sequence number 1706853570</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">1 row in set (1 min 0.00 sec)</span><br><span class="line"></span><br><span class="line">Log sequence number 1708635750</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; nopager</span><br><span class="line">PAGER set to stdout</span><br><span class="line"></span><br><span class="line">MySQL [(none)]&gt; select round ((1708635750 - 1706853570) &#x2F;1024&#x2F;1024) as MB;</span><br><span class="line">+------+</span><br><span class="line">| MB   |</span><br><span class="line">+------+</span><br><span class="line">|    2 |</span><br><span class="line">+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>通过上述操作得到 innodb 每分钟产生的日志量是 2 MB。然后计算没半小时的日志量：  </p><p>半小时日志量 = 30 * 2MB = 60MB  </p><p>这样，就可以得出 innodb_log_file_size 的大小至少应该是 60MB。</p><h3 id="调整-innodb-log-buffer-size"><a href="#调整-innodb-log-buffer-size" class="headerlink" title="调整 innodb_log_buffer_size"></a>调整 innodb_log_buffer_size</h3><p>innodb_log_buffer_size 决定 innodb 重做日志缓存池的大小，默认是 8MB。对于可能产生大量更新记录的大事务，增加 innodb_log_buffer_size 的大小，可以避免 innodb 在事务提交前就执行不必要的日志写入磁盘操作。因此，对于会在一个事务中更新，插入或删除大量记录的应用，可以通过增大 innodb_log_buffer_size 来减少日志写磁盘操作，提高事务处理性能。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化 sql 语句的一般步骤</title>
      <link href="/2017-06-10-mysql-sql-optimization-steps.html"/>
      <url>/2017-06-10-mysql-sql-optimization-steps.html</url>
      
        <content type="html"><![CDATA[<h4 id="通过-show-status-命令了解各种-sql-的执行频率"><a href="#通过-show-status-命令了解各种-sql-的执行频率" class="headerlink" title="通过 show status 命令了解各种 sql 的执行频率"></a>通过 show status 命令了解各种 sql 的执行<code>频率</code></h4><p>mysql 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息，也可以在操作系统上使用  mysqladmin extend-status 命令获取这些消息。<br>show status 命令中间可以加入选项 session（默认） 或 global：  </p><ul><li>session （当前连接）  </li><li>global （自数据上次启动至今）  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Com_xxx 表示每个 xxx 语句执行的次数。</span><br><span class="line">mysql&gt; show status like &#39;Com_%&#39;;</span><br></pre></td></tr></table></figure><h5 id="我们通常比较关心的是以下几个统计参数："><a href="#我们通常比较关心的是以下几个统计参数：" class="headerlink" title="我们通常比较关心的是以下几个统计参数："></a>我们通常比较关心的是以下几个统计参数：</h5><ul><li>Com_select : 执行 select 操作的次数，一次查询只累加 1。</li><li>Com_insert : 执行 insert 操作的次数，对于批量插入的 insert 操作，只累加一次。</li><li>Com_update : 执行 update 操作的次数。</li><li>Com_delete : 执行 delete 操作的次数。  </li></ul><p>上面这些参数对于所有存储引擎的表操作都会进行累计。下面这几个参数只是针对 innodb 的，累加的算法也略有不同：  </p><ul><li>Innodb_rows_read : select 查询返回的行数。</li><li>Innodb_rows_inserted : 执行 insert 操作插入的行数。  </li><li>Innodb_rows_updated : 执行 update 操作更新的行数。  </li><li>Innodb_rows_deleted : 执行 delete 操作删除的行数。  </li></ul><p>通过以上几个参数，可以很容易地了解当前数据库的应用是以插入更新为主还是以查询操作为主，以及各种类型的 sql 大致的执行比例是多少。对于更新操作的计数，是对执行次数的计数，不论提交还是回滚都会进行累加。<br>对于事务型的应用，通过 Com_commit 和 Com_rollback 可以了解事务提交和回滚的情况，对于回滚操作非常频繁的数据库，可能意味着应用编写存在问题。<br>此外，以下几个参数便于用户了解数据库的基本情况：  </p><ul><li>Connections ： 试图连接 mysql 服务器的次数。  </li><li>Uptime ： 服务器工作时间。  </li><li>Slow_queries : 慢查询次数。  </li></ul><h4 id="定义执行效率较低的-sql-语句"><a href="#定义执行效率较低的-sql-语句" class="headerlink" title="定义执行效率较低的 sql 语句"></a>定义执行效率较低的 sql 语句</h4><h5 id="通过慢查询日志定位那些执行效率较低的-sql-语句，用-–log-slow-queries-file-name-选项启动时，mysqld-写一个包含所有执行时间超过-long-query-time-秒的-sql-语句的日志文件。"><a href="#通过慢查询日志定位那些执行效率较低的-sql-语句，用-–log-slow-queries-file-name-选项启动时，mysqld-写一个包含所有执行时间超过-long-query-time-秒的-sql-语句的日志文件。" class="headerlink" title="通过慢查询日志定位那些执行效率较低的 sql 语句，用 –log-slow-queries[=file_name] 选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 sql 语句的日志文件。"></a>通过慢查询日志定位那些执行效率较低的 sql 语句，用 –log-slow-queries[=file_name] 选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 sql 语句的日志文件。</h5><h5 id="慢查询日志在查询结束以后才记录，所以在应用反映执行效率出现问题的时候慢查询日志并不能定位问题，可以使用-show-processlist-命令查看当前-mysql-在进行的线程，包括线程的状态、是否锁表等，可以实时的查看-sql-的执行情况，同时对一些锁表操作进行优化。"><a href="#慢查询日志在查询结束以后才记录，所以在应用反映执行效率出现问题的时候慢查询日志并不能定位问题，可以使用-show-processlist-命令查看当前-mysql-在进行的线程，包括线程的状态、是否锁表等，可以实时的查看-sql-的执行情况，同时对一些锁表操作进行优化。" class="headerlink" title="慢查询日志在查询结束以后才记录，所以在应用反映执行效率出现问题的时候慢查询日志并不能定位问题，可以使用 show processlist 命令查看当前 mysql 在进行的线程，包括线程的状态、是否锁表等，可以实时的查看 sql 的执行情况，同时对一些锁表操作进行优化。"></a>慢查询日志在查询结束以后才记录，所以在应用反映执行效率出现问题的时候慢查询日志并不能定位问题，可以使用 show processlist 命令查看当前 mysql 在进行的线程，包括线程的状态、是否锁表等，可以实时的查看 sql 的执行情况，同时对一些锁表操作进行优化。</h5><h4 id="通过-explain-分析低效-sql-的执行计划"><a href="#通过-explain-分析低效-sql-的执行计划" class="headerlink" title="通过 explain 分析低效 sql 的执行计划"></a>通过 explain 分析低效 sql 的执行计划</h4><blockquote><p>测试数据库地址：<a href="https://downloads.mysql.com/docs/sakila-db.zip" target="_blank" rel="noopener">https://downloads.mysql.com/docs/sakila-db.zip</a><br>统计某个 email 为租赁电影拷贝所支付的总金额，需要关联客户表 customer 和 付款表 payment ， 并且对付款金额 amount 字段做求和（sum） 操作，相应的执行计划如下：  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; explain select sum(amount) from customer a , payment b where a.customer_id&#x3D; b.customer_id and a.email&#x3D;&#39;JANE.BENNETT@sakilacustomer.org&#39;\G  </span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: a</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 599</span><br><span class="line">     filtered: 10.00</span><br><span class="line">        Extra: Using where</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: b</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: sakila.a.customer_id</span><br><span class="line">         rows: 26</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">2 rows in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure><ul><li>select_type: 表示 select 类型，常见的取值有：<ul><li>simple：简单表，及不使用表连接或者子查询</li><li>primary：主查询，即外层的查询</li><li>union：union 中的第二个或后面的查询语句</li><li>subquery： 子查询中的第一个 select</li></ul></li><li>table ： 输出结果集的表</li><li>type ： 表示 mysql 在表中找到所需行的方式，或者叫访问类型，常见类型性能由差到最好依次是：all、index、range、ref、eq_ref、const，system、null：  </li></ul><ol><li><p>type=ALL，全表扫描，mysql 遍历全表来找到匹配的行：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; explain select * from film where rating &gt; 9 \G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: film</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: NULL</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 1000</span><br><span class="line">     filtered: 33.33</span><br><span class="line">        Extra: Using where</span><br><span class="line"> 1 row in set, 1 warning (0.01 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=index, 索引全扫描，mysql 遍历整个索引来查询匹配的行  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; explain select title form film\G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: film</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: index</span><br><span class="line">possible_keys: NULL</span><br><span class="line">          key: idx_title</span><br><span class="line">      key_len: 767</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 1000</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using index</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=range,索引范围扫描，常见于&lt;、&lt;=、&gt;、&gt;=、between等操作：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; explain select * from payment where customer_id &gt;&#x3D; 300 and customer_id &lt;&#x3D; 350 \G  </span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: payment</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: range</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 1350</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using index condition</span><br><span class="line"> 1 row in set, 1 warning (0.07 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=ref, 使用非唯一索引扫描或唯一索引的前缀扫描，返回匹配某个单独值的记录行，例如：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; explain select * from payment where customer_id &#x3D; 350 \G  </span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: payment</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 23</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line"> 1 row in set, 1 warning (0.01 sec)</span><br></pre></td></tr></table></figure><p>索引 idx_fk_customer_id 是非唯一索引，查询条件为等值查询条件 customer_id = 350, 所以扫描索引的类型为 ref。ref 还经常出现在 join 操作中：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> mysql&gt; explain select b.*, a.* from payment a,customer b where a.customer_id &#x3D; b.customer_id \G </span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: b</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 599</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: a</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: sakila.b.customer_id</span><br><span class="line">         rows: 26</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line"> 2 rows in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=eq_ref,类似 ref，区别就在使用的索引时唯一索引，对于每个索引的键值，表中只要一条记录匹配；简单的说，就是多表连接中使用 primary key 或者 unique index 作为关联条件。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> mysql&gt; explain select * from film a , film_text b where a.film_id &#x3D; b.film_id \G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: b</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 1000</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: a</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: eq_ref</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: PRIMARY</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: sakila.b.film_id</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: Using where</span><br><span class="line"> 2 rows in set, 1 warning (0.03 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=const/system,单表中最多有一个匹配行，查起来非常迅速，所以这个匹配行中的其他列的值可以被优化器在当前查询中当作常量来处理，例如，根据主键 primary key 或者唯一索引 unique index 进行查询。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table test_const (</span><br><span class="line">    -&gt;         test_id int,</span><br><span class="line">    -&gt;         test_context varchar(10),</span><br><span class="line">    -&gt;         primary key (&#96;test_id&#96;),</span><br><span class="line">    -&gt;     );</span><br><span class="line">    </span><br><span class="line"> insert into test_const values(1,&#39;hello&#39;);</span><br><span class="line"></span><br><span class="line"> explain select * from ( select * from test_const where test_id&#x3D;1 ) a \G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: test_const</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: const</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: PRIMARY</span><br><span class="line">      key_len: 4</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">  1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></li><li><p>type=null, mysql 不用访问表或者索引，直接就能够得到结果：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> mysql&gt; explain select 1 from dual where 1 \G</span><br><span class="line"> *************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: NULL</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: NULL</span><br><span class="line">possible_keys: NULL</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: NULL</span><br><span class="line">     filtered: NULL</span><br><span class="line">        Extra: No tables used</span><br><span class="line"> 1 row in set, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></li></ol><p>　　类型 type 还有其他值，如 ref_or_null (与 ref 类似，区别在于条件中包含对 null 的查询)、index_merge(索引合并优化)、unique_subquery (in 的后面是一个查询主键字段的子查询)、index_subquery(与 unique_subquery 类似，区别在于 in 的后面是查询非唯一索引字段的子查询)等。</p><ul><li>possible_keys : 表示查询时可能使用的索引。  </li><li>key ：表示实际使用索引</li><li>key-len : 使用到索引字段的长度。 </li><li>rows ： 扫描行的数量</li><li>extra：执行情况的说明和描述，包含不适合在其他列中显示但是对执行计划非常重要的额外信息。  </li></ul><h5 id="show-warnings-命令"><a href="#show-warnings-命令" class="headerlink" title="show warnings 命令"></a>show warnings 命令</h5><p>执行explain 后再执行 show warnings，可以看到sql 真正被执行之前优化器做了哪些 sql 改写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; explain select sum(amount) from customer a , payment b where 1&#x3D;1 and a.customer_id &#x3D; b.customer_id and email &#x3D; &#39;JANE.BENNETT@sakilacustomer.org&#39;\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: a</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ALL</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: NULL</span><br><span class="line">      key_len: NULL</span><br><span class="line">          ref: NULL</span><br><span class="line">         rows: 599</span><br><span class="line">     filtered: 10.00</span><br><span class="line">        Extra: Using where</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: b</span><br><span class="line">   partitions: NULL</span><br><span class="line">         type: ref</span><br><span class="line">possible_keys: idx_fk_customer_id</span><br><span class="line">          key: idx_fk_customer_id</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: sakila.a.customer_id</span><br><span class="line">         rows: 26</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">2 rows in set, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; show warnings;</span><br><span class="line">+-------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Level | Code | Message                                                                                                                                                                                                                                                     |</span><br><span class="line">+-------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Note  | 1003 | &#x2F;* select#1 *&#x2F; select sum(&#96;sakila&#96;.&#96;b&#96;.&#96;amount&#96;) AS &#96;sum(amount)&#96; from &#96;sakila&#96;.&#96;customer&#96; &#96;a&#96; join &#96;sakila&#96;.&#96;payment&#96; &#96;b&#96; where ((&#96;sakila&#96;.&#96;b&#96;.&#96;customer_id&#96; &#x3D; &#96;sakila&#96;.&#96;a&#96;.&#96;customer_id&#96;) and (&#96;sakila&#96;.&#96;a&#96;.&#96;email&#96; &#x3D; &#39;JANE.BENNETT@sakilacustomer.org&#39;)) |</span><br><span class="line">+-------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>从 warning 的 message 字段中能够看到优化器自动去除了 1=1 恒成立的条件，也就是说优化器在改写 sql 时会自动去掉恒成立的条件。  </p><h5 id="explain-命令也有对分区的支持"><a href="#explain-命令也有对分区的支持" class="headerlink" title="explain 命令也有对分区的支持."></a>explain 命令也有对分区的支持.</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; CREATE TABLE &#96;customer_part&#96; (</span><br><span class="line">    -&gt;   &#96;customer_id&#96; smallint(5) unsigned NOT NULL AUTO_INCREMENT,</span><br><span class="line">    -&gt;   &#96;store_id&#96; tinyint(3) unsigned NOT NULL,</span><br><span class="line">    -&gt;   &#96;first_name&#96; varchar(45) NOT NULL,</span><br><span class="line">    -&gt;   &#96;last_name&#96; varchar(45) NOT NULL,</span><br><span class="line">    -&gt;   &#96;email&#96; varchar(50) DEFAULT NULL,</span><br><span class="line">    -&gt;   &#96;address_id&#96; smallint(5) unsigned NOT NULL,</span><br><span class="line">    -&gt;   &#96;active&#96; tinyint(1) NOT NULL DEFAULT &#39;1&#39;,</span><br><span class="line">    -&gt;   &#96;create_date&#96; datetime NOT NULL,</span><br><span class="line">    -&gt;   &#96;last_update&#96; timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,</span><br><span class="line">    -&gt;   PRIMARY KEY (&#96;customer_id&#96;)</span><br><span class="line">    -&gt;  </span><br><span class="line">    -&gt; ) partition by hash (customer_id) partitions 8;</span><br><span class="line">Query OK, 0 rows affected (0.06 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; insert into customer_part select * from customer;</span><br><span class="line">Query OK, 599 rows affected (0.06 sec)</span><br><span class="line">Records: 599  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; explain select * from customer_part where customer_id&#x3D;130\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">           id: 1</span><br><span class="line">  select_type: SIMPLE</span><br><span class="line">        table: customer_part</span><br><span class="line">   partitions: p2</span><br><span class="line">         type: const</span><br><span class="line">possible_keys: PRIMARY</span><br><span class="line">          key: PRIMARY</span><br><span class="line">      key_len: 2</span><br><span class="line">          ref: const</span><br><span class="line">         rows: 1</span><br><span class="line">     filtered: 100.00</span><br><span class="line">        Extra: NULL</span><br><span class="line">1 row in set, 1 warnings (0.00 sec)</span><br></pre></td></tr></table></figure><p>可以看到 sql 访问的分区是 p2。   </p><h4 id="通过-performance-schema-分析-sql-性能"><a href="#通过-performance-schema-分析-sql-性能" class="headerlink" title="通过 performance_schema 分析 sql 性能"></a>通过 performance_schema 分析 sql 性能</h4><p>旧版本的 mysql 可以使用 profiles 分析 sql 性能，我用的是5.7.18的版本，已经不允许使用 profiles 了，推荐用<br> performance_schema 分析sql。  </p><h4 id="通过-trace-分析优化器如何选择执行计划。"><a href="#通过-trace-分析优化器如何选择执行计划。" class="headerlink" title="通过 trace 分析优化器如何选择执行计划。"></a>通过 trace 分析优化器如何选择执行计划。</h4><p>mysql5.6 提供了对 sql 的跟踪 trace，可以进一步了解为什么优化器选择 A 执行计划而不是 B 执行计划，帮助我们更好的理解优化器的行为。  </p><p>使用方式：首先打开 trace ，设置格式为 json，设置 trace 最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整显示。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MySQL [sakila]&gt; set optimizer_trace&#x3D;&quot;enabled&#x3D;on&quot;,end_markers_in_json&#x3D;on;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; set optimizer_trace_max_mem_size&#x3D;1000000;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>接下来执行想做 trace 的 sql 语句，例如像了解租赁表 rental 中库存编号 inventory_id 为 4466 的电影拷贝在出租日期 rental_date 为 2005-05-25 4:00:00 ~ 5:00:00 之间出租的记录：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select rental_id from rental where 1&#x3D;1 and rental_date &gt;&#x3D; &#39;2005-05-25 04:00:00&#39; and rental_date &lt;&#x3D; &#39;2005-05-25 05:00:00&#39; and inventory_id&#x3D;4466;</span><br><span class="line">+-----------+</span><br><span class="line">| rental_id |</span><br><span class="line">+-----------+</span><br><span class="line">|        39 |</span><br><span class="line">+-----------+</span><br><span class="line">1 row in set (0.06 sec)</span><br><span class="line"></span><br><span class="line">MySQL [sakila]&gt; select * from information_schema.optimizer_trace\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">                            QUERY: select * from infomation_schema.optimizer_trace</span><br><span class="line">                            TRACE: &#123;</span><br><span class="line">  &quot;steps&quot;: [</span><br><span class="line">  ] &#x2F;* steps *&#x2F;</span><br><span class="line">&#125;</span><br><span class="line">MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0</span><br><span class="line">          INSUFFICIENT_PRIVILEGES: 0</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="确定问题并采取相应的优化措施"><a href="#确定问题并采取相应的优化措施" class="headerlink" title="确定问题并采取相应的优化措施"></a>确定问题并采取相应的优化措施</h4><p>经过以上步骤，基本就可以确认问题出现的原因。此时可以根据情况采取相应的措施，进行优化以提高执行的效率。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 命令 top 拆解</title>
      <link href="/2017-05-28-linux-top.html"/>
      <url>/2017-05-28-linux-top.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>top命令是Linux下常用的性能分析工具，能够<em>实时</em>显示系统中各个进程的资源占用状况。</p></blockquote><h3 id="命令详解"><a href="#命令详解" class="headerlink" title="命令详解"></a>命令详解</h3><p>top 命令运行图：</p><p><img src="/images/top/1.png" alt="top 命令运行图"></p><h4 id="第一行——基本信息"><a href="#第一行——基本信息" class="headerlink" title="第一行——基本信息"></a>第一行——基本信息</h4><p><img src="/images/top/2.png" alt="第一行——基本信息"></p><h5 id="load-average"><a href="#load-average" class="headerlink" title="load average:"></a>load average:</h5><p>　　load average 表示系统负载均值，使用 top 或 uptime 可以查看到负载均值的信息，三个数值分表表示 1分钟内 、5分钟内 、 15分钟内的系统负载均值，要理解这三个数值的含义，首先要了解系统的“核数”  </p><p><code>系统的核数 = CPU1 x CPU1的核数 +  CPU2 x CPU2的核数 + CPUn x CPUn的核数 + ……</code>    </p><p>　　更清楚的讲，在Linux系统中输入命令<code>grep -c &#39;model name&#39; /proc/cpuinfo</code>，即可得到核数。  </p><p>　　回到负载均值,<code>负载均值的饱和值等于系统的核数</code>, 所以,根据load average观察系统负载首先要看系统中共有多少”核”,单处理器单核的饱和值为 1,单处理器双核的饱和值为2,双处理器单核的饱和值也为2.<br>　　理解负载均值的最经典的例子是把一个CPU的核当做一座单行单向桥,多核即为多行路单向桥.如图:  </p><p> <img src="/images/top/3.png" alt="图片描述"><br>　　<br>假定,目前系统是单核系统,根据上面的描述,它的负载饱和值为1.这种条件下,各种数值的含义如下:  </p><ul><li>0.00 表示桥上没有任何车流,非常畅通</li><li>0.50 表示桥上有最高承载量一半的车流,也比较流畅.</li><li>1.00 表示桥上已经达到了最大承载量,如果再有车来,可能就要稍等才能上桥了,这种情况下,车速都会很慢,往往都会造成负载均值继续上升.</li><li>1.70 表示桥已经达到最大负载,且还有相对于桥最大负载70%的车辆等待上桥,这个时候的系统,已经要不堪重负了.  </li></ul><p><strong>在实际应用中,重点关注5分钟，15分钟的负载均值，当达到0.7时，就需要调查原因了。</strong>  </p><h4 id="任务信息"><a href="#任务信息" class="headerlink" title="任务信息"></a>任务信息</h4><p><img src="/images/top/4.png" alt="任务信息"></p><blockquote><p>僵尸进程：表示已经终止，但仍然保留一些信息的进程。其等待父进程调用wait()，就可以从内存中完全移除。 将是进程无法使用 <code>kill</code> 清理。如果要手动清理僵尸进程，需要找到其父进程，kill掉父进程后，LInux的 <code>init</code> 进程将接管该僵尸进程(linux中所有的子进程都需要有父进程，当父进程被kill后，其所有子进程将过继给init进程)，init进程隔一段时间去调用wait(),来清除僵尸进程。</p></blockquote><h4 id="CPU使用情况"><a href="#CPU使用情况" class="headerlink" title="CPU使用情况"></a>CPU使用情况</h4><p><img src="/images/top/5.png" alt="CPU使用情况"></p><h4 id="物理内存使用情况"><a href="#物理内存使用情况" class="headerlink" title="物理内存使用情况"></a>物理内存使用情况</h4><p><img src="/images/top/6.png" alt="物理内存使用情况"></p><h5 id="buff-cache："><a href="#buff-cache：" class="headerlink" title="buff/cache："></a>buff/cache：</h5><p>buffers 和 cache 都是内存中存放的数据，不同的是，buffers 存放的是准备写入磁盘的数据，而 cache 存放的是从磁盘中读取的数据<br>在Linux系统中，有一个守护进程(daemon)会定期把buffers中的数据写入的磁盘，也可以使用 sync 命令手动把buffers中的数据写入磁盘。使用buffers可以把分散的 I/O 操作集中起来，减少了磁盘寻道的时间和磁盘碎片。<br>cache是Linux把读取频率高的数据，放到内存中，减少I/O。Linux中cache没有固定大小，根据使用情况自动增加或删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 手动把buffers写入硬盘并清空cache</span><br><span class="line">sync &amp;&amp; echo 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</span><br></pre></td></tr></table></figure><h4 id="交换区使用情况"><a href="#交换区使用情况" class="headerlink" title="交换区使用情况"></a>交换区使用情况</h4><p><img src="/images/top/7.png" alt="交换区使用情况"></p><h5 id="Swap-内存交换区-："><a href="#Swap-内存交换区-：" class="headerlink" title="Swap(内存交换区)："></a>Swap(内存交换区)：</h5><p>　　是硬盘上的一块空间。在内存不足的情况下，操作系统把内存中不用的数据存到硬盘的交换区，腾出内存来让别的程序运行。因此，开启swap会一定程度的引起 I/O 性能下降(阿里服务器默认不开)。 </p><h4 id="进程详细信息"><a href="#进程详细信息" class="headerlink" title="进程详细信息"></a>进程详细信息</h4><p><img src="/images/top/8.png" alt="进程详细信息"></p><h3 id="灵活使用top"><a href="#灵活使用top" class="headerlink" title="灵活使用top"></a>灵活使用top</h3><h4 id="命令行式使用"><a href="#命令行式使用" class="headerlink" title="命令行式使用"></a>命令行式使用</h4><p><code>-b</code>：以批处理模式操作 这种方式可以把top输出的内容以可读的形式写入文件<code>top -b &gt;&gt; top.txt</code><br><code>-c</code>：显示完整的命令行(COMMAND),想查看进程执行的具体位置时，非常有用<br><code>-d</code>：屏幕刷新间隔时间 <code>top -d 1</code>:表示每隔一秒刷新一次<br><code>-s</code>：使用保密模式<br><code>-S</code>：指定累积模式<br><code>-i</code>：不显示任何闲置或者僵死进程<br><code>-u&lt;用户名</code>&gt;：指定用户名<br><code>-p&lt;进程号</code>&gt;：指定进程<br><code>-n&lt;次数&gt;</code>：指定循环显示的次数，到了次数自己退出。</p><h4 id="交互式使用"><a href="#交互式使用" class="headerlink" title="交互式使用"></a>交互式使用</h4><p>top命令显示系统实时状态，支持交互操作。执行top命令，显示系统状态界面（同时也是交互界面），输入交互命令：</p><p><code>1</code>：查看CPU每个核的使用情况<br><code>h</code>：显示帮助画面，给出一些简短的命令总结说明<br><code>k</code>：终止一个进程<br><code>i</code>：忽略闲置和僵死进程，这是一个开关式命令<br><code>q</code>：退出程序<br><code>r</code>：重新安排一个进程的优先级别<br><code>S</code>：切换到累计模式<br><code>s</code>：改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s<br><code>l</code>：切换显示平均负载和启动时间信息<br><code>m</code>：切换显示内存信息<br><code>t</code>：切换显示进程和CPU状态信息<br><code>c</code>：切换显示命令名称和完整命令行<br><code>M</code>：根据驻留内存大小进行排序<br><code>P</code>：根据CPU使用百分比大小进行排序<br><code>T</code>：根据时间/累计时间进行排序<br><code>w</code>：将当前设置写入~/.toprc文件中。</p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法效率分析</title>
      <link href="/2017-05-21-algorithm-efficiency-analysis.html"/>
      <url>/2017-05-21-algorithm-efficiency-analysis.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>衡量一个算法的效率.最直接有效的办法就是执行一遍,查看执行时间.但是更客观的方法是计算时间复杂度,并用”大O记法”表示</p></blockquote><a id="more"></a><h3 id="计算时间复杂度的基本原则"><a href="#计算时间复杂度的基本原则" class="headerlink" title="计算时间复杂度的基本原则"></a>计算时间复杂度的基本原则</h3><ol><li>基本操作,只有常数项,时间复杂度为O(1)</li><li>顺序结构，按加法进行计算</li><li>循环结构，按乘法进行计算</li><li>分支结构，取最大值</li><li>判断一个算法的效率只需要关注操作数量的最高次项，其它可以忽略</li><li>分析的算时间复杂度一般都是指最坏时间复杂度</li></ol><h3 id="常见时间复杂度"><a href="#常见时间复杂度" class="headerlink" title="常见时间复杂度"></a>常见时间复杂度</h3><table><thead><tr><th>函数</th><th>阶</th><th>通俗术语</th></tr></thead><tbody><tr><td>5</td><td>O(1)</td><td>常数阶</td></tr><tr><td>$5\log_2{^n} + 2$</td><td>O($\log^n$)</td><td>对数阶</td></tr><tr><td>$3n + 5$</td><td>O($n$)</td><td>线性阶</td></tr><tr><td>$3n\log_2{^n} + 2n + 5$</td><td>O($n\log^n$)</td><td>$n\log^n$阶</td></tr><tr><td>$3n^2 + 2n + 1$</td><td>O($n^2$)</td><td>平方阶</td></tr><tr><td>$3n^3 + 2n^2 + n +1 $</td><td>O($n^3$)</td><td>立方阶</td></tr><tr><td>$2^n$</td><td>O($2^n$)</td><td>指数阶</td></tr></tbody></table><p>所消耗的时间从低到高依次为:  </p><blockquote><p>O(1) &lt; O($\log^n$) &lt; O(n) &lt; O($n\log^n$)) &lt; O($n2$) &lt; O($n^3$) &lt; O($2^n$)</p></blockquote><h3 id="计算时间复杂度的案例"><a href="#计算时间复杂度的案例" class="headerlink" title="计算时间复杂度的案例"></a>计算时间复杂度的案例</h3><h4 id="O-n-2"><a href="#O-n-2" class="headerlink" title="O($n^2$)"></a>O($n^2$)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment"># 一次</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="comment"># n 次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># n 平方次</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># n 平方次</span></span><br><span class="line">            sum += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure><p>T(n) = 1 + n + $n^2$ + $n^2$ = O($n^2$)</p><h4 id="O-logn"><a href="#O-logn" class="headerlink" title="O($logn$)"></a>O($logn$)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def test(n):</span><br><span class="line">    i &#x3D; 1</span><br><span class="line">    while i &lt; n :</span><br><span class="line">        i *&#x3D; 2</span><br><span class="line">    return i</span><br></pre></td></tr></table></figure><p>每次循环 i 都 乘 2,所以设运算 x 次 则有 $2^x &lt;= n$, 则 x = $\log_2{^n}$<br>T(n) = 1 + $\log_2{^n}$ = O($logn$)  </p><h3 id="Python-中的list和dict常用操作的效率"><a href="#Python-中的list和dict常用操作的效率" class="headerlink" title="Python 中的list和dict常用操作的效率"></a>Python 中的list和dict常用操作的效率</h3><h4 id="list-操作"><a href="#list-操作" class="headerlink" title="list 操作"></a>list 操作</h4><p><img src="/images/list-operate.png" alt="image"></p><h4 id="dict-操作"><a href="#dict-操作" class="headerlink" title="dict 操作"></a>dict 操作</h4><p><img src="/images/dict-operate.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间复杂度 </tag>
            
            <tag> 大O算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于 Linux 进程的 UID、EUID、GID 和 EGID</title>
      <link href="/2017-05-14-uid-euid-gid-egid-introduce.html"/>
      <url>/2017-05-14-uid-euid-gid-egid-introduce.html</url>
      
        <content type="html"><![CDATA[<p>有的时候在 linux 环境中执行一些命令 , 会提示对某某文件  Permission denied, 这种权限问题通常可以通过在命令前加 <code>sudo</code> 解决,但这样难免有些繁琐,其实,还有更好的方法……</p><a id="more"></a><h4 id="UID、EUID、GID-和-EGID-简介"><a href="#UID、EUID、GID-和-EGID-简介" class="headerlink" title="UID、EUID、GID 和 EGID 简介"></a>UID、EUID、GID 和 EGID 简介</h4><ul><li>UID  真实用户ID</li><li>EUID  有效用户ID</li><li>GID  真实组ID</li><li>EGID  有效组ID<br>　　用户信息对于服务器程序的安全性来说是很重要的，比如大部分服务器就必须以 root 身份启动，但不能以 root 身份运行。一个进程拥有两个用户ID：UID 和 EUID。EUID存在的目的是方便资源访问：他使得运行程序的用户拥有该程序的有效用户权限。<br>　　比如 su 程序，任何用户都可以使用它来修改自己的账户信息，但修改账户时 su 程序不得不访问 /etc/passwd 文件，而访问该文件是需要 root 权限的。那么以普通用户身份启动的 su 程序如何能访问 /etc/passwd 文件呢？窍门就在 EUID。用 ls 命令可以查看到，su 程序的所有者是 root，并且它被设置了 set-user-id 标志。<br><img src="/images/uid-euid-gid-egid-introduce.png" alt="image"></li></ul><p>　　这个标志表示，任何普通用户运行 su 程序时，其有效用户就是该程序的所有者。那么，根据有效用户的含义，任何运行 su 程序的普通用户都能访问 /etc/passwd 文件。有效用户为 root 进程称为特权进程（privileged processes）。EGID 的含义与 EUID 类似：给运行目标程序的组用户提供有效组的权限。 </p><h4 id="设置方法"><a href="#设置方法" class="headerlink" title="设置方法"></a>设置方法</h4><p>chmod u+s filename 设置SUID位 </p><p>chmod u-s filename 去掉SUID设置 </p><p>chmod g+s filename 设置SGID位 </p><p>chmod g-s filename 去掉SGID设置 </p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p><img src="/images/uid-euid-gid-egid-introduce-docker-permission-denied.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux 权限问题 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
